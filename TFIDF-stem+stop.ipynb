{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:24:30,914 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'discu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('discus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove non-alphabets from text\n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,\n",
    "    'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_stop = set(get_stop_words('en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in word_tokenize(string):\n",
    "            if word not in en_stop:\n",
    "                tokens.append(ps.stem(word))\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:26:15,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:32:51,269 : INFO : adding document #10000 to Dictionary(172845 unique tokens: [u'gudadannavar', u'alsocont', u'beencredit', u'delhifano', u'alsocons']...)\n",
      "2018-03-16 15:38:45,492 : INFO : adding document #20000 to Dictionary(312646 unique tokens: [u'gudadannavar', u'falselitig', u'againstcomplain', u'ofday', u'nonehru']...)\n",
      "2018-03-16 15:44:18,352 : INFO : adding document #30000 to Dictionary(472300 unique tokens: [u'gudadannavar', u'mrmvenkatesancounsel', u'rkgejggll', u'boncer', u'metersand']...)\n",
      "2018-03-16 15:49:08,979 : INFO : adding document #40000 to Dictionary(654903 unique tokens: [u'gudadannavar', u'woodb', u'mrmvenkatesancounsel', u'atsugham', u'rkgejggll']...)\n",
      "2018-03-16 15:52:53,477 : INFO : adding document #50000 to Dictionary(771804 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'khallimahapatro']...)\n",
      "2018-03-16 15:56:22,025 : INFO : adding document #60000 to Dictionary(887421 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'pricepremium']...)\n",
      "2018-03-16 15:59:27,201 : INFO : adding document #70000 to Dictionary(971423 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'pricepremium']...)\n",
      "2018-03-16 16:03:17,572 : INFO : adding document #80000 to Dictionary(1068170 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n",
      "2018-03-16 16:07:39,394 : INFO : adding document #90000 to Dictionary(1159711 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n",
      "2018-03-16 16:12:52,655 : INFO : adding document #100000 to Dictionary(1278422 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-594ec205032c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create all the court specific dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictionary_DCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdictionary_NCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdictionary_SCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/atulgang/anaconda3/envs/python2/lib/python2.7/site-packages/gensim/corpora/dictionary.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/atulgang/anaconda3/envs/python2/lib/python2.7/site-packages/gensim/corpora/dictionary.pyc\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0munique\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-594ec205032c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((text,))\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create all the court specific dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictionary_DCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdictionary_NCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdictionary_SCDRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCDRC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-21947e048cc5>\u001b[0m in \u001b[0;36mfiles_to_tokens\u001b[0;34m(glob_filenames)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/atulgang/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/stem/porter.pyc\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create all the court specific dictionaries\n",
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# ## check out dictionaries\n",
    "# dictionary_DCDRC.save_as_text('newdictionary_DCDRC.txt') \n",
    "# dictionary_NCDRC.save_as_text('newdictionary_NCDRC.txt') \n",
    "# dictionary_SCDRC.save_as_text('newdictionary_SCDRC.txt') \n",
    "# dictionary_DelhiHC.save_as_text('newdictionary_DelhiHC.txt') \n",
    "# dictionary_JharkhandHC.save_as_text('newdictionary_JharkhandHC.txt') \n",
    "# dictionary_JodhpurHC.save_as_text('newdictionary_JodhpurHC.txt') \n",
    "# dictionary_KolkataHC.save_as_text('newdictionary_KolkataHC.txt') \n",
    "# dictionary_SupremeCourt.save_as_text('newdictionary_SupremeCourt.txt') \n",
    "# Save actual dictionaries\n",
    "dictionary_DCDRC.save('1') \n",
    "dictionary_NCDRC.save('2') \n",
    "dictionary_SCDRC.save('3') \n",
    "dictionary_DelhiHC.save('4') \n",
    "dictionary_JharkhandHC.save('5') \n",
    "dictionary_JodhpurHC.save('6') \n",
    "dictionary_KolkataHC.save('7') \n",
    "dictionary_SupremeCourt.save('8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load those dictionaries\n",
    "dictionary_DCDRC = corpora.Dictionary.load('1') \n",
    "dictionary_NCDRC = corpora.Dictionary.load('2') \n",
    "dictionary_SCDRC = corpora.Dictionary.load('3') \n",
    "dictionary_DelhiHC = corpora.Dictionary.load('4') \n",
    "dictionary_JharkhandHC = corpora.Dictionary.load('5') \n",
    "dictionary_JodhpurHC = corpora.Dictionary.load('6') \n",
    "dictionary_KolkataHC = corpora.Dictionary.load('7') \n",
    "dictionary_SupremeCourt= corpora.Dictionary.load('8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "dictionary.compactify()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=100)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "dictionary.save('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()\n",
    "# save BOW\n",
    "corpora.MmCorpus.serialize('newcorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('newcorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tfidf model\n",
    "# Will take time as to generate a model it processes over the whole corpus\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "# save tfidf model\n",
    "tfidf.save('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf model\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf wrapper coprus\n",
    "# doesn't take time - yield\n",
    "tfidf_corpus = tfidf[corpus]\n",
    "\n",
    "# actual tfidf corpus serialized\n",
    "corpora.MmCorpus.serialize('newtfidf_corpus.mm', tfidf_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf corpus\n",
    "tfidf_corpus = corpora.MmCorpus('newtfidf_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "# query no. gives the list of 0-1 documents\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create index - Tfidf\n",
    "tfidf_index = gensim.similarities.Similarity('tfidf.index',tfidf_corpus,len(dictionary))\n",
    "\n",
    "# save index - tfidf\n",
    "gensim.similarities.Similarity.save(tfidf_index,'TFIDF_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### load tfidf index\n",
    "tfidf_index = gensim.similarities.Similarity.load('TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate similarity for all \n",
    "tfidf_index.num_best= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################         SIMILIRATIES SCORES AND RANKING    ###########################\n",
    "tfidf_results = copy.deepcopy(query_truth)\n",
    "### lists of list (indexing from 1, dummy element)\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    # inside the square bracket determines query representation\n",
    "    \n",
    "    sims = tfidf_index[tfidf[dictionary.doc2bow(query)]]\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in tfidf_results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "        #x.append(common_words(filenames.index(x[0]+'.txt'), tfidf_corpus, query))\n",
    "    \n",
    "    # sort wrt relevance(from truth) and then ranks(from our model)\n",
    "    #tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[1]),x[3]))     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'tfidf.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"TFIDF\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in tfidf_results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluation_Qwise - Precision, Recall and Fscore\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:\n",
    "    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/atulgang/Thesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 16:48:23,513 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# WordNet only cares about 5 parts of speech.\n",
    "# The other parts of speech will be tagged as nouns.\n",
    "\n",
    "part = {\n",
    "    'N' : 'n',\n",
    "    'V' : 'v',\n",
    "    'J' : 'a',\n",
    "    'S' : 's',\n",
    "    'R' : 'r'\n",
    "}\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def convert_tag(penn_tag):\n",
    "    '''\n",
    "    convert_tag() accepts the **first letter** of a Penn part-of-speech tag,\n",
    "    then uses a dict lookup to convert it to the appropriate WordNet tag.\n",
    "    '''\n",
    "    if penn_tag in part.keys():\n",
    "        return part[penn_tag]\n",
    "    else:\n",
    "        # other parts of speech will be tagged as nouns\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def tag_and_lem(element):\n",
    "    '''\n",
    "    tag_and_lem() accepts a token list, tags, converts tags,\n",
    "    lemmatizes, and returns a string\n",
    "    '''\n",
    "    # list of tuples [('token', 'tag'), ('token2', 'tag2')...]\n",
    "    sent = pos_tag(element) # must tag in context\n",
    "    return [ wnl.lemmatize(sent[k][0], convert_tag(sent[k][1][0])) for k in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) \n",
    "\n",
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_stop = set(get_stop_words('en'))\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        #print filename\n",
    "        # read the whole file as lowercase string\n",
    "        string = (f.read()).lower()\n",
    "        try:\n",
    "            temp = word_tokenize(string)\n",
    "        except:\n",
    "            string = string.decode(\"latin-1\")\n",
    "            temp = word_tokenize(string)\n",
    "        \n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in temp:\n",
    "            w =  only_alphabet(word).lower()\n",
    "            if w not in en_stop:\n",
    "                if w:\n",
    "                    tokens.append(w)\n",
    "        tokens = tag_and_lem(tokens)\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 16:48:42,171 : INFO : loading Dictionary object from dict/lem10\n",
      "2018-04-29 16:48:42,224 : INFO : loaded dict/lem10\n"
     ]
    }
   ],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('dict/lem10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 16:48:42,300 : INFO : loaded corpus index from corpus/bow_lem10.mm.index\n",
      "2018-04-29 16:48:42,301 : INFO : initializing cython corpus reader from corpus/bow_lem10.mm\n",
      "2018-04-29 16:48:42,301 : INFO : accepted corpus with 351985 documents, 169716 features, 99651009 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('corpus/bow_lem10.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load p_C\n",
    "with open('p_C/p_C_lem10.pkl', 'rb') as ip:\n",
    "    p_C = pickle.load(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc_len\n",
    "with open('doc_len/doc_len_lem10.pkl', 'rb') as ip:\n",
    "    doc_len = pickle.load(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "\n",
    "# Dirichlet parameter\n",
    "MU = 2000\n",
    "\n",
    "\n",
    "class Dirichlet(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.corpus_size = dictionary.num_docs\n",
    "\n",
    "    def get_score(self, query, index, query_len):\n",
    "        index_doc = dict(self.corpus[index])\n",
    "        score = 0\n",
    "        for word,freq in query:\n",
    "            if word not in index_doc:\n",
    "                continue\n",
    "            score += freq*(math.log( 1 + index_doc[word]/ (MU * p_C[word]) ))\n",
    "        return score + query_len * math.log(MU/(doc_len[index] + MU))\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        scores = []\n",
    "        query_len = 0\n",
    "        for word, freq in query:\n",
    "            query_len += freq\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(query, index, query_len)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D = Dirichlet(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = copy.deepcopy(query_truth)\n",
    "\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    sims = D.get_scores(dictionary.doc2bow(query))\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "    print i\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'LM_Dir_lem10'\n",
    "\n",
    "with open('py_results/' + model_name + 'sim_list.pkl', 'wb') as output:\n",
    "    pickle.dump(sim_list, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open('py_results/' + model_name + 'rank_list.pkl', 'wb') as output:\n",
    "    pickle.dump(rank_list, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "\n",
    "\n",
    "wb = Workbook()\n",
    "dest_filename = 'result_excel/' + model_name + '.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"ground_truth\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "#Query wise (F score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile(dest_filename)\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "places = 4\n",
    "wb = load_workbook(dest_filename)\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name ]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile(dest_filename)\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "places = 4\n",
    "wb = load_workbook(dest_filename)\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name ]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name ]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                       truncate_sheet=False, \n",
    "                       **to_excel_kwargs):\n",
    "    \"\"\"\n",
    "    Append a DataFrame [df] to existing Excel file [filename]\n",
    "    into [sheet_name] Sheet.\n",
    "    If [filename] doesn't exist, then this function will create it.\n",
    "\n",
    "    Parameters:\n",
    "      filename : File path or existing ExcelWriter\n",
    "                 (Example: '/path/to/file.xlsx')\n",
    "      df : dataframe to save to workbook\n",
    "      sheet_name : Name of sheet which will contain DataFrame.\n",
    "                   (default: 'Sheet1')\n",
    "      startrow : upper left cell row to dump data frame.\n",
    "                 Per default (startrow=None) calculate the last row\n",
    "                 in the existing DF and write to the next row...\n",
    "      truncate_sheet : truncate (remove and recreate) [sheet_name]\n",
    "                       before writing DataFrame to Excel file\n",
    "      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n",
    "                        [can be dictionary]\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    try:\n",
    "        # try to open an existing workbook\n",
    "        writer.book = load_workbook(filename)\n",
    "\n",
    "        # get the last row in the existing Excel sheet\n",
    "        # if it was not specified explicitly\n",
    "        if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "            startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "        # truncate sheet\n",
    "        if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "            # index of [sheet_name] sheet\n",
    "            idx = writer.book.sheetnames.index(sheet_name)\n",
    "            # remove [sheet_name]\n",
    "            writer.book.remove(writer.book.worksheets[idx])\n",
    "            # create an empty sheet [sheet_name] using old index\n",
    "            writer.book.create_sheet(sheet_name, idx)\n",
    "\n",
    "        # copy existing sheets\n",
    "        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    except FileNotFoundError:\n",
    "        # file does not exist yet, we will create it\n",
    "        pass\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, index= False, header = False, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()\n",
    "\n",
    "    \n",
    "f = pd.ExcelFile(dest_filename)\n",
    "sec_dest = 'result_excel/all_results.xlsx'\n",
    "f2 = pd.ExcelFile(sec_dest)\n",
    "append_df_to_excel(sec_dest, f.parse(1), sheet_name= f2.sheet_names[0])\n",
    "append_df_to_excel(sec_dest, f.parse(2), sheet_name= f2.sheet_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Top_x(x, R):\n",
    "    R = list(R)\n",
    "    x_names = []\n",
    "    for i in range(x):\n",
    "        x_names.append(filenames[R.index(i + 1)].strip('.txt'))\n",
    "    return x_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_stat(word, freq, index):\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    word = dictionary.doc2bow([word])[0][0]\n",
    "    idf = bm25.idf[word] if bm25.idf[word] >= 0 else EPSILON * average_idf\n",
    "    score = freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "              / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / bm25.avgdl))))\n",
    "    return (index_doc[word], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "dest_filename = 'Temp.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "def common_words(q, F):\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    index = filenames.index(F + '.txt')\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    row = []\n",
    "    for word, freq in  zip(q_words, q_freq):\n",
    "        w = dictionary.doc2bow([word])[0][0]\n",
    "        if w in index_doc: \n",
    "            f, s = score_stat(word, freq, index)\n",
    "            row.append([word + ', ' + str(round(bm25.idf[w], 2)), freq, f,  round( (s*100)/sim_list[q][index], 1)])  \n",
    "    row.sort(key = lambda x: (x[3],x[1],x[2]), reverse = True)\n",
    "    wb = load_workbook('Temp.xlsx')\n",
    "    ws = wb.create_sheet(title=str(q) + F)\n",
    "    ws.append(['word,idf', 'Query freq', 'freq', '% score'])\n",
    "    for i in row:\n",
    "        ws.append(i)\n",
    "    wb.save(filename = 'Temp.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"toberead.txt\", 'r') \n",
    "count = 0\n",
    "q = 1\n",
    "for i in f:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    count = count + 1\n",
    "    common_words(q, i.strip())\n",
    "    if count % 5 == 0:\n",
    "        q = q + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in files_to_tokens(full_filenames[filenames.index(doc + '.txt')]):\n",
    "    for j in i:\n",
    "        print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = 'ConsumerCourt_SCDRC_30176'\n",
    "corpus[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words(1, 'ConsumerCourt_DCDRC_41588')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index= None, columns=['words','pair'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.append(['lol', ('we','wew')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncommon_words(q, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "wb = load_workbook('Explore_BM25.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_uncom.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_uncom.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "#         for word, freq in  zip(q_words, q_freq):\n",
    "#             if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "#                 f, s = score_stat(word, freq, index)\n",
    "#                 row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "#             else:\n",
    "#                 row.append('X')\n",
    "        #ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_all.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_all.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(sec_row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/atulgang/Thesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-06 15:39:57,644 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# WordNet only cares about 5 parts of speech.\n",
    "# The other parts of speech will be tagged as nouns.\n",
    "\n",
    "part = {\n",
    "    'N' : 'n',\n",
    "    'V' : 'v',\n",
    "    'J' : 'a',\n",
    "    'S' : 's',\n",
    "    'R' : 'r'\n",
    "}\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def convert_tag(penn_tag):\n",
    "    '''\n",
    "    convert_tag() accepts the **first letter** of a Penn part-of-speech tag,\n",
    "    then uses a dict lookup to convert it to the appropriate WordNet tag.\n",
    "    '''\n",
    "    if penn_tag in part.keys():\n",
    "        return part[penn_tag]\n",
    "    else:\n",
    "        # other parts of speech will be tagged as nouns\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def tag_and_lem(element):\n",
    "    '''\n",
    "    tag_and_lem() accepts a token list, tags, converts tags,\n",
    "    lemmatizes, and returns a string\n",
    "    '''\n",
    "    # list of tuples [('token', 'tag'), ('token2', 'tag2')...]\n",
    "    sent = pos_tag(element) # must tag in context\n",
    "    return [ wnl.lemmatize(sent[k][0], convert_tag(sent[k][1][0])) for k in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) \n",
    "\n",
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_stop = set(get_stop_words('en'))\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        #print filename\n",
    "        # read the whole file as lowercase string\n",
    "        string = (f.read()).lower()\n",
    "        try:\n",
    "            temp = word_tokenize(string)\n",
    "        except:\n",
    "            string = string.decode(\"latin-1\")\n",
    "            temp = word_tokenize(string)\n",
    "        \n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in temp:\n",
    "            w =  only_alphabet(word).lower()\n",
    "            if w not in en_stop:\n",
    "                if w:\n",
    "                    tokens.append(w)\n",
    "        tokens = tag_and_lem(tokens)\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:42:08,805 : INFO : loading Dictionary object from dict/lem10\n",
      "2018-04-29 17:42:08,855 : INFO : loaded dict/lem10\n"
     ]
    }
   ],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('dict/lem10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:42:08,952 : INFO : loaded corpus index from corpus/bow_lem10.mm.index\n",
      "2018-04-29 17:42:08,953 : INFO : initializing cython corpus reader from corpus/bow_lem10.mm\n",
      "2018-04-29 17:42:08,953 : INFO : accepted corpus with 351985 documents, 169716 features, 99651009 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('corpus/bow_lem10.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:42:09,035 : INFO : collecting document frequencies\n",
      "2018-04-29 17:42:09,035 : INFO : PROGRESS: processing document #0\n",
      "2018-04-29 17:42:10,803 : INFO : PROGRESS: processing document #10000\n",
      "2018-04-29 17:42:12,690 : INFO : PROGRESS: processing document #20000\n",
      "2018-04-29 17:42:14,521 : INFO : PROGRESS: processing document #30000\n",
      "2018-04-29 17:42:16,378 : INFO : PROGRESS: processing document #40000\n",
      "2018-04-29 17:42:17,842 : INFO : PROGRESS: processing document #50000\n",
      "2018-04-29 17:42:19,561 : INFO : PROGRESS: processing document #60000\n",
      "2018-04-29 17:42:21,143 : INFO : PROGRESS: processing document #70000\n",
      "2018-04-29 17:42:22,974 : INFO : PROGRESS: processing document #80000\n",
      "2018-04-29 17:42:24,919 : INFO : PROGRESS: processing document #90000\n",
      "2018-04-29 17:42:27,130 : INFO : PROGRESS: processing document #100000\n",
      "2018-04-29 17:42:29,328 : INFO : PROGRESS: processing document #110000\n",
      "2018-04-29 17:42:30,553 : INFO : PROGRESS: processing document #120000\n",
      "2018-04-29 17:42:32,367 : INFO : PROGRESS: processing document #130000\n",
      "2018-04-29 17:42:33,511 : INFO : PROGRESS: processing document #140000\n",
      "2018-04-29 17:42:34,987 : INFO : PROGRESS: processing document #150000\n",
      "2018-04-29 17:42:36,526 : INFO : PROGRESS: processing document #160000\n",
      "2018-04-29 17:42:38,111 : INFO : PROGRESS: processing document #170000\n",
      "2018-04-29 17:42:39,863 : INFO : PROGRESS: processing document #180000\n",
      "2018-04-29 17:42:41,926 : INFO : PROGRESS: processing document #190000\n",
      "2018-04-29 17:42:44,216 : INFO : PROGRESS: processing document #200000\n",
      "2018-04-29 17:42:46,398 : INFO : PROGRESS: processing document #210000\n",
      "2018-04-29 17:42:48,441 : INFO : PROGRESS: processing document #220000\n",
      "2018-04-29 17:42:49,150 : INFO : PROGRESS: processing document #230000\n",
      "2018-04-29 17:42:49,600 : INFO : PROGRESS: processing document #240000\n",
      "2018-04-29 17:42:50,186 : INFO : PROGRESS: processing document #250000\n",
      "2018-04-29 17:42:50,680 : INFO : PROGRESS: processing document #260000\n",
      "2018-04-29 17:42:51,134 : INFO : PROGRESS: processing document #270000\n",
      "2018-04-29 17:42:51,851 : INFO : PROGRESS: processing document #280000\n",
      "2018-04-29 17:42:52,439 : INFO : PROGRESS: processing document #290000\n",
      "2018-04-29 17:42:53,313 : INFO : PROGRESS: processing document #300000\n",
      "2018-04-29 17:42:54,267 : INFO : PROGRESS: processing document #310000\n",
      "2018-04-29 17:42:55,385 : INFO : PROGRESS: processing document #320000\n",
      "2018-04-29 17:42:58,547 : INFO : PROGRESS: processing document #330000\n",
      "2018-04-29 17:43:01,220 : INFO : PROGRESS: processing document #340000\n",
      "2018-04-29 17:43:03,854 : INFO : PROGRESS: processing document #350000\n",
      "2018-04-29 17:43:04,490 : INFO : calculating IDF weights for 351985 documents and 169715 features (99651009 matrix non-zeros)\n",
      "2018-04-29 17:43:04,812 : INFO : saving TfidfModel object under py_models/tfidf_lem10.mdl, separately None\n",
      "2018-04-29 17:43:05,402 : INFO : saved py_models/tfidf_lem10.mdl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 424 ms, total: 56.4 s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tfidf model\n",
    "# Will take time as to generate a model it processes over the whole corpus\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "# save tfidf model\n",
    "tfidf.save('py_models/tfidf_lem10.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:43:05,410 : INFO : loading TfidfModel object from py_models/tfidf_lem10.mdl\n",
      "2018-04-29 17:43:05,501 : INFO : loaded py_models/tfidf_lem10.mdl\n"
     ]
    }
   ],
   "source": [
    "##################################### load tfidf model\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('py_models/tfidf_lem10.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:43:05,635 : INFO : storing corpus in Matrix Market format to corpus/tfidf_lem10.mm\n",
      "2018-04-29 17:43:05,636 : INFO : saving sparse matrix to corpus/tfidf_lem10.mm\n",
      "2018-04-29 17:43:05,638 : INFO : PROGRESS: saving document #0\n",
      "2018-04-29 17:43:07,478 : INFO : PROGRESS: saving document #1000\n",
      "2018-04-29 17:43:08,806 : INFO : PROGRESS: saving document #2000\n",
      "2018-04-29 17:43:10,353 : INFO : PROGRESS: saving document #3000\n",
      "2018-04-29 17:43:12,209 : INFO : PROGRESS: saving document #4000\n",
      "2018-04-29 17:43:14,512 : INFO : PROGRESS: saving document #5000\n",
      "2018-04-29 17:43:16,109 : INFO : PROGRESS: saving document #6000\n",
      "2018-04-29 17:43:17,784 : INFO : PROGRESS: saving document #7000\n",
      "2018-04-29 17:43:20,392 : INFO : PROGRESS: saving document #8000\n",
      "2018-04-29 17:43:22,222 : INFO : PROGRESS: saving document #9000\n",
      "2018-04-29 17:43:24,589 : INFO : PROGRESS: saving document #10000\n",
      "2018-04-29 17:43:26,605 : INFO : PROGRESS: saving document #11000\n",
      "2018-04-29 17:43:28,452 : INFO : PROGRESS: saving document #12000\n",
      "2018-04-29 17:43:30,349 : INFO : PROGRESS: saving document #13000\n",
      "2018-04-29 17:43:32,369 : INFO : PROGRESS: saving document #14000\n",
      "2018-04-29 17:43:34,789 : INFO : PROGRESS: saving document #15000\n",
      "2018-04-29 17:43:37,518 : INFO : PROGRESS: saving document #16000\n",
      "2018-04-29 17:43:39,317 : INFO : PROGRESS: saving document #17000\n",
      "2018-04-29 17:43:41,657 : INFO : PROGRESS: saving document #18000\n",
      "2018-04-29 17:43:43,157 : INFO : PROGRESS: saving document #19000\n",
      "2018-04-29 17:43:44,638 : INFO : PROGRESS: saving document #20000\n",
      "2018-04-29 17:43:46,128 : INFO : PROGRESS: saving document #21000\n",
      "2018-04-29 17:43:47,679 : INFO : PROGRESS: saving document #22000\n",
      "2018-04-29 17:43:49,633 : INFO : PROGRESS: saving document #23000\n",
      "2018-04-29 17:43:51,535 : INFO : PROGRESS: saving document #24000\n",
      "2018-04-29 17:43:53,585 : INFO : PROGRESS: saving document #25000\n",
      "2018-04-29 17:43:55,604 : INFO : PROGRESS: saving document #26000\n",
      "2018-04-29 17:43:57,420 : INFO : PROGRESS: saving document #27000\n",
      "2018-04-29 17:43:59,518 : INFO : PROGRESS: saving document #28000\n",
      "2018-04-29 17:44:01,972 : INFO : PROGRESS: saving document #29000\n",
      "2018-04-29 17:44:03,912 : INFO : PROGRESS: saving document #30000\n",
      "2018-04-29 17:44:05,297 : INFO : PROGRESS: saving document #31000\n",
      "2018-04-29 17:44:06,844 : INFO : PROGRESS: saving document #32000\n",
      "2018-04-29 17:44:09,029 : INFO : PROGRESS: saving document #33000\n",
      "2018-04-29 17:44:10,949 : INFO : PROGRESS: saving document #34000\n",
      "2018-04-29 17:44:13,506 : INFO : PROGRESS: saving document #35000\n",
      "2018-04-29 17:44:16,255 : INFO : PROGRESS: saving document #36000\n",
      "2018-04-29 17:44:18,275 : INFO : PROGRESS: saving document #37000\n",
      "2018-04-29 17:44:19,724 : INFO : PROGRESS: saving document #38000\n",
      "2018-04-29 17:44:21,680 : INFO : PROGRESS: saving document #39000\n",
      "2018-04-29 17:44:23,233 : INFO : PROGRESS: saving document #40000\n",
      "2018-04-29 17:44:24,910 : INFO : PROGRESS: saving document #41000\n",
      "2018-04-29 17:44:26,054 : INFO : PROGRESS: saving document #42000\n",
      "2018-04-29 17:44:27,265 : INFO : PROGRESS: saving document #43000\n",
      "2018-04-29 17:44:28,723 : INFO : PROGRESS: saving document #44000\n",
      "2018-04-29 17:44:30,403 : INFO : PROGRESS: saving document #45000\n",
      "2018-04-29 17:44:31,692 : INFO : PROGRESS: saving document #46000\n",
      "2018-04-29 17:44:32,917 : INFO : PROGRESS: saving document #47000\n",
      "2018-04-29 17:44:34,745 : INFO : PROGRESS: saving document #48000\n",
      "2018-04-29 17:44:36,550 : INFO : PROGRESS: saving document #49000\n",
      "2018-04-29 17:44:38,434 : INFO : PROGRESS: saving document #50000\n",
      "2018-04-29 17:44:39,955 : INFO : PROGRESS: saving document #51000\n",
      "2018-04-29 17:44:41,732 : INFO : PROGRESS: saving document #52000\n",
      "2018-04-29 17:44:43,422 : INFO : PROGRESS: saving document #53000\n",
      "2018-04-29 17:44:45,008 : INFO : PROGRESS: saving document #54000\n",
      "2018-04-29 17:44:47,171 : INFO : PROGRESS: saving document #55000\n",
      "2018-04-29 17:44:48,971 : INFO : PROGRESS: saving document #56000\n",
      "2018-04-29 17:44:51,001 : INFO : PROGRESS: saving document #57000\n",
      "2018-04-29 17:44:53,122 : INFO : PROGRESS: saving document #58000\n",
      "2018-04-29 17:44:55,017 : INFO : PROGRESS: saving document #59000\n",
      "2018-04-29 17:44:56,145 : INFO : PROGRESS: saving document #60000\n",
      "2018-04-29 17:44:57,865 : INFO : PROGRESS: saving document #61000\n",
      "2018-04-29 17:44:59,218 : INFO : PROGRESS: saving document #62000\n",
      "2018-04-29 17:45:00,664 : INFO : PROGRESS: saving document #63000\n",
      "2018-04-29 17:45:02,157 : INFO : PROGRESS: saving document #64000\n",
      "2018-04-29 17:45:04,112 : INFO : PROGRESS: saving document #65000\n",
      "2018-04-29 17:45:05,737 : INFO : PROGRESS: saving document #66000\n",
      "2018-04-29 17:45:07,474 : INFO : PROGRESS: saving document #67000\n",
      "2018-04-29 17:45:09,249 : INFO : PROGRESS: saving document #68000\n",
      "2018-04-29 17:45:10,522 : INFO : PROGRESS: saving document #69000\n",
      "2018-04-29 17:45:12,127 : INFO : PROGRESS: saving document #70000\n",
      "2018-04-29 17:45:14,027 : INFO : PROGRESS: saving document #71000\n",
      "2018-04-29 17:45:16,034 : INFO : PROGRESS: saving document #72000\n",
      "2018-04-29 17:45:17,941 : INFO : PROGRESS: saving document #73000\n",
      "2018-04-29 17:45:19,689 : INFO : PROGRESS: saving document #74000\n",
      "2018-04-29 17:45:21,294 : INFO : PROGRESS: saving document #75000\n",
      "2018-04-29 17:45:23,058 : INFO : PROGRESS: saving document #76000\n",
      "2018-04-29 17:45:24,726 : INFO : PROGRESS: saving document #77000\n",
      "2018-04-29 17:45:26,654 : INFO : PROGRESS: saving document #78000\n",
      "2018-04-29 17:45:28,637 : INFO : PROGRESS: saving document #79000\n",
      "2018-04-29 17:45:30,470 : INFO : PROGRESS: saving document #80000\n",
      "2018-04-29 17:45:32,361 : INFO : PROGRESS: saving document #81000\n",
      "2018-04-29 17:45:34,453 : INFO : PROGRESS: saving document #82000\n",
      "2018-04-29 17:45:36,210 : INFO : PROGRESS: saving document #83000\n",
      "2018-04-29 17:45:38,433 : INFO : PROGRESS: saving document #84000\n",
      "2018-04-29 17:45:40,626 : INFO : PROGRESS: saving document #85000\n",
      "2018-04-29 17:45:42,657 : INFO : PROGRESS: saving document #86000\n",
      "2018-04-29 17:45:44,590 : INFO : PROGRESS: saving document #87000\n",
      "2018-04-29 17:45:46,329 : INFO : PROGRESS: saving document #88000\n",
      "2018-04-29 17:45:48,491 : INFO : PROGRESS: saving document #89000\n",
      "2018-04-29 17:45:50,586 : INFO : PROGRESS: saving document #90000\n",
      "2018-04-29 17:45:52,740 : INFO : PROGRESS: saving document #91000\n",
      "2018-04-29 17:45:55,114 : INFO : PROGRESS: saving document #92000\n",
      "2018-04-29 17:45:57,223 : INFO : PROGRESS: saving document #93000\n",
      "2018-04-29 17:45:59,375 : INFO : PROGRESS: saving document #94000\n",
      "2018-04-29 17:46:01,638 : INFO : PROGRESS: saving document #95000\n",
      "2018-04-29 17:46:03,895 : INFO : PROGRESS: saving document #96000\n",
      "2018-04-29 17:46:06,468 : INFO : PROGRESS: saving document #97000\n",
      "2018-04-29 17:46:08,799 : INFO : PROGRESS: saving document #98000\n",
      "2018-04-29 17:46:10,978 : INFO : PROGRESS: saving document #99000\n",
      "2018-04-29 17:46:13,354 : INFO : PROGRESS: saving document #100000\n",
      "2018-04-29 17:46:15,359 : INFO : PROGRESS: saving document #101000\n",
      "2018-04-29 17:46:17,418 : INFO : PROGRESS: saving document #102000\n",
      "2018-04-29 17:46:19,750 : INFO : PROGRESS: saving document #103000\n",
      "2018-04-29 17:46:21,969 : INFO : PROGRESS: saving document #104000\n",
      "2018-04-29 17:46:24,430 : INFO : PROGRESS: saving document #105000\n",
      "2018-04-29 17:46:27,069 : INFO : PROGRESS: saving document #106000\n",
      "2018-04-29 17:46:29,583 : INFO : PROGRESS: saving document #107000\n",
      "2018-04-29 17:46:32,073 : INFO : PROGRESS: saving document #108000\n",
      "2018-04-29 17:46:33,664 : INFO : PROGRESS: saving document #109000\n",
      "2018-04-29 17:46:35,308 : INFO : PROGRESS: saving document #110000\n",
      "2018-04-29 17:46:36,594 : INFO : PROGRESS: saving document #111000\n",
      "2018-04-29 17:46:37,646 : INFO : PROGRESS: saving document #112000\n",
      "2018-04-29 17:46:38,944 : INFO : PROGRESS: saving document #113000\n",
      "2018-04-29 17:46:40,381 : INFO : PROGRESS: saving document #114000\n",
      "2018-04-29 17:46:41,476 : INFO : PROGRESS: saving document #115000\n",
      "2018-04-29 17:46:42,338 : INFO : PROGRESS: saving document #116000\n",
      "2018-04-29 17:46:43,705 : INFO : PROGRESS: saving document #117000\n",
      "2018-04-29 17:46:45,176 : INFO : PROGRESS: saving document #118000\n",
      "2018-04-29 17:46:46,605 : INFO : PROGRESS: saving document #119000\n",
      "2018-04-29 17:46:47,975 : INFO : PROGRESS: saving document #120000\n",
      "2018-04-29 17:46:49,513 : INFO : PROGRESS: saving document #121000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:46:51,044 : INFO : PROGRESS: saving document #122000\n",
      "2018-04-29 17:46:53,084 : INFO : PROGRESS: saving document #123000\n",
      "2018-04-29 17:46:55,276 : INFO : PROGRESS: saving document #124000\n",
      "2018-04-29 17:46:57,913 : INFO : PROGRESS: saving document #125000\n",
      "2018-04-29 17:47:00,236 : INFO : PROGRESS: saving document #126000\n",
      "2018-04-29 17:47:02,136 : INFO : PROGRESS: saving document #127000\n",
      "2018-04-29 17:47:03,663 : INFO : PROGRESS: saving document #128000\n",
      "2018-04-29 17:47:05,237 : INFO : PROGRESS: saving document #129000\n",
      "2018-04-29 17:47:06,445 : INFO : PROGRESS: saving document #130000\n",
      "2018-04-29 17:47:07,560 : INFO : PROGRESS: saving document #131000\n",
      "2018-04-29 17:47:08,696 : INFO : PROGRESS: saving document #132000\n",
      "2018-04-29 17:47:09,816 : INFO : PROGRESS: saving document #133000\n",
      "2018-04-29 17:47:11,106 : INFO : PROGRESS: saving document #134000\n",
      "2018-04-29 17:47:12,167 : INFO : PROGRESS: saving document #135000\n",
      "2018-04-29 17:47:13,224 : INFO : PROGRESS: saving document #136000\n",
      "2018-04-29 17:47:15,179 : INFO : PROGRESS: saving document #137000\n",
      "2018-04-29 17:47:16,458 : INFO : PROGRESS: saving document #138000\n",
      "2018-04-29 17:47:17,617 : INFO : PROGRESS: saving document #139000\n",
      "2018-04-29 17:47:18,725 : INFO : PROGRESS: saving document #140000\n",
      "2018-04-29 17:47:19,953 : INFO : PROGRESS: saving document #141000\n",
      "2018-04-29 17:47:21,548 : INFO : PROGRESS: saving document #142000\n",
      "2018-04-29 17:47:23,046 : INFO : PROGRESS: saving document #143000\n",
      "2018-04-29 17:47:24,798 : INFO : PROGRESS: saving document #144000\n",
      "2018-04-29 17:47:26,467 : INFO : PROGRESS: saving document #145000\n",
      "2018-04-29 17:47:27,976 : INFO : PROGRESS: saving document #146000\n",
      "2018-04-29 17:47:29,117 : INFO : PROGRESS: saving document #147000\n",
      "2018-04-29 17:47:30,597 : INFO : PROGRESS: saving document #148000\n",
      "2018-04-29 17:47:31,737 : INFO : PROGRESS: saving document #149000\n",
      "2018-04-29 17:47:33,084 : INFO : PROGRESS: saving document #150000\n",
      "2018-04-29 17:47:34,938 : INFO : PROGRESS: saving document #151000\n",
      "2018-04-29 17:47:36,525 : INFO : PROGRESS: saving document #152000\n",
      "2018-04-29 17:47:38,168 : INFO : PROGRESS: saving document #153000\n",
      "2018-04-29 17:47:39,007 : INFO : PROGRESS: saving document #154000\n",
      "2018-04-29 17:47:40,391 : INFO : PROGRESS: saving document #155000\n",
      "2018-04-29 17:47:42,198 : INFO : PROGRESS: saving document #156000\n",
      "2018-04-29 17:47:44,298 : INFO : PROGRESS: saving document #157000\n",
      "2018-04-29 17:47:46,215 : INFO : PROGRESS: saving document #158000\n",
      "2018-04-29 17:47:47,729 : INFO : PROGRESS: saving document #159000\n",
      "2018-04-29 17:47:49,113 : INFO : PROGRESS: saving document #160000\n",
      "2018-04-29 17:47:50,757 : INFO : PROGRESS: saving document #161000\n",
      "2018-04-29 17:47:52,448 : INFO : PROGRESS: saving document #162000\n",
      "2018-04-29 17:47:54,541 : INFO : PROGRESS: saving document #163000\n",
      "2018-04-29 17:47:56,732 : INFO : PROGRESS: saving document #164000\n",
      "2018-04-29 17:47:58,263 : INFO : PROGRESS: saving document #165000\n",
      "2018-04-29 17:47:59,646 : INFO : PROGRESS: saving document #166000\n",
      "2018-04-29 17:48:00,929 : INFO : PROGRESS: saving document #167000\n",
      "2018-04-29 17:48:02,275 : INFO : PROGRESS: saving document #168000\n",
      "2018-04-29 17:48:03,483 : INFO : PROGRESS: saving document #169000\n",
      "2018-04-29 17:48:05,285 : INFO : PROGRESS: saving document #170000\n",
      "2018-04-29 17:48:07,363 : INFO : PROGRESS: saving document #171000\n",
      "2018-04-29 17:48:08,940 : INFO : PROGRESS: saving document #172000\n",
      "2018-04-29 17:48:10,136 : INFO : PROGRESS: saving document #173000\n",
      "2018-04-29 17:48:12,196 : INFO : PROGRESS: saving document #174000\n",
      "2018-04-29 17:48:14,904 : INFO : PROGRESS: saving document #175000\n",
      "2018-04-29 17:48:16,885 : INFO : PROGRESS: saving document #176000\n",
      "2018-04-29 17:48:18,330 : INFO : PROGRESS: saving document #177000\n",
      "2018-04-29 17:48:19,961 : INFO : PROGRESS: saving document #178000\n",
      "2018-04-29 17:48:21,570 : INFO : PROGRESS: saving document #179000\n",
      "2018-04-29 17:48:23,258 : INFO : PROGRESS: saving document #180000\n",
      "2018-04-29 17:48:24,911 : INFO : PROGRESS: saving document #181000\n",
      "2018-04-29 17:48:26,578 : INFO : PROGRESS: saving document #182000\n",
      "2018-04-29 17:48:28,146 : INFO : PROGRESS: saving document #183000\n",
      "2018-04-29 17:48:30,629 : INFO : PROGRESS: saving document #184000\n",
      "2018-04-29 17:48:32,793 : INFO : PROGRESS: saving document #185000\n",
      "2018-04-29 17:48:34,992 : INFO : PROGRESS: saving document #186000\n",
      "2018-04-29 17:48:37,408 : INFO : PROGRESS: saving document #187000\n",
      "2018-04-29 17:48:39,748 : INFO : PROGRESS: saving document #188000\n",
      "2018-04-29 17:48:41,867 : INFO : PROGRESS: saving document #189000\n",
      "2018-04-29 17:48:44,670 : INFO : PROGRESS: saving document #190000\n",
      "2018-04-29 17:48:47,225 : INFO : PROGRESS: saving document #191000\n",
      "2018-04-29 17:48:49,384 : INFO : PROGRESS: saving document #192000\n",
      "2018-04-29 17:48:51,791 : INFO : PROGRESS: saving document #193000\n",
      "2018-04-29 17:48:54,466 : INFO : PROGRESS: saving document #194000\n",
      "2018-04-29 17:48:56,735 : INFO : PROGRESS: saving document #195000\n",
      "2018-04-29 17:48:59,390 : INFO : PROGRESS: saving document #196000\n",
      "2018-04-29 17:49:01,526 : INFO : PROGRESS: saving document #197000\n",
      "2018-04-29 17:49:03,521 : INFO : PROGRESS: saving document #198000\n",
      "2018-04-29 17:49:05,841 : INFO : PROGRESS: saving document #199000\n",
      "2018-04-29 17:49:08,551 : INFO : PROGRESS: saving document #200000\n",
      "2018-04-29 17:49:10,662 : INFO : PROGRESS: saving document #201000\n",
      "2018-04-29 17:49:13,312 : INFO : PROGRESS: saving document #202000\n",
      "2018-04-29 17:49:15,714 : INFO : PROGRESS: saving document #203000\n",
      "2018-04-29 17:49:17,651 : INFO : PROGRESS: saving document #204000\n",
      "2018-04-29 17:49:20,088 : INFO : PROGRESS: saving document #205000\n",
      "2018-04-29 17:49:22,293 : INFO : PROGRESS: saving document #206000\n",
      "2018-04-29 17:49:24,790 : INFO : PROGRESS: saving document #207000\n",
      "2018-04-29 17:49:26,481 : INFO : PROGRESS: saving document #208000\n",
      "2018-04-29 17:49:28,775 : INFO : PROGRESS: saving document #209000\n",
      "2018-04-29 17:49:31,108 : INFO : PROGRESS: saving document #210000\n",
      "2018-04-29 17:49:33,720 : INFO : PROGRESS: saving document #211000\n",
      "2018-04-29 17:49:36,305 : INFO : PROGRESS: saving document #212000\n",
      "2018-04-29 17:49:38,918 : INFO : PROGRESS: saving document #213000\n",
      "2018-04-29 17:49:41,292 : INFO : PROGRESS: saving document #214000\n",
      "2018-04-29 17:49:43,837 : INFO : PROGRESS: saving document #215000\n",
      "2018-04-29 17:49:45,778 : INFO : PROGRESS: saving document #216000\n",
      "2018-04-29 17:49:48,159 : INFO : PROGRESS: saving document #217000\n",
      "2018-04-29 17:49:49,462 : INFO : PROGRESS: saving document #218000\n",
      "2018-04-29 17:49:50,526 : INFO : PROGRESS: saving document #219000\n",
      "2018-04-29 17:49:51,820 : INFO : PROGRESS: saving document #220000\n",
      "2018-04-29 17:49:53,095 : INFO : PROGRESS: saving document #221000\n",
      "2018-04-29 17:49:54,218 : INFO : PROGRESS: saving document #222000\n",
      "2018-04-29 17:49:55,325 : INFO : PROGRESS: saving document #223000\n",
      "2018-04-29 17:49:55,922 : INFO : PROGRESS: saving document #224000\n",
      "2018-04-29 17:49:56,472 : INFO : PROGRESS: saving document #225000\n",
      "2018-04-29 17:49:56,939 : INFO : PROGRESS: saving document #226000\n",
      "2018-04-29 17:49:57,413 : INFO : PROGRESS: saving document #227000\n",
      "2018-04-29 17:49:57,909 : INFO : PROGRESS: saving document #228000\n",
      "2018-04-29 17:49:58,415 : INFO : PROGRESS: saving document #229000\n",
      "2018-04-29 17:49:58,908 : INFO : PROGRESS: saving document #230000\n",
      "2018-04-29 17:49:59,407 : INFO : PROGRESS: saving document #231000\n",
      "2018-04-29 17:49:59,906 : INFO : PROGRESS: saving document #232000\n",
      "2018-04-29 17:50:00,410 : INFO : PROGRESS: saving document #233000\n",
      "2018-04-29 17:50:00,886 : INFO : PROGRESS: saving document #234000\n",
      "2018-04-29 17:50:01,372 : INFO : PROGRESS: saving document #235000\n",
      "2018-04-29 17:50:01,861 : INFO : PROGRESS: saving document #236000\n",
      "2018-04-29 17:50:02,344 : INFO : PROGRESS: saving document #237000\n",
      "2018-04-29 17:50:02,819 : INFO : PROGRESS: saving document #238000\n",
      "2018-04-29 17:50:03,287 : INFO : PROGRESS: saving document #239000\n",
      "2018-04-29 17:50:03,742 : INFO : PROGRESS: saving document #240000\n",
      "2018-04-29 17:50:04,201 : INFO : PROGRESS: saving document #241000\n",
      "2018-04-29 17:50:04,839 : INFO : PROGRESS: saving document #242000\n",
      "2018-04-29 17:50:05,451 : INFO : PROGRESS: saving document #243000\n",
      "2018-04-29 17:50:06,113 : INFO : PROGRESS: saving document #244000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:50:06,732 : INFO : PROGRESS: saving document #245000\n",
      "2018-04-29 17:50:07,299 : INFO : PROGRESS: saving document #246000\n",
      "2018-04-29 17:50:07,982 : INFO : PROGRESS: saving document #247000\n",
      "2018-04-29 17:50:08,613 : INFO : PROGRESS: saving document #248000\n",
      "2018-04-29 17:50:09,242 : INFO : PROGRESS: saving document #249000\n",
      "2018-04-29 17:50:09,958 : INFO : PROGRESS: saving document #250000\n",
      "2018-04-29 17:50:10,650 : INFO : PROGRESS: saving document #251000\n",
      "2018-04-29 17:50:11,287 : INFO : PROGRESS: saving document #252000\n",
      "2018-04-29 17:50:11,870 : INFO : PROGRESS: saving document #253000\n",
      "2018-04-29 17:50:12,399 : INFO : PROGRESS: saving document #254000\n",
      "2018-04-29 17:50:12,912 : INFO : PROGRESS: saving document #255000\n",
      "2018-04-29 17:50:13,375 : INFO : PROGRESS: saving document #256000\n",
      "2018-04-29 17:50:13,831 : INFO : PROGRESS: saving document #257000\n",
      "2018-04-29 17:50:14,277 : INFO : PROGRESS: saving document #258000\n",
      "2018-04-29 17:50:14,731 : INFO : PROGRESS: saving document #259000\n",
      "2018-04-29 17:50:15,179 : INFO : PROGRESS: saving document #260000\n",
      "2018-04-29 17:50:15,641 : INFO : PROGRESS: saving document #261000\n",
      "2018-04-29 17:50:16,105 : INFO : PROGRESS: saving document #262000\n",
      "2018-04-29 17:50:16,582 : INFO : PROGRESS: saving document #263000\n",
      "2018-04-29 17:50:17,054 : INFO : PROGRESS: saving document #264000\n",
      "2018-04-29 17:50:17,525 : INFO : PROGRESS: saving document #265000\n",
      "2018-04-29 17:50:18,008 : INFO : PROGRESS: saving document #266000\n",
      "2018-04-29 17:50:18,516 : INFO : PROGRESS: saving document #267000\n",
      "2018-04-29 17:50:19,018 : INFO : PROGRESS: saving document #268000\n",
      "2018-04-29 17:50:19,496 : INFO : PROGRESS: saving document #269000\n",
      "2018-04-29 17:50:20,062 : INFO : PROGRESS: saving document #270000\n",
      "2018-04-29 17:50:20,808 : INFO : PROGRESS: saving document #271000\n",
      "2018-04-29 17:50:21,453 : INFO : PROGRESS: saving document #272000\n",
      "2018-04-29 17:50:22,259 : INFO : PROGRESS: saving document #273000\n",
      "2018-04-29 17:50:22,892 : INFO : PROGRESS: saving document #274000\n",
      "2018-04-29 17:50:23,487 : INFO : PROGRESS: saving document #275000\n",
      "2018-04-29 17:50:24,326 : INFO : PROGRESS: saving document #276000\n",
      "2018-04-29 17:50:25,011 : INFO : PROGRESS: saving document #277000\n",
      "2018-04-29 17:50:25,679 : INFO : PROGRESS: saving document #278000\n",
      "2018-04-29 17:50:26,346 : INFO : PROGRESS: saving document #279000\n",
      "2018-04-29 17:50:27,013 : INFO : PROGRESS: saving document #280000\n",
      "2018-04-29 17:50:27,669 : INFO : PROGRESS: saving document #281000\n",
      "2018-04-29 17:50:28,292 : INFO : PROGRESS: saving document #282000\n",
      "2018-04-29 17:50:28,864 : INFO : PROGRESS: saving document #283000\n",
      "2018-04-29 17:50:29,353 : INFO : PROGRESS: saving document #284000\n",
      "2018-04-29 17:50:29,890 : INFO : PROGRESS: saving document #285000\n",
      "2018-04-29 17:50:30,422 : INFO : PROGRESS: saving document #286000\n",
      "2018-04-29 17:50:30,914 : INFO : PROGRESS: saving document #287000\n",
      "2018-04-29 17:50:31,389 : INFO : PROGRESS: saving document #288000\n",
      "2018-04-29 17:50:32,088 : INFO : PROGRESS: saving document #289000\n",
      "2018-04-29 17:50:33,105 : INFO : PROGRESS: saving document #290000\n",
      "2018-04-29 17:50:34,024 : INFO : PROGRESS: saving document #291000\n",
      "2018-04-29 17:50:34,998 : INFO : PROGRESS: saving document #292000\n",
      "2018-04-29 17:50:36,008 : INFO : PROGRESS: saving document #293000\n",
      "2018-04-29 17:50:36,934 : INFO : PROGRESS: saving document #294000\n",
      "2018-04-29 17:50:37,941 : INFO : PROGRESS: saving document #295000\n",
      "2018-04-29 17:50:38,696 : INFO : PROGRESS: saving document #296000\n",
      "2018-04-29 17:50:39,584 : INFO : PROGRESS: saving document #297000\n",
      "2018-04-29 17:50:40,402 : INFO : PROGRESS: saving document #298000\n",
      "2018-04-29 17:50:41,389 : INFO : PROGRESS: saving document #299000\n",
      "2018-04-29 17:50:42,212 : INFO : PROGRESS: saving document #300000\n",
      "2018-04-29 17:50:43,201 : INFO : PROGRESS: saving document #301000\n",
      "2018-04-29 17:50:43,976 : INFO : PROGRESS: saving document #302000\n",
      "2018-04-29 17:50:44,723 : INFO : PROGRESS: saving document #303000\n",
      "2018-04-29 17:50:45,613 : INFO : PROGRESS: saving document #304000\n",
      "2018-04-29 17:50:46,423 : INFO : PROGRESS: saving document #305000\n",
      "2018-04-29 17:50:47,396 : INFO : PROGRESS: saving document #306000\n",
      "2018-04-29 17:50:48,438 : INFO : PROGRESS: saving document #307000\n",
      "2018-04-29 17:50:49,629 : INFO : PROGRESS: saving document #308000\n",
      "2018-04-29 17:50:50,519 : INFO : PROGRESS: saving document #309000\n",
      "2018-04-29 17:50:51,986 : INFO : PROGRESS: saving document #310000\n",
      "2018-04-29 17:50:53,297 : INFO : PROGRESS: saving document #311000\n",
      "2018-04-29 17:50:54,619 : INFO : PROGRESS: saving document #312000\n",
      "2018-04-29 17:50:55,857 : INFO : PROGRESS: saving document #313000\n",
      "2018-04-29 17:50:56,880 : INFO : PROGRESS: saving document #314000\n",
      "2018-04-29 17:50:57,839 : INFO : PROGRESS: saving document #315000\n",
      "2018-04-29 17:50:58,994 : INFO : PROGRESS: saving document #316000\n",
      "2018-04-29 17:50:59,906 : INFO : PROGRESS: saving document #317000\n",
      "2018-04-29 17:51:00,765 : INFO : PROGRESS: saving document #318000\n",
      "2018-04-29 17:51:01,790 : INFO : PROGRESS: saving document #319000\n",
      "2018-04-29 17:51:03,028 : INFO : PROGRESS: saving document #320000\n",
      "2018-04-29 17:51:04,802 : INFO : PROGRESS: saving document #321000\n",
      "2018-04-29 17:51:08,035 : INFO : PROGRESS: saving document #322000\n",
      "2018-04-29 17:51:11,814 : INFO : PROGRESS: saving document #323000\n",
      "2018-04-29 17:51:15,080 : INFO : PROGRESS: saving document #324000\n",
      "2018-04-29 17:51:18,317 : INFO : PROGRESS: saving document #325000\n",
      "2018-04-29 17:51:21,278 : INFO : PROGRESS: saving document #326000\n",
      "2018-04-29 17:51:24,358 : INFO : PROGRESS: saving document #327000\n",
      "2018-04-29 17:51:27,892 : INFO : PROGRESS: saving document #328000\n",
      "2018-04-29 17:51:31,508 : INFO : PROGRESS: saving document #329000\n",
      "2018-04-29 17:51:35,162 : INFO : PROGRESS: saving document #330000\n",
      "2018-04-29 17:51:38,513 : INFO : PROGRESS: saving document #331000\n",
      "2018-04-29 17:51:41,828 : INFO : PROGRESS: saving document #332000\n",
      "2018-04-29 17:51:44,776 : INFO : PROGRESS: saving document #333000\n",
      "2018-04-29 17:51:47,429 : INFO : PROGRESS: saving document #334000\n",
      "2018-04-29 17:51:49,431 : INFO : PROGRESS: saving document #335000\n",
      "2018-04-29 17:51:51,453 : INFO : PROGRESS: saving document #336000\n",
      "2018-04-29 17:51:53,450 : INFO : PROGRESS: saving document #337000\n",
      "2018-04-29 17:51:55,760 : INFO : PROGRESS: saving document #338000\n",
      "2018-04-29 17:51:58,352 : INFO : PROGRESS: saving document #339000\n",
      "2018-04-29 17:52:00,858 : INFO : PROGRESS: saving document #340000\n",
      "2018-04-29 17:52:03,585 : INFO : PROGRESS: saving document #341000\n",
      "2018-04-29 17:52:06,418 : INFO : PROGRESS: saving document #342000\n",
      "2018-04-29 17:52:09,736 : INFO : PROGRESS: saving document #343000\n",
      "2018-04-29 17:52:12,598 : INFO : PROGRESS: saving document #344000\n",
      "2018-04-29 17:52:15,083 : INFO : PROGRESS: saving document #345000\n",
      "2018-04-29 17:52:17,086 : INFO : PROGRESS: saving document #346000\n",
      "2018-04-29 17:52:19,275 : INFO : PROGRESS: saving document #347000\n",
      "2018-04-29 17:52:21,692 : INFO : PROGRESS: saving document #348000\n",
      "2018-04-29 17:52:23,874 : INFO : PROGRESS: saving document #349000\n",
      "2018-04-29 17:52:26,469 : INFO : PROGRESS: saving document #350000\n",
      "2018-04-29 17:52:29,307 : INFO : PROGRESS: saving document #351000\n",
      "2018-04-29 17:52:32,256 : INFO : saved 351985x169716 matrix, density=0.167% (99651009/59737486260)\n",
      "2018-04-29 17:52:32,278 : INFO : saving MmCorpus index to corpus/tfidf_lem10.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 21s, sys: 3.45 s, total: 9min 24s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tfidf wrapper coprus\n",
    "# doesn't take time - yield\n",
    "tfidf_corpus = tfidf[corpus]\n",
    "\n",
    "# actual tfidf corpus serialized\n",
    "corpora.MmCorpus.serialize('corpus/tfidf_lem10.mm', tfidf_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:52:32,381 : INFO : loaded corpus index from corpus/tfidf_lem10.mm.index\n",
      "2018-04-29 17:52:32,382 : INFO : initializing cython corpus reader from corpus/tfidf_lem10.mm\n",
      "2018-04-29 17:52:32,382 : INFO : accepted corpus with 351985 documents, 169716 features, 99651009 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "##################################### load tfidf corpus\n",
    "tfidf_corpus = corpora.MmCorpus('corpus/tfidf_lem10.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:52:32,562 : INFO : starting similarity index under py_models/tfidf_lem10.index\n",
      "2018-04-29 17:52:37,110 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:52:42,448 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:52:47,068 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:52:48,356 : INFO : creating sparse index\n",
      "2018-04-29 17:52:48,356 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:52:48,357 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:52:53,527 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:52:59,000 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:53:04,175 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:53:05,426 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 10888040 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:53:05,427 : INFO : creating sparse shard #0\n",
      "2018-04-29 17:53:05,428 : INFO : saving index shard to py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:53:05,428 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.0, separately None\n",
      "2018-04-29 17:53:05,429 : INFO : storing scipy.sparse array 'index' under py_models/tfidf_lem10.index.0.index.npy\n",
      "2018-04-29 17:53:05,465 : INFO : saved py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:53:05,465 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:53:05,466 : INFO : loading index from py_models/tfidf_lem10.index.0.index.npy with mmap=r\n",
      "2018-04-29 17:53:05,467 : INFO : loaded py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:53:05,501 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:53:10,101 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:53:14,895 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:53:19,322 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:53:20,548 : INFO : creating sparse index\n",
      "2018-04-29 17:53:20,549 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:53:20,550 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:53:25,614 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:53:30,043 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:53:34,781 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:53:36,076 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 10028994 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:53:36,076 : INFO : creating sparse shard #1\n",
      "2018-04-29 17:53:36,077 : INFO : saving index shard to py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:53:36,077 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.1, separately None\n",
      "2018-04-29 17:53:36,138 : INFO : saved py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:53:36,139 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:53:36,195 : INFO : loaded py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:53:36,229 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:53:40,914 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:53:46,235 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:53:51,099 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:53:52,486 : INFO : creating sparse index\n",
      "2018-04-29 17:53:52,486 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:53:52,488 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:53:57,197 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:54:02,540 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:54:08,286 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:54:10,188 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 11396165 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:54:10,189 : INFO : creating sparse shard #2\n",
      "2018-04-29 17:54:10,189 : INFO : saving index shard to py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:54:10,190 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.2, separately None\n",
      "2018-04-29 17:54:10,190 : INFO : storing scipy.sparse array 'index' under py_models/tfidf_lem10.index.2.index.npy\n",
      "2018-04-29 17:54:10,227 : INFO : saved py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:54:10,227 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:54:10,228 : INFO : loading index from py_models/tfidf_lem10.index.2.index.npy with mmap=r\n",
      "2018-04-29 17:54:10,229 : INFO : loaded py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:54:10,269 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:54:15,525 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:54:19,261 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:54:23,903 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:54:24,968 : INFO : creating sparse index\n",
      "2018-04-29 17:54:24,969 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:54:24,970 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:54:31,181 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:54:34,700 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:54:39,721 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:54:40,685 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 10255122 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:54:40,686 : INFO : creating sparse shard #3\n",
      "2018-04-29 17:54:40,686 : INFO : saving index shard to py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:54:40,687 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.3, separately None\n",
      "2018-04-29 17:54:40,752 : INFO : saved py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:54:40,752 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:54:40,810 : INFO : loaded py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:54:40,844 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:54:45,431 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:54:49,469 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:54:53,711 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:54:55,015 : INFO : creating sparse index\n",
      "2018-04-29 17:54:55,015 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:54:55,016 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:54:58,200 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:55:02,282 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:55:06,546 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:55:08,064 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 8423799 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:55:08,064 : INFO : creating sparse shard #4\n",
      "2018-04-29 17:55:08,065 : INFO : saving index shard to py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:55:08,066 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.4, separately None\n",
      "2018-04-29 17:55:08,116 : INFO : saved py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:55:08,117 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:55:08,163 : INFO : loaded py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:55:08,200 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:55:12,803 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:55:17,829 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:55:23,308 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:55:24,787 : INFO : creating sparse index\n",
      "2018-04-29 17:55:24,788 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:55:24,790 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:55:29,050 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:55:34,237 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:55:40,827 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:55:42,627 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 11418531 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:55:42,628 : INFO : creating sparse shard #5\n",
      "2018-04-29 17:55:42,628 : INFO : saving index shard to py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:55:42,629 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.5, separately None\n",
      "2018-04-29 17:55:42,629 : INFO : storing scipy.sparse array 'index' under py_models/tfidf_lem10.index.5.index.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:55:42,667 : INFO : saved py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:55:42,667 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:55:42,668 : INFO : loading index from py_models/tfidf_lem10.index.5.index.npy with mmap=r\n",
      "2018-04-29 17:55:42,669 : INFO : loaded py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:55:42,703 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:55:47,822 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:55:52,892 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:55:56,233 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:55:56,985 : INFO : creating sparse index\n",
      "2018-04-29 17:55:56,986 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:55:56,987 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:56:03,380 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:56:09,667 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:56:12,513 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:56:12,882 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 10234475 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:56:12,882 : INFO : creating sparse shard #6\n",
      "2018-04-29 17:56:12,883 : INFO : saving index shard to py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:56:12,883 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.6, separately None\n",
      "2018-04-29 17:56:12,945 : INFO : saved py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:56:12,945 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:56:13,002 : INFO : loaded py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:56:13,034 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:56:15,999 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:56:18,777 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:56:21,467 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:56:22,200 : INFO : creating sparse index\n",
      "2018-04-29 17:56:22,201 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:56:22,202 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:56:23,504 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:56:25,107 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:56:26,679 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:56:27,179 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 3000776 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:56:27,179 : INFO : creating sparse shard #7\n",
      "2018-04-29 17:56:27,180 : INFO : saving index shard to py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:56:27,180 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.7, separately None\n",
      "2018-04-29 17:56:27,199 : INFO : saved py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:56:27,200 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:56:27,216 : INFO : loaded py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:56:27,244 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:56:29,978 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:56:32,842 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:56:35,963 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:56:36,862 : INFO : creating sparse index\n",
      "2018-04-29 17:56:36,863 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:56:36,864 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:56:38,300 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:56:40,119 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:56:41,932 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:56:42,664 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 3719550 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:56:42,665 : INFO : creating sparse shard #8\n",
      "2018-04-29 17:56:42,665 : INFO : saving index shard to py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:56:42,666 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.8, separately None\n",
      "2018-04-29 17:56:42,685 : INFO : saved py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:56:42,686 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:56:42,704 : INFO : loaded py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:56:42,731 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:56:46,280 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:56:49,927 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:56:54,598 : INFO : PROGRESS: fresh_shard size=30000\n",
      "2018-04-29 17:56:56,249 : INFO : creating sparse index\n",
      "2018-04-29 17:56:56,249 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:56:56,250 : INFO : PROGRESS: at document #0/32768\n",
      "2018-04-29 17:56:58,615 : INFO : PROGRESS: at document #10000/32768\n",
      "2018-04-29 17:57:01,701 : INFO : PROGRESS: at document #20000/32768\n",
      "2018-04-29 17:57:07,222 : INFO : PROGRESS: at document #30000/32768\n",
      "2018-04-29 17:57:09,613 : INFO : created <32768x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 8673888 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:57:09,614 : INFO : creating sparse shard #9\n",
      "2018-04-29 17:57:09,615 : INFO : saving index shard to py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:09,616 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.9, separately None\n",
      "2018-04-29 17:57:09,670 : INFO : saved py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:09,671 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:09,720 : INFO : loaded py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:09,750 : INFO : PROGRESS: fresh_shard size=0\n",
      "2018-04-29 17:57:15,557 : INFO : PROGRESS: fresh_shard size=10000\n",
      "2018-04-29 17:57:20,908 : INFO : PROGRESS: fresh_shard size=20000\n",
      "2018-04-29 17:57:23,240 : INFO : creating sparse index\n",
      "2018-04-29 17:57:23,241 : INFO : creating sparse matrix from corpus\n",
      "2018-04-29 17:57:23,243 : INFO : PROGRESS: at document #0/24305\n",
      "2018-04-29 17:57:30,959 : INFO : PROGRESS: at document #10000/24305\n",
      "2018-04-29 17:57:37,986 : INFO : PROGRESS: at document #20000/24305\n",
      "2018-04-29 17:57:41,048 : INFO : created <24305x169716 sparse matrix of type '<type 'numpy.float32'>'\n",
      "\twith 11611669 stored elements in Compressed Sparse Row format>\n",
      "2018-04-29 17:57:41,049 : INFO : creating sparse shard #10\n",
      "2018-04-29 17:57:41,049 : INFO : saving index shard to py_models/tfidf_lem10.index.10\n",
      "2018-04-29 17:57:41,050 : INFO : saving SparseMatrixSimilarity object under py_models/tfidf_lem10.index.10, separately None\n",
      "2018-04-29 17:57:41,050 : INFO : storing scipy.sparse array 'index' under py_models/tfidf_lem10.index.10.index.npy\n",
      "2018-04-29 17:57:41,087 : INFO : saved py_models/tfidf_lem10.index.10\n",
      "2018-04-29 17:57:41,088 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.10\n",
      "2018-04-29 17:57:41,088 : INFO : loading index from py_models/tfidf_lem10.index.10.index.npy with mmap=r\n",
      "2018-04-29 17:57:41,090 : INFO : loaded py_models/tfidf_lem10.index.10\n",
      "2018-04-29 17:57:41,117 : INFO : saving Similarity object under py_models/TFIDF_lem10_index, separately None\n",
      "2018-04-29 17:57:41,119 : INFO : saved py_models/TFIDF_lem10_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 58s, sys: 2.48 s, total: 5min\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create index - Tfidf\n",
    "tfidf_index = gensim.similarities.Similarity('py_models/tfidf_lem10.index',tfidf_corpus,len(dictionary))\n",
    "\n",
    "# save index - tfidf\n",
    "gensim.similarities.Similarity.save(tfidf_index,'py_models/TFIDF_lem10_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:57:41,123 : INFO : loading Similarity object from py_models/TFIDF_lem10_index\n",
      "2018-04-29 17:57:41,123 : INFO : loaded py_models/TFIDF_lem10_index\n"
     ]
    }
   ],
   "source": [
    "########################### load tfidf index\n",
    "tfidf_index = gensim.similarities.Similarity.load('py_models/TFIDF_lem10_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-29 17:57:43,118 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:57:43,119 : INFO : loading index from py_models/tfidf_lem10.index.0.index.npy with mmap=r\n",
      "2018-04-29 17:57:43,120 : INFO : loaded py_models/tfidf_lem10.index.0\n",
      "2018-04-29 17:57:43,157 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:57:43,203 : INFO : loaded py_models/tfidf_lem10.index.1\n",
      "2018-04-29 17:57:43,237 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:57:43,238 : INFO : loading index from py_models/tfidf_lem10.index.2.index.npy with mmap=r\n",
      "2018-04-29 17:57:43,240 : INFO : loaded py_models/tfidf_lem10.index.2\n",
      "2018-04-29 17:57:43,282 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:57:43,338 : INFO : loaded py_models/tfidf_lem10.index.3\n",
      "2018-04-29 17:57:43,372 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:57:43,418 : INFO : loaded py_models/tfidf_lem10.index.4\n",
      "2018-04-29 17:57:43,446 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:57:43,447 : INFO : loading index from py_models/tfidf_lem10.index.5.index.npy with mmap=r\n",
      "2018-04-29 17:57:43,448 : INFO : loaded py_models/tfidf_lem10.index.5\n",
      "2018-04-29 17:57:43,488 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:57:43,545 : INFO : loaded py_models/tfidf_lem10.index.6\n",
      "2018-04-29 17:57:43,578 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:57:43,589 : INFO : loaded py_models/tfidf_lem10.index.7\n",
      "2018-04-29 17:57:43,599 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:57:43,616 : INFO : loaded py_models/tfidf_lem10.index.8\n",
      "2018-04-29 17:57:43,629 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:43,676 : INFO : loaded py_models/tfidf_lem10.index.9\n",
      "2018-04-29 17:57:43,704 : INFO : loading SparseMatrixSimilarity object from py_models/tfidf_lem10.index.10\n",
      "2018-04-29 17:57:43,704 : INFO : loading index from py_models/tfidf_lem10.index.10.index.npy with mmap=r\n",
      "2018-04-29 17:57:43,706 : INFO : loaded py_models/tfidf_lem10.index.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.72 s, sys: 240 ms, total: 6.96 s\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# calculate similarity for all \n",
    "tfidf_index.num_best= None\n",
    "i#######################         SIMILIRATIES SCORES AND RANKING    ###########################\n",
    "results = copy.deepcopy(query_truth)\n",
    "### lists of list (indexing from 1, dummy element)\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    # inside the square bracket determines query representation\n",
    "    \n",
    "    sims = tfidf_index[tfidf[dictionary.doc2bow(query)]]\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "        #x.append(common_words(filenames.index(x[0]+'.txt'), tfidf_corpus, query))\n",
    "    \n",
    "    # sort wrt relevance(from truth) and then ranks(from our model)\n",
    "    #tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[1]),x[3]))     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [['ConsumerCourt_DCDRC_100385', '1', 0.16185194, 205],\n",
       "  ['ConsumerCourt_DCDRC_106530', '0', 0.066299856, 2049],\n",
       "  ['ConsumerCourt_DCDRC_107608', '1', 0.08884313, 1231],\n",
       "  ['ConsumerCourt_DCDRC_114291', '1', 0.12928788, 532],\n",
       "  ['ConsumerCourt_DCDRC_114382', '1', 0.10607624, 893],\n",
       "  ['ConsumerCourt_DCDRC_118185', '1', 0.09642149, 1088],\n",
       "  ['ConsumerCourt_DCDRC_130318', '0', 0.1199432, 653],\n",
       "  ['ConsumerCourt_DCDRC_130570', '1', 0.27709824, 7],\n",
       "  ['ConsumerCourt_DCDRC_131146', '1', 0.06343228, 2197],\n",
       "  ['ConsumerCourt_DCDRC_131717', '1', 0.08180869, 1438],\n",
       "  ['ConsumerCourt_DCDRC_131741', '1', 0.07304419, 1746],\n",
       "  ['ConsumerCourt_DCDRC_131818', '1', 0.09741112, 1068],\n",
       "  ['ConsumerCourt_DCDRC_131950', '0', 0.05442484, 2824],\n",
       "  ['ConsumerCourt_DCDRC_131972', '0', 0.08816901, 1251],\n",
       "  ['ConsumerCourt_DCDRC_132932', '0', 0.06848099, 1925],\n",
       "  ['ConsumerCourt_DCDRC_133592', '1', 0.1699525, 155],\n",
       "  ['ConsumerCourt_DCDRC_134386', '0', 0.33104795, 1],\n",
       "  ['ConsumerCourt_DCDRC_135474', '1', 0.061556537, 2329],\n",
       "  ['ConsumerCourt_DCDRC_139024', '0', 0.18092416, 105],\n",
       "  ['ConsumerCourt_DCDRC_139205', '0', 0.2791601, 5],\n",
       "  ['ConsumerCourt_DCDRC_140039', '1', 0.25601915, 14],\n",
       "  ['ConsumerCourt_DCDRC_145708', '0', 0.23000303, 24],\n",
       "  ['ConsumerCourt_DCDRC_187214', '1', 0.055584818, 2722],\n",
       "  ['ConsumerCourt_DCDRC_207784', '1', 0.03674183, 5278],\n",
       "  ['ConsumerCourt_DCDRC_217344', '1', 0.18479005, 89],\n",
       "  ['ConsumerCourt_DCDRC_217471', '1', 0.13771655, 417],\n",
       "  ['ConsumerCourt_DCDRC_222797', '0', 0.142882, 365],\n",
       "  ['ConsumerCourt_DCDRC_222844', '1', 0.14383788, 355],\n",
       "  ['ConsumerCourt_DCDRC_224228', '0', 0.18566678, 85],\n",
       "  ['ConsumerCourt_DCDRC_224833', '0', 0.11406111, 740],\n",
       "  ['ConsumerCourt_DCDRC_226072', '1', 0.2247357, 26],\n",
       "  ['ConsumerCourt_DCDRC_38498', '0', 0.25151008, 17],\n",
       "  ['ConsumerCourt_DCDRC_39186', '1', 0.15644196, 237],\n",
       "  ['ConsumerCourt_DCDRC_41317', '1', 0.22002132, 31],\n",
       "  ['ConsumerCourt_DCDRC_41588', '1', 0.18472102, 90],\n",
       "  ['ConsumerCourt_DCDRC_42118', '1', 0.13853158, 408],\n",
       "  ['ConsumerCourt_DCDRC_42618', '1', 0.13754933, 420],\n",
       "  ['ConsumerCourt_DCDRC_42649', '1', 0.31588802, 2],\n",
       "  ['ConsumerCourt_DCDRC_44200', '1', 0.12468188, 585],\n",
       "  ['ConsumerCourt_DCDRC_46368', '1', 0.118411794, 672],\n",
       "  ['ConsumerCourt_DCDRC_46519', '1', 0.15088913, 294],\n",
       "  ['ConsumerCourt_DCDRC_46543', '1', 0.103566095, 944],\n",
       "  ['ConsumerCourt_DCDRC_53138', '1', 0.1321914, 487],\n",
       "  ['ConsumerCourt_DCDRC_55517', '1', 0.20306638, 53],\n",
       "  ['ConsumerCourt_DCDRC_55612', '1', 0.112715356, 764],\n",
       "  ['ConsumerCourt_DCDRC_55678', '1', 0.17311196, 143],\n",
       "  ['ConsumerCourt_DCDRC_55899', '1', 0.18001184, 111],\n",
       "  ['ConsumerCourt_DCDRC_57484', '0', 0.2532293, 15],\n",
       "  ['ConsumerCourt_DCDRC_74868', '1', 0.1236761, 601],\n",
       "  ['ConsumerCourt_DCDRC_80057', '1', 0.26865602, 10],\n",
       "  ['ConsumerCourt_DCDRC_83596', '0', 0.15309563, 270],\n",
       "  ['ConsumerCourt_DCDRC_83779', '0', 0.14001663, 396],\n",
       "  ['ConsumerCourt_DCDRC_96667', '1', 0.2404279, 20],\n",
       "  ['ConsumerCourt_DCDRC_98733', '1', 0.25267765, 16],\n",
       "  ['ConsumerCourt_SCDRC_52678', '0', 0.13144559, 498],\n",
       "  ['DelhiHC_2011_4267', '0', 0.10414553, 934],\n",
       "  ['KolkataHCOriginalSite_2012_36', '0', 0.033456348, 6232]],\n",
       " '10': [['ConsumerCourt_DCDRC_220958', '0', 0.09642182, 1011],\n",
       "  ['ConsumerCourt_DCDRC_222509', '0', 0.31595787, 4],\n",
       "  ['ConsumerCourt_DCDRC_223570', '0', 0.30834275, 5],\n",
       "  ['ConsumerCourt_DCDRC_227314', '0', 0.12794511, 471],\n",
       "  ['ConsumerCourt_DCDRC_228688', '1', 0.28662106, 8],\n",
       "  ['ConsumerCourt_DCDRC_235883', '0', 0.26839116, 12],\n",
       "  ['ConsumerCourt_DCDRC_38192', '0', 0.41595137, 2],\n",
       "  ['ConsumerCourt_DCDRC_41485', '0', 0.26378343, 16],\n",
       "  ['ConsumerCourt_DCDRC_41489', '0', 0.26378343, 15],\n",
       "  ['ConsumerCourt_DCDRC_41520', '0', 0.2668716, 14],\n",
       "  ['ConsumerCourt_SCDRC_44836', '0', 0.19624285, 74],\n",
       "  ['ConsumerCourt_SCDRC_56350', '0', 0.27424642, 11],\n",
       "  ['ConsumerCourt_SCDRC_65898', '0', 0.25429, 20],\n",
       "  ['ConsumerCourt_SCDRC_68718', '0', 0.25706196, 19],\n",
       "  ['ConsumerCourt_SCDRC_69937', '1', 0.5464984, 1],\n",
       "  ['ConsumerCourt_SCDRC_70632', '0', 0.105837665, 790],\n",
       "  ['DelhiHC_2008_2771', '0', 0.17349324, 140],\n",
       "  ['DelhiHC_2008_2772', '0', 0.17349324, 139]],\n",
       " '2': [['ConsumerCourt_DCDRC_103241', '0', 0.05116509, 1934],\n",
       "  ['ConsumerCourt_DCDRC_107786', '1', 0.29168144, 184],\n",
       "  ['ConsumerCourt_DCDRC_114305', '1', 0.387323, 35],\n",
       "  ['ConsumerCourt_DCDRC_120930', '1', 0.3830618, 41],\n",
       "  ['ConsumerCourt_DCDRC_123490', '1', 0.33731648, 100],\n",
       "  ['ConsumerCourt_DCDRC_125352', '1', 0.43188536, 6],\n",
       "  ['ConsumerCourt_DCDRC_127791', '0', 0.06714895, 1299],\n",
       "  ['ConsumerCourt_DCDRC_130071', '0', 0.08628778, 964],\n",
       "  ['ConsumerCourt_DCDRC_130920', '1', 0.2891528, 188],\n",
       "  ['ConsumerCourt_DCDRC_131972', '0', 0.059945945, 1496],\n",
       "  ['ConsumerCourt_DCDRC_133106', '1', 0.3951745, 31],\n",
       "  ['ConsumerCourt_DCDRC_133260', '0', 0.37977216, 43],\n",
       "  ['ConsumerCourt_DCDRC_143284', '1', 0.30487663, 149],\n",
       "  ['ConsumerCourt_DCDRC_143334', '0', 0.40973663, 18],\n",
       "  ['ConsumerCourt_DCDRC_144440', '0', 0.17394999, 476],\n",
       "  ['ConsumerCourt_DCDRC_202900', '0', 0.018241521, 36529],\n",
       "  ['ConsumerCourt_DCDRC_202903', '0', 0.01594568, 52683],\n",
       "  ['ConsumerCourt_DCDRC_207376', '1', 0.058631595, 1554],\n",
       "  ['ConsumerCourt_DCDRC_212601', '0', 0.28668, 197],\n",
       "  ['ConsumerCourt_DCDRC_212692', '1', 0.4339986, 5],\n",
       "  ['ConsumerCourt_DCDRC_214518', '1', 0.36593345, 58],\n",
       "  ['ConsumerCourt_DCDRC_226924', '0', 0.25521833, 263],\n",
       "  ['ConsumerCourt_DCDRC_227866', '1', 0.42666116, 10],\n",
       "  ['ConsumerCourt_DCDRC_228749', '1', 0.3500358, 84],\n",
       "  ['ConsumerCourt_DCDRC_229221', '0', 0.35415915, 78],\n",
       "  ['ConsumerCourt_DCDRC_230361', '1', 0.42097464, 12],\n",
       "  ['ConsumerCourt_DCDRC_41190', '0', 0.054519515, 1734],\n",
       "  ['ConsumerCourt_DCDRC_50671', '1', 0.081553325, 1017],\n",
       "  ['ConsumerCourt_DCDRC_52620', '1', 0.06963155, 1256],\n",
       "  ['ConsumerCourt_DCDRC_53093', '1', 0.3672748, 55],\n",
       "  ['ConsumerCourt_DCDRC_53138', '0', 0.040126923, 3397],\n",
       "  ['ConsumerCourt_DCDRC_53153', '1', 0.10388352, 794],\n",
       "  ['ConsumerCourt_DCDRC_53432', '0', 0.05672295, 1631],\n",
       "  ['ConsumerCourt_DCDRC_53875', '0', 0.06323096, 1391],\n",
       "  ['ConsumerCourt_DCDRC_53881', '0', 0.06323096, 1390],\n",
       "  ['ConsumerCourt_DCDRC_56471', '1', 0.19100128, 420],\n",
       "  ['ConsumerCourt_DCDRC_56761', '0', 0.04129476, 3144],\n",
       "  ['ConsumerCourt_DCDRC_57453', '0', 0.3275077, 118],\n",
       "  ['ConsumerCourt_DCDRC_57583', '0', 0.038953524, 3671],\n",
       "  ['ConsumerCourt_DCDRC_62986', '0', 0.34571543, 91],\n",
       "  ['ConsumerCourt_DCDRC_63724', '0', 0.28970197, 187],\n",
       "  ['ConsumerCourt_DCDRC_68765', '0', 0.2720819, 222],\n",
       "  ['ConsumerCourt_DCDRC_68808', '0', 0.18697113, 432],\n",
       "  ['ConsumerCourt_DCDRC_69863', '0', 0.2272508, 335],\n",
       "  ['ConsumerCourt_DCDRC_74887', '0', 0.30115408, 165],\n",
       "  ['ConsumerCourt_DCDRC_75562', '0', 0.37434208, 50],\n",
       "  ['ConsumerCourt_DCDRC_76114', '0', 0.36304817, 64],\n",
       "  ['ConsumerCourt_DCDRC_76186', '0', 0.19231616, 417],\n",
       "  ['ConsumerCourt_DCDRC_76270', '0', 0.24704473, 283],\n",
       "  ['ConsumerCourt_DCDRC_77085', '1', 0.33069566, 113],\n",
       "  ['ConsumerCourt_DCDRC_77280', '0', 0.042803638, 2897],\n",
       "  ['ConsumerCourt_DCDRC_77935', '1', 0.38617077, 37],\n",
       "  ['ConsumerCourt_DCDRC_87186', '0', 0.29542562, 178],\n",
       "  ['ConsumerCourt_DCDRC_91864', '1', 0.40101635, 25],\n",
       "  ['ConsumerCourt_DCDRC_96244', '1', 0.43737224, 4],\n",
       "  ['ConsumerCourt_NCDRC_1317', '0', 0.35751197, 73],\n",
       "  ['ConsumerCourt_SCDRC_49685', '0', 0.32498217, 123],\n",
       "  ['ConsumerCourt_SCDRC_60064', '1', 0.4747356, 1],\n",
       "  ['ConsumerCourt_SCDRC_62062', '1', 0.38968801, 33]],\n",
       " '3': [['ConsumerCourt_DCDRC_103355', '0', 0.105795614, 383],\n",
       "  ['ConsumerCourt_DCDRC_129293', '1', 0.3341779, 4],\n",
       "  ['ConsumerCourt_DCDRC_133013', '1', 0.15773119, 93],\n",
       "  ['ConsumerCourt_DCDRC_133017', '0', 0.22281633, 26],\n",
       "  ['ConsumerCourt_DCDRC_137661', '0', 0.10284864, 435],\n",
       "  ['ConsumerCourt_DCDRC_153064', '0', 0.032149572, 7621],\n",
       "  ['ConsumerCourt_DCDRC_187619', '1', 0.28784367, 9],\n",
       "  ['ConsumerCourt_DCDRC_200840', '0', 0.14488558, 136],\n",
       "  ['ConsumerCourt_DCDRC_222260', '1', 0.21672036, 28],\n",
       "  ['ConsumerCourt_DCDRC_222772', '1', 0.22915016, 22],\n",
       "  ['ConsumerCourt_DCDRC_223618', '1', 0.20526212, 36],\n",
       "  ['ConsumerCourt_DCDRC_228476', '0', 0.10687368, 371],\n",
       "  ['ConsumerCourt_DCDRC_228963', '1', 0.23827143, 20],\n",
       "  ['ConsumerCourt_DCDRC_230634', '1', 0.25547072, 15],\n",
       "  ['ConsumerCourt_DCDRC_235824', '0', 0.3343033, 3],\n",
       "  ['ConsumerCourt_DCDRC_45666', '0', 0.081762105, 863],\n",
       "  ['ConsumerCourt_DCDRC_47493', '1', 0.15302804, 104],\n",
       "  ['ConsumerCourt_DCDRC_52566', '0', 0.16868195, 70],\n",
       "  ['ConsumerCourt_DCDRC_73184', '0', 0.15768361, 94],\n",
       "  ['ConsumerCourt_DCDRC_73185', '0', 0.16764536, 72],\n",
       "  ['ConsumerCourt_NCDRC_2861', '1', 0.124772586, 214],\n",
       "  ['ConsumerCourt_NCDRC_2902', '0', 0.057073794, 2263],\n",
       "  ['ConsumerCourt_NCDRC_2979', '0', 0.102176756, 446],\n",
       "  ['ConsumerCourt_NCDRC_632', '0', 0.102791876, 437],\n",
       "  ['ConsumerCourt_NCDRC_637', '0', 0.10263641, 441],\n",
       "  ['ConsumerCourt_SCDRC_2052', '0', 0.28015804, 12],\n",
       "  ['ConsumerCourt_SCDRC_22704', '0', 0.09891514, 499],\n",
       "  ['ConsumerCourt_SCDRC_29484', '0', 0.16089258, 89],\n",
       "  ['ConsumerCourt_SCDRC_29552', '0', 0.06217157, 1855],\n",
       "  ['ConsumerCourt_SCDRC_30176', '0', 0.17636095, 60],\n",
       "  ['ConsumerCourt_SCDRC_48394', '0', 0.11484798, 295],\n",
       "  ['ConsumerCourt_SCDRC_63569', '0', 0.098075025, 505],\n",
       "  ['ConsumerCourt_SCDRC_63609', '0', 0.17358449, 64],\n",
       "  ['ConsumerCourt_SCDRC_65919', '0', 0.23171596, 21],\n",
       "  ['ConsumerCourt_SCDRC_66200', '0', 0.14707991, 128],\n",
       "  ['ConsumerCourt_SCDRC_69801', '0', 0.09011969, 652],\n",
       "  ['ConsumerCourt_SCDRC_814', '0', 0.078205794, 986],\n",
       "  ['DelhiHC_2012_388', '0', 0.043359995, 4094]],\n",
       " '4': [['ConsumerCourt_DCDRC_106570', '0', 0.24891731, 188],\n",
       "  ['ConsumerCourt_DCDRC_116129', '1', 0.25496536, 146],\n",
       "  ['ConsumerCourt_DCDRC_128836', '0', 0.1828827, 839],\n",
       "  ['ConsumerCourt_DCDRC_130725', '1', 0.2194786, 411],\n",
       "  ['ConsumerCourt_DCDRC_133137', '0', 0.25046524, 173],\n",
       "  ['ConsumerCourt_DCDRC_173376', '0', 0.025167977, 44608],\n",
       "  ['ConsumerCourt_DCDRC_174292', '0', 0.17596701, 945],\n",
       "  ['ConsumerCourt_DCDRC_174300', '1', 0.15809764, 1312],\n",
       "  ['ConsumerCourt_DCDRC_202903', '0', 0.010021221, 149658],\n",
       "  ['ConsumerCourt_DCDRC_205816', '1', 0.19839443, 610],\n",
       "  ['ConsumerCourt_DCDRC_207155', '0', 0.088249676, 6144],\n",
       "  ['ConsumerCourt_DCDRC_207197', '0', 0.2201435, 404],\n",
       "  ['ConsumerCourt_DCDRC_212467', '1', 0.26130852, 118],\n",
       "  ['ConsumerCourt_DCDRC_213156', '0', 0.2203923, 401],\n",
       "  ['ConsumerCourt_DCDRC_213473', '0', 0.23071097, 320],\n",
       "  ['ConsumerCourt_DCDRC_213784', '0', 0.26569358, 96],\n",
       "  ['ConsumerCourt_DCDRC_214475', '0', 0.23070188, 321],\n",
       "  ['ConsumerCourt_DCDRC_218582', '0', 0.16745216, 1111],\n",
       "  ['ConsumerCourt_DCDRC_221177', '0', 0.18151459, 862],\n",
       "  ['ConsumerCourt_DCDRC_221424', '0', 0.23873559, 275],\n",
       "  ['ConsumerCourt_DCDRC_223275', '0', 0.22893964, 334],\n",
       "  ['ConsumerCourt_DCDRC_226909', '0', 0.067029245, 10314],\n",
       "  ['ConsumerCourt_DCDRC_227099', '0', 0.17884895, 901],\n",
       "  ['ConsumerCourt_DCDRC_229902', '0', 0.14634013, 1654],\n",
       "  ['ConsumerCourt_DCDRC_47636', '0', 0.14041694, 1846],\n",
       "  ['ConsumerCourt_DCDRC_49131', '0', 0.3270531, 6],\n",
       "  ['ConsumerCourt_DCDRC_51881', '0', 0.28924116, 30],\n",
       "  ['ConsumerCourt_DCDRC_52357', '0', 0.31467402, 11],\n",
       "  ['ConsumerCourt_DCDRC_53202', '0', 0.29179263, 26],\n",
       "  ['ConsumerCourt_DCDRC_53580', '1', 0.28916618, 31],\n",
       "  ['ConsumerCourt_DCDRC_54056', '1', 0.2478979, 201],\n",
       "  ['ConsumerCourt_DCDRC_70122', '0', 0.26983234, 82],\n",
       "  ['ConsumerCourt_DCDRC_73309', '0', 0.26981872, 83],\n",
       "  ['ConsumerCourt_NCDRC_2154', '0', 0.060251523, 12173],\n",
       "  ['ConsumerCourt_NCDRC_3506', '0', 0.085250735, 6644],\n",
       "  ['ConsumerCourt_SCDRC_1143', '1', 0.27000868, 80],\n",
       "  ['ConsumerCourt_SCDRC_1288', '1', 0.31690243, 10],\n",
       "  ['ConsumerCourt_SCDRC_22830', '0', 0.16168913, 1227],\n",
       "  ['ConsumerCourt_SCDRC_37679', '1', 0.25058728, 171],\n",
       "  ['ConsumerCourt_SCDRC_471', '0', 0.06492559, 10853],\n",
       "  ['ConsumerCourt_SCDRC_49973', '1', 0.25199616, 165],\n",
       "  ['ConsumerCourt_SCDRC_5406', '0', 0.12062388, 2879],\n",
       "  ['ConsumerCourt_SCDRC_7046', '1', 0.33807057, 3],\n",
       "  ['DelhiHC_2012_3856', '0', 0.09199201, 5625],\n",
       "  ['SupremeCourt_2009_1421', '0', 0.18089704, 872]],\n",
       " '5': [['ConsumerCourt_DCDRC_124651', '0', 0.1380009, 1710],\n",
       "  ['ConsumerCourt_DCDRC_138545', '0', 0.21531987, 371],\n",
       "  ['ConsumerCourt_DCDRC_139183', '0', 0.29358536, 31],\n",
       "  ['ConsumerCourt_DCDRC_141380', '0', 0.10652409, 2868],\n",
       "  ['ConsumerCourt_DCDRC_148691', '1', 0.31213897, 10],\n",
       "  ['ConsumerCourt_DCDRC_202903', '0', 0.0063688266, 187527],\n",
       "  ['ConsumerCourt_DCDRC_208296', '0', 0.17331758, 921],\n",
       "  ['ConsumerCourt_DCDRC_223817', '0', 0.19273905, 602],\n",
       "  ['ConsumerCourt_DCDRC_226994', '0', 0.045381997, 9427],\n",
       "  ['ConsumerCourt_DCDRC_229911', '1', 0.22368848, 299],\n",
       "  ['ConsumerCourt_DCDRC_41155', '1', 0.25733244, 118],\n",
       "  ['ConsumerCourt_DCDRC_41262', '0', 0.2505672, 142],\n",
       "  ['ConsumerCourt_DCDRC_41428', '0', 0.22484241, 291],\n",
       "  ['ConsumerCourt_DCDRC_42411', '0', 0.13904752, 1684],\n",
       "  ['ConsumerCourt_DCDRC_46404', '0', 0.26110348, 103],\n",
       "  ['ConsumerCourt_DCDRC_53679', '1', 0.15456897, 1276],\n",
       "  ['ConsumerCourt_DCDRC_57315', '1', 0.23405942, 237],\n",
       "  ['ConsumerCourt_DCDRC_57465', '0', 0.1258907, 2075],\n",
       "  ['ConsumerCourt_DCDRC_57488', '0', 0.23475426, 231],\n",
       "  ['ConsumerCourt_DCDRC_57914', '1', 0.2681473, 83],\n",
       "  ['ConsumerCourt_DCDRC_66294', '0', 0.22213382, 307],\n",
       "  ['ConsumerCourt_DCDRC_96799', '0', 0.2952276, 29],\n",
       "  ['ConsumerCourt_NCDRC_1337', '1', 0.30042246, 24],\n",
       "  ['ConsumerCourt_NCDRC_1338', '0', 0.30042246, 23],\n",
       "  ['ConsumerCourt_NCDRC_2983', '1', 0.26576623, 88],\n",
       "  ['ConsumerCourt_SCDRC_28656', '0', 0.23637797, 223],\n",
       "  ['ConsumerCourt_SCDRC_33517', '1', 0.33069202, 3],\n",
       "  ['ConsumerCourt_SCDRC_5122', '0', 0.19560242, 572],\n",
       "  ['ConsumerCourt_SCDRC_71374', '1', 0.19175802, 620]],\n",
       " '6': [['ConsumerCourt_DCDRC_103021', '1', 0.36711788, 1],\n",
       "  ['ConsumerCourt_DCDRC_119836', '1', 0.08526744, 750],\n",
       "  ['ConsumerCourt_DCDRC_125237', '1', 0.03284732, 4500],\n",
       "  ['ConsumerCourt_DCDRC_126276', '0', 0.045523252, 2496],\n",
       "  ['ConsumerCourt_DCDRC_131266', '1', 0.1562739, 169],\n",
       "  ['ConsumerCourt_DCDRC_133353', '1', 0.09529114, 575],\n",
       "  ['ConsumerCourt_DCDRC_133354', '0', 0.08403072, 773],\n",
       "  ['ConsumerCourt_DCDRC_134804', '0', 0.08163838, 824],\n",
       "  ['ConsumerCourt_DCDRC_138617', '1', 0.12635577, 309],\n",
       "  ['ConsumerCourt_DCDRC_143983', '0', 0.025932606, 7435],\n",
       "  ['ConsumerCourt_DCDRC_207387', '1', 0.11922688, 356],\n",
       "  ['ConsumerCourt_DCDRC_220958', '1', 0.14947952, 196],\n",
       "  ['ConsumerCourt_DCDRC_223570', '1', 0.09919912, 523],\n",
       "  ['ConsumerCourt_DCDRC_223783', '0', 0.110854834, 416],\n",
       "  ['ConsumerCourt_DCDRC_38192', '1', 0.18152007, 108],\n",
       "  ['ConsumerCourt_DCDRC_41485', '0', 0.11963005, 354],\n",
       "  ['ConsumerCourt_DCDRC_41489', '0', 0.11963005, 353],\n",
       "  ['ConsumerCourt_DCDRC_41520', '1', 0.12564632, 314],\n",
       "  ['ConsumerCourt_DCDRC_45421', '0', 0.13699622, 257],\n",
       "  ['ConsumerCourt_DCDRC_52148', '0', 0.14308992, 230],\n",
       "  ['ConsumerCourt_DCDRC_70186', '0', 0.3350037, 4],\n",
       "  ['ConsumerCourt_DCDRC_70341', '0', 0.31165537, 6],\n",
       "  ['ConsumerCourt_DCDRC_90054', '0', 0.06956695, 1143],\n",
       "  ['ConsumerCourt_SCDRC_28640', '1', 0.15771793, 162],\n",
       "  ['ConsumerCourt_SCDRC_64152', '0', 0.1562502, 170],\n",
       "  ['ConsumerCourt_SCDRC_68718', '1', 0.16174522, 146],\n",
       "  ['ConsumerCourt_SCDRC_70632', '0', 0.21051668, 64],\n",
       "  ['DelhiHC_2009_735', '1', 0.049783476, 2147],\n",
       "  ['DelhiHC_2012_372', '0', 0.019550774, 13957],\n",
       "  ['DelhiHC_2012_374', '0', 0.019550774, 13956],\n",
       "  ['SupremeCourt_1996_416', '0', 0.08286074, 802]],\n",
       " '7': [['ConsumerCourt_DCDRC_119406', '1', 0.07700122, 219],\n",
       "  ['ConsumerCourt_DCDRC_121344', '0', 0.08713765, 153],\n",
       "  ['ConsumerCourt_DCDRC_127019', '0', 0.09750148, 109],\n",
       "  ['ConsumerCourt_DCDRC_130205', '0', 0.20515528, 17],\n",
       "  ['ConsumerCourt_DCDRC_207672', '1', 0.15500039, 27],\n",
       "  ['ConsumerCourt_DCDRC_222487', '0', 0.26769084, 8],\n",
       "  ['ConsumerCourt_DCDRC_224805', '0', 0.08499279, 166],\n",
       "  ['ConsumerCourt_DCDRC_235755', '0', 0.10645334, 88],\n",
       "  ['ConsumerCourt_DCDRC_236026', '1', 0.18713242, 20],\n",
       "  ['ConsumerCourt_DCDRC_236229', '0', 0.4570618, 5],\n",
       "  ['ConsumerCourt_DCDRC_33939', '0', 0.14739224, 30],\n",
       "  ['ConsumerCourt_DCDRC_44337', '1', 0.12962462, 45],\n",
       "  ['ConsumerCourt_DCDRC_45534', '0', 0.58968306, 1],\n",
       "  ['ConsumerCourt_DCDRC_46470', '0', 0.29182968, 7],\n",
       "  ['ConsumerCourt_DCDRC_48811', '0', 0.124101706, 53],\n",
       "  ['ConsumerCourt_DCDRC_51454', '1', 0.121700026, 58],\n",
       "  ['ConsumerCourt_DCDRC_64180', '0', 0.58522964, 2],\n",
       "  ['ConsumerCourt_DCDRC_73953', '0', 0.13533789, 38],\n",
       "  ['ConsumerCourt_DCDRC_75554', '0', 0.0827841, 178],\n",
       "  ['ConsumerCourt_SCDRC_1050', '0', 0.12768492, 49],\n",
       "  ['ConsumerCourt_SCDRC_1387', '0', 0.5129691, 4],\n",
       "  ['ConsumerCourt_SCDRC_1418', '0', 0.34952796, 6],\n",
       "  ['ConsumerCourt_SCDRC_65323', '0', 0.15895015, 24],\n",
       "  ['ConsumerCourt_SCDRC_66203', '0', 0.035691578, 1494],\n",
       "  ['DelhiHC_2012_2026', '0', 0.22411771, 9]],\n",
       " '8': [['ConsumerCourt_DCDRC_101057', '1', 0.10931648, 18],\n",
       "  ['ConsumerCourt_DCDRC_118168', '0', 0.034153078, 1818],\n",
       "  ['ConsumerCourt_DCDRC_118989', '0', 0.10121461, 25],\n",
       "  ['ConsumerCourt_DCDRC_120003', '1', 0.047669776, 469],\n",
       "  ['ConsumerCourt_DCDRC_120004', '1', 0.048152998, 449],\n",
       "  ['ConsumerCourt_DCDRC_121058', '0', 0.049880885, 387],\n",
       "  ['ConsumerCourt_DCDRC_125286', '0', 0.05475113, 258],\n",
       "  ['ConsumerCourt_DCDRC_126274', '0', 0.03525163, 1574],\n",
       "  ['ConsumerCourt_DCDRC_128072', '1', 0.10268682, 23],\n",
       "  ['ConsumerCourt_DCDRC_129352', '0', 0.05383814, 280],\n",
       "  ['ConsumerCourt_DCDRC_129820', '0', 0.124922395, 11],\n",
       "  ['ConsumerCourt_DCDRC_131144', '0', 0.03183586, 2434],\n",
       "  ['ConsumerCourt_DCDRC_131587', '0', 0.051139213, 353],\n",
       "  ['ConsumerCourt_DCDRC_131846', '0', 0.019118095, 12926],\n",
       "  ['ConsumerCourt_DCDRC_132917', '0', 0.030200586, 2956],\n",
       "  ['ConsumerCourt_DCDRC_132928', '0', 0.09650337, 31],\n",
       "  ['ConsumerCourt_DCDRC_134709', '0', 0.15563917, 3],\n",
       "  ['ConsumerCourt_DCDRC_140632', '0', 0.08023442, 70],\n",
       "  ['ConsumerCourt_DCDRC_140644', '0', 0.035971686, 1444],\n",
       "  ['ConsumerCourt_DCDRC_145502', '0', 0.21882568, 1],\n",
       "  ['ConsumerCourt_DCDRC_169855', '0', 0.013309098, 30432],\n",
       "  ['ConsumerCourt_DCDRC_202901', '0', 0.010666257, 46726],\n",
       "  ['ConsumerCourt_DCDRC_203935', '0', 0.04931935, 408],\n",
       "  ['ConsumerCourt_DCDRC_222781', '0', 0.041030366, 866],\n",
       "  ['ConsumerCourt_DCDRC_223340', '0', 0.0984898, 29],\n",
       "  ['ConsumerCourt_DCDRC_228779', '0', 0.059722915, 175],\n",
       "  ['ConsumerCourt_DCDRC_229540', '0', 0.060987756, 165],\n",
       "  ['ConsumerCourt_DCDRC_39867', '0', 0.08580633, 56],\n",
       "  ['ConsumerCourt_DCDRC_40822', '0', 0.13299286, 10],\n",
       "  ['ConsumerCourt_DCDRC_41058', '0', 0.08600744, 55],\n",
       "  ['ConsumerCourt_DCDRC_41514', '0', 0.08399893, 59],\n",
       "  ['ConsumerCourt_DCDRC_57900', '1', 0.050087053, 383],\n",
       "  ['ConsumerCourt_DCDRC_69224', '0', 0.08091902, 66],\n",
       "  ['ConsumerCourt_DCDRC_71619', '0', 0.02780113, 3999],\n",
       "  ['ConsumerCourt_DCDRC_73864', '0', 0.06340193, 135],\n",
       "  ['ConsumerCourt_DCDRC_75339', '0', 0.085053116, 57],\n",
       "  ['ConsumerCourt_DCDRC_75574', '1', 0.028744778, 3540],\n",
       "  ['ConsumerCourt_DCDRC_76123', '0', 0.07817657, 71],\n",
       "  ['ConsumerCourt_DCDRC_85243', '0', 0.05827902, 196],\n",
       "  ['ConsumerCourt_SCDRC_29744', '0', 0.09364603, 37],\n",
       "  ['ConsumerCourt_SCDRC_33757', '0', 0.15049091, 4],\n",
       "  ['ConsumerCourt_SCDRC_53283', '0', 0.06380063, 131],\n",
       "  ['ConsumerCourt_SCDRC_63603', '0', 0.04523133, 570],\n",
       "  ['ConsumerCourt_SCDRC_66106', '1', 0.055392, 245],\n",
       "  ['ConsumerCourt_SCDRC_66890', '0', 0.040729888, 891],\n",
       "  ['ConsumerCourt_SCDRC_67067', '0', 0.036466334, 1361],\n",
       "  ['ConsumerCourt_SCDRC_67949', '0', 0.06639166, 116],\n",
       "  ['ConsumerCourt_SCDRC_67951', '0', 0.05266119, 305],\n",
       "  ['ConsumerCourt_SCDRC_72333', '1', 0.04983908, 388],\n",
       "  ['JodhpurHC_2008_1906', '0', 0.008238676, 70133]],\n",
       " '9': [['ConsumerCourt_DCDRC_108097', '0', 0.07101656, 1800],\n",
       "  ['ConsumerCourt_DCDRC_119856', '1', 0.1855714, 38],\n",
       "  ['ConsumerCourt_DCDRC_119986', '0', 0.22666712, 6],\n",
       "  ['ConsumerCourt_DCDRC_128297', '0', 0.052711163, 4346],\n",
       "  ['ConsumerCourt_DCDRC_128836', '0', 0.017190233, 70930],\n",
       "  ['ConsumerCourt_DCDRC_130625', '0', 0.026259042, 29493],\n",
       "  ['ConsumerCourt_DCDRC_133010', '0', 0.076752506, 1467],\n",
       "  ['ConsumerCourt_DCDRC_138306', '1', 0.2921061, 4],\n",
       "  ['ConsumerCourt_DCDRC_139625', '0', 0.15522379, 132],\n",
       "  ['ConsumerCourt_DCDRC_143537', '0', 0.1833765, 41],\n",
       "  ['ConsumerCourt_DCDRC_202901', '0', 0.00243948, 285072],\n",
       "  ['ConsumerCourt_DCDRC_202902', '0', 0.011053559, 134995],\n",
       "  ['ConsumerCourt_DCDRC_202903', '0', 0.009872075, 152611],\n",
       "  ['ConsumerCourt_DCDRC_222047', '1', 0.118984275, 410],\n",
       "  ['ConsumerCourt_DCDRC_224718', '1', 0.109472655, 535],\n",
       "  ['ConsumerCourt_DCDRC_226994', '0', 0.03822827, 11462],\n",
       "  ['ConsumerCourt_DCDRC_227115', '0', 0.12726915, 330],\n",
       "  ['ConsumerCourt_DCDRC_227445', '0', 0.095713474, 794],\n",
       "  ['ConsumerCourt_DCDRC_236013', '1', 0.31705767, 3],\n",
       "  ['ConsumerCourt_DCDRC_38039', '1', 0.32157362, 2],\n",
       "  ['ConsumerCourt_DCDRC_38552', '0', 0.19962743, 18],\n",
       "  ['ConsumerCourt_DCDRC_45418', '0', 0.33407182, 1],\n",
       "  ['ConsumerCourt_DCDRC_53237', '1', 0.13149814, 287],\n",
       "  ['ConsumerCourt_DCDRC_56957', '0', 0.1695722, 76],\n",
       "  ['ConsumerCourt_DCDRC_89958', '0', 0.0652692, 2270],\n",
       "  ['ConsumerCourt_NCDRC_2916', '0', 0.15453728, 134],\n",
       "  ['ConsumerCourt_NCDRC_437', '0', 0.016535986, 75833],\n",
       "  ['ConsumerCourt_SCDRC_40659', '0', 0.15831865, 121],\n",
       "  ['ConsumerCourt_SCDRC_49689', '0', 0.065745026, 2218],\n",
       "  ['ConsumerCourt_SCDRC_53178', '0', 0.08437232, 1165],\n",
       "  ['ConsumerCourt_SCDRC_65050', '0', 0.2848596, 5],\n",
       "  ['ConsumerCourt_SCDRC_67634', '0', 0.017923096, 65838],\n",
       "  ['ConsumerCourt_SCDRC_68803', '0', 0.12768802, 325],\n",
       "  ['DelhiHC_2008_1343', '0', 0.027151834, 27254],\n",
       "  ['DelhiHC_2009_453', '0', 0.02004609, 52820],\n",
       "  ['DelhiHC_2010_4683', '0', 0.04252152, 8466],\n",
       "  ['DelhiHC_2010_6192', '0', 0.051371463, 4717],\n",
       "  ['DelhiHC_2011_69', '0', 0.017098067, 71626],\n",
       "  ['DelhiHC_2013_811', '0', 0.022599204, 41128],\n",
       "  ['SupremeCourt_2008_278', '0', 0.012008673, 121955]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'TFIDF_lem10'\n",
    "\n",
    "with open('py_results/' + model_name + 'sim_list.pkl', 'wb') as output:\n",
    "    pickle.dump(sim_list, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open('py_results/' + model_name + 'rank_list.pkl', 'wb') as output:\n",
    "    pickle.dump(rank_list, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "\n",
    "\n",
    "wb = Workbook()\n",
    "dest_filename = 'result_excel/' + model_name + '.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"ground_truth\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "#Query wise (F score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile(dest_filename)\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "places = 4\n",
    "wb = load_workbook(dest_filename)\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name ]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name ]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile(dest_filename)\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "places = 4\n",
    "wb = load_workbook(dest_filename)\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name ]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name ]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                       truncate_sheet=False, \n",
    "                       **to_excel_kwargs):\n",
    "    \"\"\"\n",
    "    Append a DataFrame [df] to existing Excel file [filename]\n",
    "    into [sheet_name] Sheet.\n",
    "    If [filename] doesn't exist, then this function will create it.\n",
    "\n",
    "    Parameters:\n",
    "      filename : File path or existing ExcelWriter\n",
    "                 (Example: '/path/to/file.xlsx')\n",
    "      df : dataframe to save to workbook\n",
    "      sheet_name : Name of sheet which will contain DataFrame.\n",
    "                   (default: 'Sheet1')\n",
    "      startrow : upper left cell row to dump data frame.\n",
    "                 Per default (startrow=None) calculate the last row\n",
    "                 in the existing DF and write to the next row...\n",
    "      truncate_sheet : truncate (remove and recreate) [sheet_name]\n",
    "                       before writing DataFrame to Excel file\n",
    "      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n",
    "                        [can be dictionary]\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    try:\n",
    "        # try to open an existing workbook\n",
    "        writer.book = load_workbook(filename)\n",
    "\n",
    "        # get the last row in the existing Excel sheet\n",
    "        # if it was not specified explicitly\n",
    "        if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "            startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "        # truncate sheet\n",
    "        if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "            # index of [sheet_name] sheet\n",
    "            idx = writer.book.sheetnames.index(sheet_name)\n",
    "            # remove [sheet_name]\n",
    "            writer.book.remove(writer.book.worksheets[idx])\n",
    "            # create an empty sheet [sheet_name] using old index\n",
    "            writer.book.create_sheet(sheet_name, idx)\n",
    "\n",
    "        # copy existing sheets\n",
    "        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    except FileNotFoundError:\n",
    "        # file does not exist yet, we will create it\n",
    "        pass\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, index= False, header = False, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()\n",
    "\n",
    "model_name = 'TFIDF_lem10'\n",
    "dest_filename = 'result_excel/' + model_name + '.xlsx'\n",
    "\n",
    "f = pd.ExcelFile(dest_filename)\n",
    "sec_dest = 'result_excel/all_results.xlsx'\n",
    "f2 = pd.ExcelFile(sec_dest)\n",
    "append_df_to_excel(sec_dest, f.parse(1), sheet_name= f2.sheet_names[0])\n",
    "append_df_to_excel(sec_dest, f.parse(2), sheet_name= f2.sheet_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Top_x(x, R):\n",
    "    R = list(R)\n",
    "    x_names = []\n",
    "    for i in range(x):\n",
    "        x_names.append(filenames[R.index(i + 1)].strip('.txt'))\n",
    "    return x_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_stat(word, freq, index):\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    word = dictionary.doc2bow([word])[0][0]\n",
    "    idf = bm25.idf[word] if bm25.idf[word] >= 0 else EPSILON * average_idf\n",
    "    score = freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "              / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / bm25.avgdl))))\n",
    "    return (index_doc[word], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "dest_filename = 'Temp.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "def common_words(q, F):\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    index = filenames.index(F + '.txt')\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    row = []\n",
    "    for word, freq in  zip(q_words, q_freq):\n",
    "        w = dictionary.doc2bow([word])[0][0]\n",
    "        if w in index_doc: \n",
    "            f, s = score_stat(word, freq, index)\n",
    "            row.append([word + ', ' + str(round(bm25.idf[w], 2)), freq, f,  round( (s*100)/sim_list[q][index], 1)])  \n",
    "    row.sort(key = lambda x: (x[3],x[1],x[2]), reverse = True)\n",
    "    wb = load_workbook('Temp.xlsx')\n",
    "    ws = wb.create_sheet(title=str(q) + F)\n",
    "    ws.append(['word,idf', 'Query freq', 'freq', '% score'])\n",
    "    for i in row:\n",
    "        ws.append(i)\n",
    "    wb.save(filename = 'Temp.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"toberead.txt\", 'r') \n",
    "count = 0\n",
    "q = 1\n",
    "for i in f:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    count = count + 1\n",
    "    common_words(q, i.strip())\n",
    "    if count % 5 == 0:\n",
    "        q = q + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in files_to_tokens(full_filenames[filenames.index(doc + '.txt')]):\n",
    "    for j in i:\n",
    "        print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = 'ConsumerCourt_SCDRC_30176'\n",
    "corpus[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words(1, 'ConsumerCourt_DCDRC_41588')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index= None, columns=['words','pair'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.append(['lol', ('we','wew')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncommon_words(q, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "wb = load_workbook('Explore_BM25.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_uncom.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_uncom.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "#         for word, freq in  zip(q_words, q_freq):\n",
    "#             if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "#                 f, s = score_stat(word, freq, index)\n",
    "#                 row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "#             else:\n",
    "#                 row.append('X')\n",
    "        #ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_all.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_all.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(sec_row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

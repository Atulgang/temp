{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 15:33:05,987 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "en_stop = set(get_stop_words('en'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in word_tokenize(string):\n",
    "            if word not in en_stop:\n",
    "                tokens.append(ps.stem(word))\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 15:33:23,779 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-17 15:39:55,344 : INFO : adding document #10000 to Dictionary(172845 unique tokens: [u'gudadannavar', u'alsocont', u'beencredit', u'delhifano', u'alsocons']...)\n",
      "2018-03-17 15:45:51,487 : INFO : adding document #20000 to Dictionary(312646 unique tokens: [u'gudadannavar', u'falselitig', u'againstcomplain', u'ofday', u'nonehru']...)\n",
      "2018-03-17 15:51:31,350 : INFO : adding document #30000 to Dictionary(472300 unique tokens: [u'gudadannavar', u'mrmvenkatesancounsel', u'rkgejggll', u'boncer', u'metersand']...)\n",
      "2018-03-17 15:56:24,613 : INFO : adding document #40000 to Dictionary(654903 unique tokens: [u'gudadannavar', u'woodb', u'mrmvenkatesancounsel', u'atsugham', u'rkgejggll']...)\n"
     ]
    }
   ],
   "source": [
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "dictionary.compactify()\n",
    "dictionary.filter_extremes(keep_n=None)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('t.mm', corpus)\n",
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('t.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of document lengths in the corpus\n",
    "doc_len = []\n",
    "for doc in corpus:\n",
    "    temp = 0\n",
    "    for word, freq in doc:\n",
    "        temp += freq \n",
    "    doc_len.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "#     The variable k1 is a positive tuning parameter that calibrates the\n",
    "#     document term frequency scaling. A k1 value of 0 corresponds to a binary\n",
    "#     model (no term frequency), and a large value corresponds to using raw term\n",
    "#     frequency. b is another tuning parameter (0 ≤ b ≤ 1) which determines\n",
    "#     the scaling by document length: b = 1 corresponds to fully scaling the term\n",
    "#     weight by the document length, while b = 0 corresponds to no length normalization\n",
    "\n",
    "\n",
    "# BM25 parameters.\n",
    "PARAM_K1 =1.2\n",
    "PARAM_B = 0.75\n",
    "\n",
    "# incase the idf is zero\n",
    "EPSILON = 0.01\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus_size = dictionary.num_docs\n",
    "        s = 0\n",
    "        for i in corpus:\n",
    "            for x in i:\n",
    "                s = s + x[1]\n",
    "        s = float(s)    \n",
    "        self.avgdl = s / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.df = dictionary.dfs\n",
    "        self.idf = {}\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, query, index, average_idf):\n",
    "        score = 0\n",
    "        index_doc = dict(self.corpus[index])\n",
    "        \n",
    "        for word,freq in query:\n",
    "            if word not in index_doc:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "                      / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / self.avgdl))))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(query, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm25 = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to pass it to the function that calculates the score\n",
    "average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = copy.deepcopy(query_truth)\n",
    "\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    sims = bm25.get_scores(dictionary.doc2bow(query), average_idf)\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "    print i\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'BM25.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"K1 = 1.2 B = 0.75 EPS = 0.01 |\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise (F score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Top_x(x, R):\n",
    "    R = list(R)\n",
    "    x_names = []\n",
    "    for i in range(x):\n",
    "        x_names.append(filenames[R.index(i + 1)].strip('.txt'))\n",
    "    return x_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_stat(word, freq, index):\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    word = dictionary.doc2bow([word])[0][0]\n",
    "    idf = bm25.idf[word] if bm25.idf[word] >= 0 else EPSILON * average_idf\n",
    "    score = freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "              / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / bm25.avgdl))))\n",
    "    return (index_doc[word], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "dest_filename = 'Temp.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "def common_words(q, F):\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    index = filenames.index(F + '.txt')\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    row = []\n",
    "    for word, freq in  zip(q_words, q_freq):\n",
    "        w = dictionary.doc2bow([word])[0][0]\n",
    "        if w in index_doc: \n",
    "            f, s = score_stat(word, freq, index)\n",
    "            row.append([word + ', ' + str(round(bm25.idf[w], 2)), freq, f,  round( (s*100)/sim_list[q][index], 1)])  \n",
    "    row.sort(key = lambda x: (x[3],x[1],x[2]), reverse = True)\n",
    "    wb = load_workbook('Temp.xlsx')\n",
    "    ws = wb.create_sheet(title=str(q) + F)\n",
    "    ws.append(['word,idf', 'Query freq', 'freq', '% score'])\n",
    "    for i in row:\n",
    "        ws.append(i)\n",
    "    wb.save(filename = 'Temp.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"toberead.txt\", 'r') \n",
    "count = 0\n",
    "q = 1\n",
    "for i in f:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    count = count + 1\n",
    "    common_words(q, i.strip())\n",
    "    if count % 5 == 0:\n",
    "        q = q + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in files_to_tokens(full_filenames[filenames.index(doc + '.txt')]):\n",
    "    for j in i:\n",
    "        print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = 'ConsumerCourt_SCDRC_30176'\n",
    "corpus[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words(1, 'ConsumerCourt_DCDRC_41588')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index= None, columns=['words','pair'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.append(['lol', ('we','wew')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncommon_words(q, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "wb = load_workbook('Explore_BM25.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_uncom.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_uncom.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "#         for word, freq in  zip(q_words, q_freq):\n",
    "#             if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "#                 f, s = score_stat(word, freq, index)\n",
    "#                 row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "#             else:\n",
    "#                 row.append('X')\n",
    "        #ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_all.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_all.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(sec_row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 14:54:42,272 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove non-alphabets from text\n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,\n",
    "    'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        \n",
    "        # tokenize that string\n",
    "        tokens =  word_tokenize(string)\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 14:55:10,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 14:56:54,091 : INFO : adding document #10000 to Dictionary(190225 unique tokens: [u'gudadannavar', u'coolege', u'withoutcomplying', u'ofaffidavitcumreply', u'metersand']...)\n",
      "2018-03-16 14:58:25,245 : INFO : adding document #20000 to Dictionary(341648 unique tokens: [u'gudadannavar', u'srimohanchandra', u'coolege', u'withoutcomplying', u'ofday']...)\n",
      "2018-03-16 14:59:52,238 : INFO : adding document #30000 to Dictionary(515380 unique tokens: [u'gudadannavar', u'gavecredit', u'mrmvenkatesancounsel', u'withoutcomplying', u'rkgejggll']...)\n",
      "2018-03-16 15:01:07,052 : INFO : adding document #40000 to Dictionary(713926 unique tokens: [u'vaishnavaccountant', u'tdrwas', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:02:07,567 : INFO : adding document #50000 to Dictionary(840531 unique tokens: [u'vaishnavaccountant', u'tdrwas', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:03:03,042 : INFO : adding document #60000 to Dictionary(967106 unique tokens: [u'vaishnavaccountant', u'tdrwas', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:03:51,520 : INFO : adding document #70000 to Dictionary(1058833 unique tokens: [u'vaishnavaccountant', u'tdrwas', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:04:50,746 : INFO : adding document #80000 to Dictionary(1164414 unique tokens: [u'vaishnavaccountant', u'mdbg', u'tdrwas', u'withoutcomplying', u'nexusvisvis']...)\n",
      "2018-03-16 15:06:00,361 : INFO : adding document #90000 to Dictionary(1264843 unique tokens: [u'msgkpinfrastrcuture', u'vaishnavaccountant', u'mdbg', u'tdrwas', u'withoutcomplying']...)\n",
      "2018-03-16 15:07:21,430 : INFO : adding document #100000 to Dictionary(1393619 unique tokens: [u'msgkpinfrastrcuture', u'vaishnavaccountant', u'mdbg', u'tdrwas', u'withoutcomplying']...)\n",
      "2018-03-16 15:08:40,079 : INFO : adding document #110000 to Dictionary(1530313 unique tokens: [u'msgkpinfrastrcuture', u'mrmvenkatesancounsel', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:09:33,286 : INFO : adding document #120000 to Dictionary(1593628 unique tokens: [u'msgkpinfrastrcuture', u'mrmvenkatesancounsel', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-16 15:09:55,992 : INFO : built Dictionary(1615531 unique tokens: [u'msgkpinfrastrcuture', u'mrmvenkatesancounsel', u'withoutcomplying', u'nexusvisvis', u'boncer']...) from 122895 documents (total 187613161 corpus positions)\n",
      "2018-03-16 15:09:56,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:10:26,459 : INFO : built Dictionary(208777 unique tokens: [u'withoutcomplying', u'theeverincreasing', u'fromdrrajendra', u'pricepremium', u'pawnor']...) from 3484 documents (total 5890063 corpus positions)\n",
      "2018-03-16 15:10:27,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:11:16,823 : INFO : adding document #10000 to Dictionary(109482 unique tokens: [u'cfpvfzdzm', u'sonji', u'nehatero', u'fawe', u'gavat']...)\n",
      "2018-03-16 15:12:02,496 : INFO : adding document #20000 to Dictionary(272462 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'vwvs', u'gavas', u'gavat']...)\n",
      "2018-03-16 15:12:44,105 : INFO : adding document #30000 to Dictionary(439227 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'vihykfkhzifjoknhizrfkhz', u'noseventh', u'pricepremium']...)\n",
      "2018-03-16 15:13:43,982 : INFO : adding document #40000 to Dictionary(618049 unique tokens: [u'purchasedobliviously', u'mrchandrabali', u'withoutcomplying', u'cfpvfzdzm', u'vihykfkhzifjoknhizrfkhz']...)\n",
      "2018-03-16 15:14:45,896 : INFO : adding document #50000 to Dictionary(759208 unique tokens: [u'mrchandrabali', u'withoutcomplying', u'vwvs', u'pricepremium', u'ltdvill']...)\n",
      "2018-03-16 15:15:19,298 : INFO : built Dictionary(831569 unique tokens: [u'gavecredit', u'mrchandrabali', u'withoutcomplying', u'vwvs', u'pricepremium']...) from 56745 documents (total 58383075 corpus positions)\n",
      "2018-03-16 15:15:20,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:17:17,938 : INFO : adding document #10000 to Dictionary(142348 unique tokens: [u'gangoor', u'corelating', u'wereunauthorisedly', u'vani', u'circuitry']...)\n",
      "2018-03-16 15:19:16,228 : INFO : adding document #20000 to Dictionary(198624 unique tokens: [u'gangoor', u'gai', u'drlpa', u'proceedingsxxx', u'pricepremium']...)\n",
      "2018-03-16 15:21:10,668 : INFO : adding document #30000 to Dictionary(287634 unique tokens: [u'commissioningwould', u'anyex', u'possessionsubletassigned', u'gangoor', u'gai']...)\n",
      "2018-03-16 15:21:57,816 : INFO : built Dictionary(318811 unique tokens: [u'commissioningwould', u'anyex', u'possessionsubletassigned', u'gangoor', u'gai']...) from 34008 documents (total 86858152 corpus positions)\n",
      "2018-03-16 15:21:59,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:22:36,332 : INFO : adding document #10000 to Dictionary(64156 unique tokens: [u'msanandasenfortherespondents', u'deferment', u'ikkzir', u'indardeo', u'vani']...)\n",
      "2018-03-16 15:22:52,985 : INFO : adding document #20000 to Dictionary(86360 unique tokens: [u'msanandasenfortherespondents', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed', u'deferment', u'ikkzir', u'indardeo']...)\n",
      "2018-03-16 15:23:15,968 : INFO : adding document #30000 to Dictionary(123759 unique tokens: [u'ramdhir', u'withoutcomplying', u'honblemrjusticednpatelforthepetitionermsmanojtandonnksinghadvocatesfortherespondentsjctoscii', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed', u'icici']...)\n",
      "2018-03-16 15:23:39,777 : INFO : adding document #40000 to Dictionary(158066 unique tokens: [u'oranyotherpurposes', u'ramdhir', u'unsupportable', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof']...)\n",
      "2018-03-16 15:23:53,714 : INFO : adding document #50000 to Dictionary(171805 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'oranyotherpurposes', u'ramdhir', u'unsupportable', u'withoutcomplying']...)\n",
      "2018-03-16 15:24:27,450 : INFO : adding document #60000 to Dictionary(214074 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...)\n",
      "2018-03-16 15:24:52,984 : INFO : adding document #70000 to Dictionary(251618 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...)\n",
      "2018-03-16 15:24:55,216 : INFO : built Dictionary(253824 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...) from 71312 documents (total 22753017 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:24:55,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:25:36,639 : INFO : adding document #10000 to Dictionary(179819 unique tokens: [u'horses', u'withoutcomplying', u'whichincites', u'otheractivitesas', u'metersand']...)\n",
      "2018-03-16 15:26:18,284 : INFO : adding document #20000 to Dictionary(273204 unique tokens: [u'berma', u'horses', u'withoutcomplying', u'suchdefinition', u'whichincites']...)\n",
      "2018-03-16 15:27:08,006 : INFO : adding document #30000 to Dictionary(368342 unique tokens: [u'purposenarendra', u'withoutcomplying', u'whichincites', u'arithmeticallyincorrect', u'brokenetowel']...)\n",
      "2018-03-16 15:27:20,954 : INFO : built Dictionary(386629 unique tokens: [u'purposenarendra', u'withoutcomplying', u'whichincites', u'arithmeticallyincorrect', u'brokenetowel']...) from 32240 documents (total 26325476 corpus positions)\n",
      "2018-03-16 15:27:20,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:27:41,015 : INFO : built Dictionary(40016 unique tokens: [u'noezi', u'deferment', u'tapodip', u'woods', u'spiders']...) from 1210 documents (total 4552202 corpus positions)\n",
      "2018-03-16 15:27:41,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 15:31:32,567 : INFO : adding document #10000 to Dictionary(222963 unique tokens: [u'kajumal', u'vang', u'nunnery', u'jaluram', u'balipura']...)\n",
      "2018-03-16 15:33:45,343 : INFO : adding document #20000 to Dictionary(305098 unique tokens: [u'refarred', u'kajumal', u'vang', u'nunnery', u'jaluram']...)\n",
      "2018-03-16 15:35:58,380 : INFO : adding document #30000 to Dictionary(366921 unique tokens: [u'refarred', u'santawana', u'jaluram', u'justiceability', u'pricepremium']...)\n",
      "2018-03-16 15:36:00,530 : INFO : built Dictionary(367860 unique tokens: [u'refarred', u'santawana', u'jaluram', u'justiceability', u'pricepremium']...) from 30091 documents (total 110555608 corpus positions)\n",
      "2018-03-16 15:36:00,531 : INFO : saving Dictionary object under newdictionary_DCDRC, separately None\n",
      "2018-03-16 15:36:01,452 : INFO : saved newdictionary_DCDRC\n",
      "2018-03-16 15:36:01,453 : INFO : saving Dictionary object under newdictionary_NCDRC, separately None\n",
      "2018-03-16 15:36:01,638 : INFO : saved newdictionary_NCDRC\n",
      "2018-03-16 15:36:01,639 : INFO : saving Dictionary object under newdictionary_SCDRC, separately None\n",
      "2018-03-16 15:36:02,100 : INFO : saved newdictionary_SCDRC\n",
      "2018-03-16 15:36:02,101 : INFO : saving Dictionary object under newdictionary_DelhiHC, separately None\n",
      "2018-03-16 15:36:02,268 : INFO : saved newdictionary_DelhiHC\n",
      "2018-03-16 15:36:02,269 : INFO : saving Dictionary object under newdictionary_JharkhandHC, separately None\n",
      "2018-03-16 15:36:02,440 : INFO : saved newdictionary_JharkhandHC\n",
      "2018-03-16 15:36:02,441 : INFO : saving Dictionary object under newdictionary_JodhpurHC, separately None\n",
      "2018-03-16 15:36:02,658 : INFO : saved newdictionary_JodhpurHC\n",
      "2018-03-16 15:36:02,659 : INFO : saving Dictionary object under newdictionary_KolkataHC, separately None\n",
      "2018-03-16 15:36:02,681 : INFO : saved newdictionary_KolkataHC\n",
      "2018-03-16 15:36:02,682 : INFO : saving Dictionary object under newdictionary_SupremeCourt, separately None\n",
      "2018-03-16 15:36:02,880 : INFO : saved newdictionary_SupremeCourt\n"
     ]
    }
   ],
   "source": [
    "# Create all the court specific dictionaries\n",
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# ## check out dictionaries\n",
    "# dictionary_DCDRC.save_as_text('newdictionary_DCDRC.txt') \n",
    "# dictionary_NCDRC.save_as_text('newdictionary_NCDRC.txt') \n",
    "# dictionary_SCDRC.save_as_text('newdictionary_SCDRC.txt') \n",
    "# dictionary_DelhiHC.save_as_text('newdictionary_DelhiHC.txt') \n",
    "# dictionary_JharkhandHC.save_as_text('newdictionary_JharkhandHC.txt') \n",
    "# dictionary_JodhpurHC.save_as_text('newdictionary_JodhpurHC.txt') \n",
    "# dictionary_KolkataHC.save_as_text('newdictionary_KolkataHC.txt') \n",
    "# dictionary_SupremeCourt.save_as_text('newdictionary_SupremeCourt.txt') \n",
    "# Save actual dictionaries\n",
    "dictionary_DCDRC.save('newdictionary_DCDRC') \n",
    "dictionary_NCDRC.save('newdictionary_NCDRC') \n",
    "dictionary_SCDRC.save('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC.save('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC.save('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC.save('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC.save('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt.save('newdictionary_SupremeCourt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:36:02,888 : INFO : loading Dictionary object from newdictionary_DCDRC\n",
      "2018-03-16 15:36:03,849 : INFO : loaded newdictionary_DCDRC\n",
      "2018-03-16 15:36:04,098 : INFO : loading Dictionary object from newdictionary_NCDRC\n",
      "2018-03-16 15:36:04,182 : INFO : loaded newdictionary_NCDRC\n",
      "2018-03-16 15:36:04,208 : INFO : loading Dictionary object from newdictionary_SCDRC\n",
      "2018-03-16 15:36:04,528 : INFO : loaded newdictionary_SCDRC\n",
      "2018-03-16 15:36:04,644 : INFO : loading Dictionary object from newdictionary_DelhiHC\n",
      "2018-03-16 15:36:04,751 : INFO : loaded newdictionary_DelhiHC\n",
      "2018-03-16 15:36:04,790 : INFO : loading Dictionary object from newdictionary_JharkhandHC\n",
      "2018-03-16 15:36:04,918 : INFO : loaded newdictionary_JharkhandHC\n",
      "2018-03-16 15:36:04,960 : INFO : loading Dictionary object from newdictionary_JodhpurHC\n",
      "2018-03-16 15:36:05,100 : INFO : loaded newdictionary_JodhpurHC\n",
      "2018-03-16 15:36:05,157 : INFO : loading Dictionary object from newdictionary_KolkataHC\n",
      "2018-03-16 15:36:05,175 : INFO : loaded newdictionary_KolkataHC\n",
      "2018-03-16 15:36:05,181 : INFO : loading Dictionary object from newdictionary_SupremeCourt\n",
      "2018-03-16 15:36:05,319 : INFO : loaded newdictionary_SupremeCourt\n"
     ]
    }
   ],
   "source": [
    "# load those dictionaries\n",
    "dictionary_DCDRC = corpora.Dictionary.load('newdictionary_DCDRC') \n",
    "dictionary_NCDRC = corpora.Dictionary.load('newdictionary_NCDRC') \n",
    "dictionary_SCDRC = corpora.Dictionary.load('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC = corpora.Dictionary.load('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC = corpora.Dictionary.load('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC = corpora.Dictionary.load('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC = corpora.Dictionary.load('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt= corpora.Dictionary.load('newdictionary_SupremeCourt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:44:47,562 : INFO : discarding 1370740 tokens: [(u'all', 70971), (u'chempazhanthi', 3), (u'issued', 78775), (u'to', 120964), (u'beenakumariamember', 2), (u'th', 63546), (u'under', 94240), (u'narendranthis', 1), (u'cost', 69475), (u'further', 71982)]...\n",
      "2018-03-16 15:44:47,563 : INFO : keeping 244791 tokens which were in no less than 5 and no more than 61447 (=50.0%) documents\n",
      "2018-03-16 15:44:48,446 : INFO : resulting dictionary: Dictionary(244791 unique tokens: [u'filingoriginal', u'withoutcomplying', u'nonehru', u'partiesnilexhibits', u'distributers']...)\n",
      "2018-03-16 15:44:49,042 : INFO : discarding 181434 tokens: [(u'ccomplaints', 2), (u'docdocnoconsumercourtncdrcdocnotextnational', 2487), (u'to', 3481), (u'th', 2030), (u'under', 2881), (u'ofthe', 2456), (u'theexpansion', 2), (u'cost', 1925), (u'further', 2307), (u'deducteda', 2)]...\n",
      "2018-03-16 15:44:49,042 : INFO : keeping 27343 tokens which were in no less than 5 and no more than 1742 (=50.0%) documents\n",
      "2018-03-16 15:44:49,148 : INFO : resulting dictionary: Dictionary(27343 unique tokens: [u'ofexpenses', u'hanging', u'commissionchandigarh', u'paymentschedule', u'thecover']...)\n",
      "2018-03-16 15:44:50,651 : INFO : discarding 713933 tokens: [(u'through', 30218), (u'its', 29874), (u'before', 42770), (u'member', 49369), (u'had', 33229), (u'to', 49556), (u'under', 33437), (u'has', 42513), (u'complaint', 43815), (u'not', 46310)]...\n",
      "2018-03-16 15:44:50,651 : INFO : keeping 117636 tokens which were in no less than 5 and no more than 28372 (=50.0%) documents\n",
      "2018-03-16 15:44:51,095 : INFO : resulting dictionary: Dictionary(117636 unique tokens: [u'blgn', u'nqlrh', u'ksrinivasa', u'gai', u'withdamages']...)\n",
      "2018-03-16 15:44:51,759 : INFO : discarding 256392 tokens: [(u'coram', 29494), (u'textdoc', 32983), (u'allowed', 27567), (u'high', 33958), (u'see', 24721), (u'through', 31017), (u'at', 33670), (u'in', 33983), (u'digest', 20695), (u'court', 33986)]...\n",
      "2018-03-16 15:44:51,760 : INFO : keeping 62419 tokens which were in no less than 5 and no more than 17004 (=50.0%) documents\n",
      "2018-03-16 15:44:51,929 : INFO : resulting dictionary: Dictionary(62419 unique tokens: [u'uoirespondent', u'corelating', u'causans', u'deferment', u'sowell']...)\n",
      "2018-03-16 15:44:52,420 : INFO : discarding 221608 tokens: [(u'justice', 55009), (u'to', 59423), (u'has', 42172), (u'for', 61809), (u'state', 48737), (u'jharkhand', 56620), (u'by', 39897), (u'on', 48744), (u'of', 65411), (u'honble', 55847)]...\n",
      "2018-03-16 15:44:52,420 : INFO : keeping 32216 tokens which were in no less than 5 and no more than 35656 (=50.0%) documents\n",
      "2018-03-16 15:44:52,531 : INFO : resulting dictionary: Dictionary(32216 unique tokens: [u'aloksinghjmanishtextdoc', u'woods', u'hanging', u'heardlearnedcounsel', u'regularize']...)\n",
      "2018-03-16 15:44:53,269 : INFO : discarding 290617 tokens: [(u'to', 28523), (u'petition', 21598), (u'rajasthan', 21074), (u'palidecide', 1), (u'for', 31724), (u'case', 18541), (u'state', 17787), (u'clarificationsought', 1), (u'learned', 25544), (u'be', 23495)]...\n",
      "2018-03-16 15:44:53,270 : INFO : keeping 96012 tokens which were in no less than 5 and no more than 16120 (=50.0%) documents\n",
      "2018-03-16 15:44:53,514 : INFO : resulting dictionary: Dictionary(96012 unique tokens: [u'casesangeet', u'withoutcomplying', u'sbawejatextdoc', u'woods', u'clotted']...)\n",
      "2018-03-16 15:44:53,616 : INFO : discarding 28716 tokens: [(u'limited', 713), (u'all', 1084), (u'issued', 797), (u'scc', 607), (u'concerned', 643), (u'to', 1208), (u'those', 642), (u'th', 893), (u'under', 1157), (u'division', 612)]...\n",
      "2018-03-16 15:44:53,616 : INFO : keeping 11300 tokens which were in no less than 5 and no more than 605 (=50.0%) documents\n",
      "2018-03-16 15:44:53,638 : INFO : resulting dictionary: Dictionary(11300 unique tokens: [u'writings', u'indranil', u'deferment', u'yellow', u'four']...)\n",
      "2018-03-16 15:44:54,279 : INFO : discarding 304721 tokens: [(u'all', 24105), (u'allowed', 20542), (u'jjin', 3), (u'concerned', 16778), (u'to', 29817), (u'present', 19620), (u'under', 28062), (u'must', 18635), (u'did', 20423), (u'these', 22098)]...\n",
      "2018-03-16 15:44:54,280 : INFO : keeping 63139 tokens which were in no less than 5 and no more than 15045 (=50.0%) documents\n",
      "2018-03-16 15:44:54,487 : INFO : resulting dictionary: Dictionary(63139 unique tokens: [u'corelating', u'deferment', u'pricepremium', u'pawnor', u'fortythird']...)\n"
     ]
    }
   ],
   "source": [
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = copy.deepcopy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76125"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:49:17,247 : INFO : discarding 294233 tokens: [(u'metersand', 9), (u'sowell', 6), (u'documentupon', 5), (u'trawling', 9), (u'ofjmic', 6), (u'canes', 25), (u'andprinciple', 7), (u'halfshare', 33), (u'canel', 6), (u'fkhbl', 11)]...\n",
      "2018-03-16 15:49:17,248 : INFO : keeping 76125 tokens which were in no less than 50 and no more than 175992 (=50.0%) documents\n",
      "2018-03-16 15:49:17,498 : INFO : resulting dictionary: Dictionary(76125 unique tokens: [u'withdamages', u'majumder', u'withoutcomplying', u'struck', u'aloksinghjmanishtextdoc']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=50)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 15:57:03,777 : INFO : saving Dictionary object under newdictionary, separately None\n",
      "2018-03-16 15:57:03,918 : INFO : saved newdictionary\n"
     ]
    }
   ],
   "source": [
    "# save the dictionary\n",
    "dictionary.save('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()\n",
    "# save BOW\n",
    "corpora.MmCorpus.serialize('newcorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('newcorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tfidf model\n",
    "# Will take time as to generate a model it processes over the whole corpus\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "# save tfidf model\n",
    "tfidf.save('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### load tfidf model\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf wrapper coprus\n",
    "# doesn't take time - yield\n",
    "tfidf_corpus = tfidf[corpus]\n",
    "\n",
    "# actual tfidf corpus serialized\n",
    "corpora.MmCorpus.serialize('newtfidf_corpus.mm', tfidf_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf corpus\n",
    "tfidf_corpus = corpora.MmCorpus('newtfidf_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "# query no. gives the list of 0-1 documents\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create index - Tfidf\n",
    "tfidf_index = gensim.similarities.Similarity('tfidf.index',tfidf_corpus,len(dictionary))\n",
    "\n",
    "# save index - tfidf\n",
    "gensim.similarities.Similarity.save(tfidf_index,'TFIDF_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### load tfidf index\n",
    "tfidf_index = gensim.similarities.Similarity.load('TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate similarity for all \n",
    "tfidf_index.num_best= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################         SIMILIRATIES SCORES AND RANKING    ###########################\n",
    "tfidf_results = copy.deepcopy(query_truth)\n",
    "### lists of list (indexing from 1, dummy element)\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    # inside the square bracket determines query representation\n",
    "    \n",
    "    sims = tfidf_index[tfidf[dictionary.doc2bow(query)]]\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in tfidf_results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "        #x.append(common_words(filenames.index(x[0]+'.txt'), tfidf_corpus, query))\n",
    "    \n",
    "    # sort wrt relevance(from truth) and then ranks(from our model)\n",
    "    #tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[1]),x[3]))     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'tfidf.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"TFIDF\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in tfidf_results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluation_Qwise - Precision, Recall and Fscore\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:\n",
    "    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

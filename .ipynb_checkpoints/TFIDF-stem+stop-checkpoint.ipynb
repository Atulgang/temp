{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 14:51:52,138 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove non-alphabets from text\n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt',\n",
    "    'LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', \n",
    "    'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,\n",
    "    'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt',\n",
    "    'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        \n",
    "        # tokenize that string\n",
    "        tokens =  word_tokenize(string)\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 16:02:34,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 16:06:29,424 : INFO : adding document #10000 to Dictionary(189825 unique tokens: [u'gudadannavar', u'coolege', u'withoutcomplying', u'ofaffidavitcumreply', u'icici']...)\n",
      "2018-03-15 16:09:51,427 : INFO : adding document #20000 to Dictionary(341298 unique tokens: [u'gudadannavar', u'coolege', u'withoutcomplying', u'ofday', u'nonehru']...)\n",
      "2018-03-15 16:12:44,750 : INFO : adding document #30000 to Dictionary(515058 unique tokens: [u'gudadannavar', u'gavecredit', u'mrmvenkatesancounsel', u'withoutcomplying', u'rkgejggll']...)\n",
      "2018-03-15 16:15:12,634 : INFO : adding document #40000 to Dictionary(713320 unique tokens: [u'vaishnavaccountant', u'vsopponentsshimoga', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:17:18,812 : INFO : adding document #50000 to Dictionary(839941 unique tokens: [u'vaishnavaccountant', u'vsopponentsshimoga', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:19:06,592 : INFO : adding document #60000 to Dictionary(966535 unique tokens: [u'vaishnavaccountant', u'vsopponentsshimoga', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:20:50,154 : INFO : adding document #70000 to Dictionary(1058274 unique tokens: [u'vaishnavaccountant', u'vsopponentsshimoga', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:23:05,457 : INFO : adding document #80000 to Dictionary(1163852 unique tokens: [u'vaishnavaccountant', u'mdbg', u'vsopponentsshimoga', u'withoutcomplying', u'dealerbarog']...)\n",
      "2018-03-15 16:25:47,818 : INFO : adding document #90000 to Dictionary(1264295 unique tokens: [u'msgkpinfrastrcuture', u'vaishnavaccountant', u'mdbg', u'vsopponentsshimoga', u'withoutcomplying']...)\n",
      "2018-03-15 16:29:03,043 : INFO : adding document #100000 to Dictionary(1393082 unique tokens: [u'msgkpinfrastrcuture', u'vaishnavaccountant', u'mdbg', u'vsopponentsshimoga', u'withoutcomplying']...)\n",
      "2018-03-15 16:32:07,983 : INFO : adding document #110000 to Dictionary(1529785 unique tokens: [u'msgkpinfrastrcuture', u'amitashree', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:34:13,040 : INFO : adding document #120000 to Dictionary(1592319 unique tokens: [u'msgkpinfrastrcuture', u'amitashree', u'withoutcomplying', u'nexusvisvis', u'boncer']...)\n",
      "2018-03-15 16:35:05,779 : INFO : built Dictionary(1613868 unique tokens: [u'msgkpinfrastrcuture', u'amitashree', u'withoutcomplying', u'nexusvisvis', u'boncer']...) from 122895 documents (total 187295819 corpus positions)\n",
      "2018-03-15 16:35:06,246 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 16:36:10,094 : INFO : built Dictionary(208687 unique tokens: [u'withoutcomplying', u'theeverincreasing', u'fromdrrajendra', u'pricepremium', u'pawnor']...) from 3484 documents (total 5887429 corpus positions)\n",
      "2018-03-15 16:36:16,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 16:38:12,947 : INFO : adding document #10000 to Dictionary(109465 unique tokens: [u'cfpvfzdzm', u'sonji', u'nehatero', u'fawe', u'gavat']...)\n",
      "2018-03-15 16:40:07,773 : INFO : adding document #20000 to Dictionary(270547 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'hansli', u'vwvs', u'gavas']...)\n",
      "2018-03-15 16:41:54,983 : INFO : adding document #30000 to Dictionary(428686 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'vihykfkhzifjoknhizrfkhz', u'noseventh', u'pricepremium']...)\n",
      "2018-03-15 16:44:04,773 : INFO : adding document #40000 to Dictionary(607474 unique tokens: [u'purchasedobliviously', u'mrchandrabali', u'withoutcomplying', u'cfpvfzdzm', u'vihykfkhzifjoknhizrfkhz']...)\n",
      "2018-03-15 16:46:21,971 : INFO : adding document #50000 to Dictionary(748241 unique tokens: [u'mrchandrabali', u'withoutcomplying', u'vwvs', u'pricepremium', u'ltdvill']...)\n",
      "2018-03-15 16:47:35,944 : INFO : built Dictionary(820588 unique tokens: [u'gavecredit', u'mrchandrabali', u'withoutcomplying', u'vwvs', u'pricepremium']...) from 56745 documents (total 58345536 corpus positions)\n",
      "2018-03-15 16:47:40,055 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 16:52:13,091 : INFO : adding document #10000 to Dictionary(142255 unique tokens: [u'gangoor', u'corelating', u'wereunauthorisedly', u'vani', u'circuitry']...)\n",
      "2018-03-15 16:56:50,894 : INFO : adding document #20000 to Dictionary(198537 unique tokens: [u'gangoor', u'gai', u'drlpa', u'proceedingsxxx', u'pricepremium']...)\n",
      "2018-03-15 17:01:12,964 : INFO : adding document #30000 to Dictionary(287537 unique tokens: [u'anyex', u'possessionsubletassigned', u'gangoor', u'gai', u'drlpa']...)\n",
      "2018-03-15 17:03:01,947 : INFO : built Dictionary(318721 unique tokens: [u'anyex', u'possessionsubletassigned', u'gangoor', u'gai', u'drlpa']...) from 34008 documents (total 86837099 corpus positions)\n",
      "2018-03-15 17:03:14,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 17:04:56,894 : INFO : adding document #10000 to Dictionary(64156 unique tokens: [u'msanandasenfortherespondents', u'singhji', u'deferment', u'ikkzir', u'indardeo']...)\n",
      "2018-03-15 17:06:06,138 : INFO : adding document #20000 to Dictionary(86360 unique tokens: [u'msanandasenfortherespondents', u'singhji', u'deferment', u'ikkzir', u'indardeo']...)\n",
      "2018-03-15 17:07:13,434 : INFO : adding document #30000 to Dictionary(123759 unique tokens: [u'ramdhir', u'withoutcomplying', u'honblemrjusticednpatelforthepetitionermsmanojtandonnksinghadvocatesfortherespondentsjctoscii', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed', u'distributers']...)\n",
      "2018-03-15 17:08:34,914 : INFO : adding document #40000 to Dictionary(158066 unique tokens: [u'oranyotherpurposes', u'ramdhir', u'unsupportable', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof']...)\n",
      "2018-03-15 17:09:22,470 : INFO : adding document #50000 to Dictionary(171805 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'oranyotherpurposes', u'ramdhir', u'unsupportable', u'withoutcomplying']...)\n",
      "2018-03-15 17:11:11,888 : INFO : adding document #60000 to Dictionary(214074 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...)\n",
      "2018-03-15 17:12:30,217 : INFO : adding document #70000 to Dictionary(251618 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...)\n",
      "2018-03-15 17:12:36,280 : INFO : built Dictionary(253824 unique tokens: [u'thaprilexceptthepetitionernonamelysmtsushilasinhainabanoofafter', u'hasbul', u'withoutcomplying', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'possessioncoaccusedrajusinghandamjadansariwhowerearrestedonspothavebeengrantedbailbythiscourtinbanoofandofrespectivelythepetitionerisincustodysinceaugustwithoutanycogentbasisheisalocalpermanentresidentthereisnochanceofhisabscondinglearnedappopposedthepetitionersprayerforbailbuthasnotdisputed']...) from 71312 documents (total 22753017 corpus positions)\n",
      "2018-03-15 17:12:44,221 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 17:14:25,736 : INFO : adding document #10000 to Dictionary(166319 unique tokens: [u'awasalready', u'withoutcomplying', u'regulationsclause', u'whichincites', u'rightlyfiled']...)\n",
      "2018-03-15 17:16:06,138 : INFO : adding document #20000 to Dictionary(254054 unique tokens: [u'shecaught', u'withoutcomplying', u'suchdefinition', u'whichincites', u'otheractivitesas']...)\n",
      "2018-03-15 17:17:54,792 : INFO : adding document #30000 to Dictionary(345873 unique tokens: [u'purposenarendra', u'shecaught', u'withoutcomplying', u'suchdefinition', u'whichincites']...)\n",
      "2018-03-15 17:18:19,499 : INFO : built Dictionary(362771 unique tokens: [u'purposenarendra', u'withoutcomplying', u'whichincites', u'metersand', u'brokenetowel']...) from 32240 documents (total 23970776 corpus positions)\n",
      "2018-03-15 17:18:19,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 17:19:06,305 : INFO : built Dictionary(40016 unique tokens: [u'noezi', u'deferment', u'tapodip', u'woods', u'spiders']...) from 1210 documents (total 4552202 corpus positions)\n",
      "2018-03-15 17:19:12,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-15 17:26:55,296 : INFO : adding document #10000 to Dictionary(222963 unique tokens: [u'kajumal', u'vang', u'nunnery', u'jaluram', u'balipura']...)\n",
      "2018-03-15 17:31:35,411 : INFO : adding document #20000 to Dictionary(305098 unique tokens: [u'refarred', u'kajumal', u'vang', u'nunnery', u'jaluram']...)\n",
      "2018-03-15 17:36:35,713 : INFO : adding document #30000 to Dictionary(366921 unique tokens: [u'refarred', u'santawana', u'jaluram', u'justiceability', u'pricepremium']...)\n",
      "2018-03-15 17:36:39,934 : INFO : built Dictionary(367860 unique tokens: [u'refarred', u'santawana', u'jaluram', u'justiceability', u'pricepremium']...) from 30091 documents (total 110555608 corpus positions)\n",
      "2018-03-15 17:36:39,940 : INFO : saving Dictionary object under newdictionary_DCDRC, separately None\n",
      "2018-03-15 17:36:42,755 : INFO : saved newdictionary_DCDRC\n",
      "2018-03-15 17:36:42,763 : INFO : saving Dictionary object under newdictionary_NCDRC, separately None\n",
      "2018-03-15 17:36:43,194 : INFO : saved newdictionary_NCDRC\n",
      "2018-03-15 17:36:43,197 : INFO : saving Dictionary object under newdictionary_SCDRC, separately None\n",
      "2018-03-15 17:36:44,642 : INFO : saved newdictionary_SCDRC\n",
      "2018-03-15 17:36:44,645 : INFO : saving Dictionary object under newdictionary_DelhiHC, separately None\n",
      "2018-03-15 17:36:45,224 : INFO : saved newdictionary_DelhiHC\n",
      "2018-03-15 17:36:45,227 : INFO : saving Dictionary object under newdictionary_JharkhandHC, separately None\n",
      "2018-03-15 17:36:45,825 : INFO : saved newdictionary_JharkhandHC\n",
      "2018-03-15 17:36:45,828 : INFO : saving Dictionary object under newdictionary_JodhpurHC, separately None\n",
      "2018-03-15 17:36:46,408 : INFO : saved newdictionary_JodhpurHC\n",
      "2018-03-15 17:36:46,411 : INFO : saving Dictionary object under newdictionary_KolkataHC, separately None\n",
      "2018-03-15 17:36:46,480 : INFO : saved newdictionary_KolkataHC\n",
      "2018-03-15 17:36:46,482 : INFO : saving Dictionary object under newdictionary_SupremeCourt, separately None\n",
      "2018-03-15 17:36:47,104 : INFO : saved newdictionary_SupremeCourt\n"
     ]
    }
   ],
   "source": [
    "# Create all the court specific dictionaries\n",
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# ## check out dictionaries\n",
    "# dictionary_DCDRC.save_as_text('newdictionary_DCDRC.txt') \n",
    "# dictionary_NCDRC.save_as_text('newdictionary_NCDRC.txt') \n",
    "# dictionary_SCDRC.save_as_text('newdictionary_SCDRC.txt') \n",
    "# dictionary_DelhiHC.save_as_text('newdictionary_DelhiHC.txt') \n",
    "# dictionary_JharkhandHC.save_as_text('newdictionary_JharkhandHC.txt') \n",
    "# dictionary_JodhpurHC.save_as_text('newdictionary_JodhpurHC.txt') \n",
    "# dictionary_KolkataHC.save_as_text('newdictionary_KolkataHC.txt') \n",
    "# dictionary_SupremeCourt.save_as_text('newdictionary_SupremeCourt.txt') \n",
    "# Save actual dictionaries\n",
    "dictionary_DCDRC.save('newdictionary_DCDRC') \n",
    "dictionary_NCDRC.save('newdictionary_NCDRC') \n",
    "dictionary_SCDRC.save('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC.save('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC.save('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC.save('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC.save('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt.save('newdictionary_SupremeCourt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 22:10:21,977 : INFO : loading Dictionary object from newdictionary_DCDRC\n",
      "2018-03-15 22:11:02,548 : INFO : loaded newdictionary_DCDRC\n",
      "2018-03-15 22:11:08,706 : INFO : loading Dictionary object from newdictionary_NCDRC\n",
      "2018-03-15 22:11:09,319 : INFO : loaded newdictionary_NCDRC\n",
      "2018-03-15 22:11:09,601 : INFO : loading Dictionary object from newdictionary_SCDRC\n",
      "2018-03-15 22:11:12,065 : INFO : loaded newdictionary_SCDRC\n",
      "2018-03-15 22:11:12,213 : INFO : loading Dictionary object from newdictionary_DelhiHC\n",
      "2018-03-15 22:11:12,526 : INFO : loaded newdictionary_DelhiHC\n",
      "2018-03-15 22:11:12,744 : INFO : loading Dictionary object from newdictionary_JharkhandHC\n",
      "2018-03-15 22:11:14,246 : INFO : loaded newdictionary_JharkhandHC\n",
      "2018-03-15 22:11:14,394 : INFO : loading Dictionary object from newdictionary_JodhpurHC\n",
      "2018-03-15 22:11:15,084 : INFO : loaded newdictionary_JodhpurHC\n",
      "2018-03-15 22:11:15,256 : INFO : loading Dictionary object from newdictionary_KolkataHC\n",
      "2018-03-15 22:11:15,351 : INFO : loaded newdictionary_KolkataHC\n",
      "2018-03-15 22:11:15,397 : INFO : loading Dictionary object from newdictionary_SupremeCourt\n",
      "2018-03-15 22:11:16,625 : INFO : loaded newdictionary_SupremeCourt\n"
     ]
    }
   ],
   "source": [
    "# load those dictionaries\n",
    "dictionary_DCDRC = corpora.Dictionary.load('newdictionary_DCDRC') \n",
    "dictionary_NCDRC = corpora.Dictionary.load('newdictionary_NCDRC') \n",
    "dictionary_SCDRC = corpora.Dictionary.load('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC = corpora.Dictionary.load('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC = corpora.Dictionary.load('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC = corpora.Dictionary.load('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC = corpora.Dictionary.load('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt= corpora.Dictionary.load('newdictionary_SupremeCourt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 22:11:33,069 : INFO : discarding 1369350 tokens: [(u'all', 70934), (u'chempazhanthi', 3), (u'issued', 78740), (u'to', 120964), (u'beenakumariamember', 2), (u'th', 63408), (u'under', 94225), (u'narendranthis', 1), (u'cost', 69435), (u'further', 71943)]...\n",
      "2018-03-15 22:11:33,071 : INFO : keeping 244518 tokens which were in no less than 5 and no more than 61447 (=50.0%) documents\n",
      "2018-03-15 22:11:34,594 : INFO : resulting dictionary: Dictionary(244518 unique tokens: [u'filingoriginal', u'withoutcomplying', u'nonehru', u'partiesnilexhibits', u'metersand']...)\n",
      "2018-03-15 22:11:35,434 : INFO : discarding 181352 tokens: [(u'ccomplaints', 2), (u'docdocnoconsumercourtncdrcdocnotextnational', 2487), (u'to', 3481), (u'th', 2030), (u'under', 2879), (u'ofthe', 2455), (u'theexpansion', 2), (u'cost', 1925), (u'further', 2307), (u'deducteda', 2)]...\n",
      "2018-03-15 22:11:35,437 : INFO : keeping 27335 tokens which were in no less than 5 and no more than 1742 (=50.0%) documents\n",
      "2018-03-15 22:11:35,668 : INFO : resulting dictionary: Dictionary(27335 unique tokens: [u'ofexpenses', u'hanging', u'commissionchandigarh', u'paymentschedule', u'thecover']...)\n",
      "2018-03-15 22:11:37,871 : INFO : discarding 703260 tokens: [(u'through', 30215), (u'its', 29870), (u'before', 42767), (u'member', 49365), (u'had', 33225), (u'to', 49552), (u'under', 33430), (u'has', 42509), (u'complaint', 43812), (u'not', 46306)]...\n",
      "2018-03-15 22:11:37,871 : INFO : keeping 117328 tokens which were in no less than 5 and no more than 28372 (=50.0%) documents\n",
      "2018-03-15 22:11:38,605 : INFO : resulting dictionary: Dictionary(117328 unique tokens: [u'blgn', u'nqlrh', u'ksrinivasa', u'gai', u'withdamages']...)\n",
      "2018-03-15 22:11:39,561 : INFO : discarding 256320 tokens: [(u'coram', 29494), (u'textdoc', 32978), (u'allowed', 27567), (u'high', 33958), (u'see', 24721), (u'through', 31016), (u'at', 33670), (u'in', 33983), (u'digest', 20695), (u'court', 33986)]...\n",
      "2018-03-15 22:11:39,562 : INFO : keeping 62401 tokens which were in no less than 5 and no more than 17004 (=50.0%) documents\n",
      "2018-03-15 22:11:39,903 : INFO : resulting dictionary: Dictionary(62401 unique tokens: [u'uoirespondent', u'corelating', u'causans', u'deferment', u'sowell']...)\n",
      "2018-03-15 22:11:40,605 : INFO : discarding 221608 tokens: [(u'justice', 55009), (u'to', 59423), (u'has', 42172), (u'for', 61809), (u'state', 48737), (u'jharkhand', 56620), (u'by', 39897), (u'on', 48744), (u'of', 65411), (u'honble', 55847)]...\n",
      "2018-03-15 22:11:40,608 : INFO : keeping 32216 tokens which were in no less than 5 and no more than 35656 (=50.0%) documents\n",
      "2018-03-15 22:11:40,855 : INFO : resulting dictionary: Dictionary(32216 unique tokens: [u'aloksinghjmanishtextdoc', u'woods', u'hanging', u'heardlearnedcounsel', u'regularize']...)\n",
      "2018-03-15 22:11:41,862 : INFO : discarding 273192 tokens: [(u'to', 28428), (u'petition', 21557), (u'rajasthan', 21033), (u'palidecide', 1), (u'for', 31698), (u'case', 18250), (u'state', 17641), (u'clarificationsought', 1), (u'learned', 25261), (u'be', 23163)]...\n",
      "2018-03-15 22:11:41,865 : INFO : keeping 89579 tokens which were in no less than 5 and no more than 16120 (=50.0%) documents\n",
      "2018-03-15 22:11:42,433 : INFO : resulting dictionary: Dictionary(89579 unique tokens: [u'casesangeet', u'withoutcomplying', u'sbawejatextdoc', u'woods', u'clotted']...)\n",
      "2018-03-15 22:11:42,634 : INFO : discarding 28716 tokens: [(u'limited', 713), (u'all', 1084), (u'issued', 797), (u'scc', 607), (u'concerned', 643), (u'to', 1208), (u'those', 642), (u'th', 893), (u'under', 1157), (u'division', 612)]...\n",
      "2018-03-15 22:11:42,635 : INFO : keeping 11300 tokens which were in no less than 5 and no more than 605 (=50.0%) documents\n",
      "2018-03-15 22:11:42,674 : INFO : resulting dictionary: Dictionary(11300 unique tokens: [u'writings', u'indranil', u'deferment', u'yellow', u'four']...)\n",
      "2018-03-15 22:11:43,832 : INFO : discarding 304721 tokens: [(u'all', 24105), (u'allowed', 20542), (u'jjin', 3), (u'concerned', 16778), (u'to', 29817), (u'present', 19620), (u'under', 28062), (u'must', 18635), (u'did', 20423), (u'these', 22098)]...\n",
      "2018-03-15 22:11:43,835 : INFO : keeping 63139 tokens which were in no less than 5 and no more than 15045 (=50.0%) documents\n",
      "2018-03-15 22:11:44,181 : INFO : resulting dictionary: Dictionary(63139 unique tokens: [u'corelating', u'deferment', u'pricepremium', u'pawnor', u'fortythird']...)\n",
      "2018-03-15 22:11:46,184 : INFO : discarding 319010 tokens: [(u'diagrammap', 5), (u'withoutcomplying', 60), (u'bkanta', 11), (u'nonehru', 5), (u'withdamages', 67), (u'distributers', 10), (u'pricepremium', 67), (u'pawnor', 16), (u'makesout', 14), (u'onemrsirene', 45)]...\n",
      "2018-03-15 22:11:46,184 : INFO : keeping 47131 tokens which were in no less than 100 and no more than 175992 (=50.0%) documents\n",
      "2018-03-15 22:11:46,526 : INFO : resulting dictionary: Dictionary(47131 unique tokens: [u'majumder', u'aloksinghjmanishtextdoc', u'deferment', u'alsoserved', u'woods']...)\n"
     ]
    }
   ],
   "source": [
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "\n",
    "dictionary.filter_extremes(no_below=100)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 22:12:07,061 : INFO : saving Dictionary object under newdictionary, separately None\n",
      "2018-03-15 22:12:07,253 : INFO : saved newdictionary\n"
     ]
    }
   ],
   "source": [
    "# save the dictionary\n",
    "dictionary.save('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 22:12:08,983 : INFO : loading Dictionary object from newdictionary\n",
      "2018-03-15 22:12:09,038 : INFO : loaded newdictionary\n"
     ]
    }
   ],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47131"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-15 22:12:20,388 : INFO : storing corpus in Matrix Market format to newcorpus.mm\n",
      "2018-03-15 22:12:20,953 : INFO : saving sparse matrix to newcorpus.mm\n",
      "2018-03-15 22:12:21,976 : INFO : PROGRESS: saving document #0\n",
      "2018-03-15 22:12:56,253 : INFO : PROGRESS: saving document #1000\n",
      "2018-03-15 22:13:28,625 : INFO : PROGRESS: saving document #2000\n",
      "2018-03-15 22:13:53,970 : INFO : PROGRESS: saving document #3000\n",
      "2018-03-15 22:14:20,236 : INFO : PROGRESS: saving document #4000\n",
      "2018-03-15 22:14:49,039 : INFO : PROGRESS: saving document #5000\n",
      "2018-03-15 22:15:16,295 : INFO : PROGRESS: saving document #6000\n",
      "2018-03-15 22:15:39,174 : INFO : PROGRESS: saving document #7000\n",
      "2018-03-15 22:16:16,697 : INFO : PROGRESS: saving document #8000\n",
      "2018-03-15 22:16:36,766 : INFO : PROGRESS: saving document #9000\n",
      "2018-03-15 22:17:14,779 : INFO : PROGRESS: saving document #10000\n",
      "2018-03-15 22:17:34,558 : INFO : PROGRESS: saving document #11000\n",
      "2018-03-15 22:18:03,427 : INFO : PROGRESS: saving document #12000\n",
      "2018-03-15 22:18:30,677 : INFO : PROGRESS: saving document #13000\n",
      "2018-03-15 22:18:55,819 : INFO : PROGRESS: saving document #14000\n",
      "2018-03-15 22:19:26,713 : INFO : PROGRESS: saving document #15000\n",
      "2018-03-15 22:19:54,407 : INFO : PROGRESS: saving document #16000\n",
      "2018-03-15 22:20:14,476 : INFO : PROGRESS: saving document #17000\n",
      "2018-03-15 22:20:36,805 : INFO : PROGRESS: saving document #18000\n",
      "2018-03-15 22:21:00,793 : INFO : PROGRESS: saving document #19000\n",
      "2018-03-15 22:21:22,499 : INFO : PROGRESS: saving document #20000\n",
      "2018-03-15 22:21:41,693 : INFO : PROGRESS: saving document #21000\n",
      "2018-03-15 22:22:00,173 : INFO : PROGRESS: saving document #22000\n",
      "2018-03-15 22:22:26,480 : INFO : PROGRESS: saving document #23000\n",
      "2018-03-15 22:22:58,825 : INFO : PROGRESS: saving document #24000\n",
      "2018-03-15 22:23:27,440 : INFO : PROGRESS: saving document #25000\n",
      "2018-03-15 22:24:01,694 : INFO : PROGRESS: saving document #26000\n",
      "2018-03-15 22:24:22,006 : INFO : PROGRESS: saving document #27000\n",
      "2018-03-15 22:24:45,976 : INFO : PROGRESS: saving document #28000\n",
      "2018-03-15 22:25:08,918 : INFO : PROGRESS: saving document #29000\n",
      "2018-03-15 22:25:27,834 : INFO : PROGRESS: saving document #30000\n",
      "2018-03-15 22:25:41,973 : INFO : PROGRESS: saving document #31000\n",
      "2018-03-15 22:25:57,273 : INFO : PROGRESS: saving document #32000\n",
      "2018-03-15 22:26:22,617 : INFO : PROGRESS: saving document #33000\n",
      "2018-03-15 22:26:44,943 : INFO : PROGRESS: saving document #34000\n",
      "2018-03-15 22:27:15,917 : INFO : PROGRESS: saving document #35000\n",
      "2018-03-15 22:27:52,938 : INFO : PROGRESS: saving document #36000\n",
      "2018-03-15 22:28:16,598 : INFO : PROGRESS: saving document #37000\n",
      "2018-03-15 22:28:29,388 : INFO : PROGRESS: saving document #38000\n",
      "2018-03-15 22:28:50,142 : INFO : PROGRESS: saving document #39000\n",
      "2018-03-15 22:29:09,740 : INFO : PROGRESS: saving document #40000\n",
      "2018-03-15 22:29:28,213 : INFO : PROGRESS: saving document #41000\n",
      "2018-03-15 22:29:41,002 : INFO : PROGRESS: saving document #42000\n",
      "2018-03-15 22:29:56,565 : INFO : PROGRESS: saving document #43000\n",
      "2018-03-15 22:30:14,736 : INFO : PROGRESS: saving document #44000\n",
      "2018-03-15 22:30:34,700 : INFO : PROGRESS: saving document #45000\n",
      "2018-03-15 22:30:53,295 : INFO : PROGRESS: saving document #46000\n",
      "2018-03-15 22:31:12,394 : INFO : PROGRESS: saving document #47000\n",
      "2018-03-15 22:31:48,262 : INFO : PROGRESS: saving document #48000\n",
      "2018-03-15 22:32:11,042 : INFO : PROGRESS: saving document #49000\n",
      "2018-03-15 22:32:25,779 : INFO : PROGRESS: saving document #50000\n",
      "2018-03-15 22:32:37,703 : INFO : PROGRESS: saving document #51000\n",
      "2018-03-15 22:32:52,566 : INFO : PROGRESS: saving document #52000\n",
      "2018-03-15 22:33:07,513 : INFO : PROGRESS: saving document #53000\n",
      "2018-03-15 22:33:19,322 : INFO : PROGRESS: saving document #54000\n",
      "2018-03-15 22:33:44,375 : INFO : PROGRESS: saving document #55000\n",
      "2018-03-15 22:33:59,644 : INFO : PROGRESS: saving document #56000\n",
      "2018-03-15 22:34:17,428 : INFO : PROGRESS: saving document #57000\n",
      "2018-03-15 22:34:37,404 : INFO : PROGRESS: saving document #58000\n",
      "2018-03-15 22:34:54,657 : INFO : PROGRESS: saving document #59000\n",
      "2018-03-15 22:35:02,904 : INFO : PROGRESS: saving document #60000\n",
      "2018-03-15 22:35:15,959 : INFO : PROGRESS: saving document #61000\n",
      "2018-03-15 22:35:26,943 : INFO : PROGRESS: saving document #62000\n",
      "2018-03-15 22:35:39,803 : INFO : PROGRESS: saving document #63000\n",
      "2018-03-15 22:35:50,673 : INFO : PROGRESS: saving document #64000\n",
      "2018-03-15 22:36:06,426 : INFO : PROGRESS: saving document #65000\n",
      "2018-03-15 22:36:18,928 : INFO : PROGRESS: saving document #66000\n",
      "2018-03-15 22:36:32,710 : INFO : PROGRESS: saving document #67000\n",
      "2018-03-15 22:36:46,796 : INFO : PROGRESS: saving document #68000\n",
      "2018-03-15 22:37:01,168 : INFO : PROGRESS: saving document #69000\n",
      "2018-03-15 22:37:12,792 : INFO : PROGRESS: saving document #70000\n",
      "2018-03-15 22:37:29,349 : INFO : PROGRESS: saving document #71000\n",
      "2018-03-15 22:37:45,585 : INFO : PROGRESS: saving document #72000\n",
      "2018-03-15 22:38:00,869 : INFO : PROGRESS: saving document #73000\n",
      "2018-03-15 22:38:13,677 : INFO : PROGRESS: saving document #74000\n",
      "2018-03-15 22:38:25,131 : INFO : PROGRESS: saving document #75000\n",
      "2018-03-15 22:38:39,569 : INFO : PROGRESS: saving document #76000\n",
      "2018-03-15 22:38:54,385 : INFO : PROGRESS: saving document #77000\n",
      "2018-03-15 22:39:13,282 : INFO : PROGRESS: saving document #78000\n",
      "2018-03-15 22:39:30,937 : INFO : PROGRESS: saving document #79000\n",
      "2018-03-15 22:39:45,779 : INFO : PROGRESS: saving document #80000\n",
      "2018-03-15 22:40:05,084 : INFO : PROGRESS: saving document #81000\n",
      "2018-03-15 22:40:22,846 : INFO : PROGRESS: saving document #82000\n",
      "2018-03-15 22:40:38,453 : INFO : PROGRESS: saving document #83000\n",
      "2018-03-15 22:40:57,855 : INFO : PROGRESS: saving document #84000\n",
      "2018-03-15 22:41:18,680 : INFO : PROGRESS: saving document #85000\n",
      "2018-03-15 22:41:35,645 : INFO : PROGRESS: saving document #86000\n",
      "2018-03-15 22:41:53,609 : INFO : PROGRESS: saving document #87000\n",
      "2018-03-15 22:42:07,315 : INFO : PROGRESS: saving document #88000\n",
      "2018-03-15 22:42:23,680 : INFO : PROGRESS: saving document #89000\n",
      "2018-03-15 22:42:46,503 : INFO : PROGRESS: saving document #90000\n",
      "2018-03-15 22:43:10,221 : INFO : PROGRESS: saving document #91000\n",
      "2018-03-15 22:43:33,150 : INFO : PROGRESS: saving document #92000\n",
      "2018-03-15 22:43:59,867 : INFO : PROGRESS: saving document #93000\n",
      "2018-03-15 22:44:19,450 : INFO : PROGRESS: saving document #94000\n",
      "2018-03-15 22:44:49,358 : INFO : PROGRESS: saving document #95000\n",
      "2018-03-15 22:45:10,019 : INFO : PROGRESS: saving document #96000\n",
      "2018-03-15 22:45:36,071 : INFO : PROGRESS: saving document #97000\n",
      "2018-03-15 22:45:58,631 : INFO : PROGRESS: saving document #98000\n",
      "2018-03-15 22:46:20,084 : INFO : PROGRESS: saving document #99000\n",
      "2018-03-15 22:46:42,128 : INFO : PROGRESS: saving document #100000\n",
      "2018-03-15 22:46:59,380 : INFO : PROGRESS: saving document #101000\n",
      "2018-03-15 22:47:20,841 : INFO : PROGRESS: saving document #102000\n",
      "2018-03-15 22:47:41,058 : INFO : PROGRESS: saving document #103000\n",
      "2018-03-15 22:47:59,976 : INFO : PROGRESS: saving document #104000\n",
      "2018-03-15 22:48:20,585 : INFO : PROGRESS: saving document #105000\n",
      "2018-03-15 22:48:46,084 : INFO : PROGRESS: saving document #106000\n",
      "2018-03-15 22:49:08,325 : INFO : PROGRESS: saving document #107000\n",
      "2018-03-15 22:49:30,980 : INFO : PROGRESS: saving document #108000\n",
      "2018-03-15 22:49:45,325 : INFO : PROGRESS: saving document #109000\n",
      "2018-03-15 22:50:02,513 : INFO : PROGRESS: saving document #110000\n",
      "2018-03-15 22:50:17,355 : INFO : PROGRESS: saving document #111000\n",
      "2018-03-15 22:50:28,183 : INFO : PROGRESS: saving document #112000\n",
      "2018-03-15 22:50:43,575 : INFO : PROGRESS: saving document #113000\n",
      "2018-03-15 22:50:59,194 : INFO : PROGRESS: saving document #114000\n",
      "2018-03-15 22:51:08,253 : INFO : PROGRESS: saving document #115000\n",
      "2018-03-15 22:51:16,670 : INFO : PROGRESS: saving document #116000\n",
      "2018-03-15 22:51:29,049 : INFO : PROGRESS: saving document #117000\n",
      "2018-03-15 22:51:42,901 : INFO : PROGRESS: saving document #118000\n",
      "2018-03-15 22:52:00,618 : INFO : PROGRESS: saving document #119000\n",
      "2018-03-15 22:52:12,670 : INFO : PROGRESS: saving document #120000\n",
      "2018-03-15 22:52:30,586 : INFO : PROGRESS: saving document #121000\n",
      "2018-03-15 22:52:48,180 : INFO : PROGRESS: saving document #122000\n",
      "2018-03-15 22:53:10,253 : INFO : PROGRESS: saving document #123000\n",
      "2018-03-15 22:53:28,378 : INFO : PROGRESS: saving document #124000\n",
      "2018-03-15 22:53:51,163 : INFO : PROGRESS: saving document #125000\n",
      "2018-03-15 22:54:10,655 : INFO : PROGRESS: saving document #126000\n",
      "2018-03-15 22:54:58,756 : INFO : PROGRESS: saving document #127000\n",
      "2018-03-15 22:55:16,282 : INFO : PROGRESS: saving document #128000\n",
      "2018-03-15 22:55:33,546 : INFO : PROGRESS: saving document #129000\n",
      "2018-03-15 22:55:46,017 : INFO : PROGRESS: saving document #130000\n",
      "2018-03-15 22:55:56,617 : INFO : PROGRESS: saving document #131000\n",
      "2018-03-15 22:56:07,276 : INFO : PROGRESS: saving document #132000\n",
      "2018-03-15 22:56:18,066 : INFO : PROGRESS: saving document #133000\n",
      "2018-03-15 22:56:31,069 : INFO : PROGRESS: saving document #134000\n",
      "2018-03-15 22:56:40,742 : INFO : PROGRESS: saving document #135000\n",
      "2018-03-15 22:56:51,233 : INFO : PROGRESS: saving document #136000\n",
      "2018-03-15 22:57:02,934 : INFO : PROGRESS: saving document #137000\n",
      "2018-03-15 22:57:16,092 : INFO : PROGRESS: saving document #138000\n",
      "2018-03-15 22:57:27,296 : INFO : PROGRESS: saving document #139000\n",
      "2018-03-15 22:57:37,354 : INFO : PROGRESS: saving document #140000\n",
      "2018-03-15 22:57:50,144 : INFO : PROGRESS: saving document #141000\n",
      "2018-03-15 22:58:04,269 : INFO : PROGRESS: saving document #142000\n",
      "2018-03-15 22:58:16,576 : INFO : PROGRESS: saving document #143000\n",
      "2018-03-15 22:58:30,632 : INFO : PROGRESS: saving document #144000\n",
      "2018-03-15 22:58:46,151 : INFO : PROGRESS: saving document #145000\n",
      "2018-03-15 22:58:59,058 : INFO : PROGRESS: saving document #146000\n",
      "2018-03-15 22:59:08,595 : INFO : PROGRESS: saving document #147000\n",
      "2018-03-15 22:59:22,717 : INFO : PROGRESS: saving document #148000\n",
      "2018-03-15 22:59:34,007 : INFO : PROGRESS: saving document #149000\n",
      "2018-03-15 22:59:49,099 : INFO : PROGRESS: saving document #150000\n",
      "2018-03-15 23:00:03,282 : INFO : PROGRESS: saving document #151000\n",
      "2018-03-15 23:00:17,071 : INFO : PROGRESS: saving document #152000\n",
      "2018-03-15 23:00:31,140 : INFO : PROGRESS: saving document #153000\n",
      "2018-03-15 23:00:40,388 : INFO : PROGRESS: saving document #154000\n",
      "2018-03-15 23:00:51,357 : INFO : PROGRESS: saving document #155000\n",
      "2018-03-15 23:01:11,015 : INFO : PROGRESS: saving document #156000\n",
      "2018-03-15 23:01:35,625 : INFO : PROGRESS: saving document #157000\n",
      "2018-03-15 23:01:54,644 : INFO : PROGRESS: saving document #158000\n",
      "2018-03-15 23:02:08,368 : INFO : PROGRESS: saving document #159000\n",
      "2018-03-15 23:02:20,206 : INFO : PROGRESS: saving document #160000\n",
      "2018-03-15 23:02:34,292 : INFO : PROGRESS: saving document #161000\n",
      "2018-03-15 23:02:51,058 : INFO : PROGRESS: saving document #162000\n",
      "2018-03-15 23:03:08,829 : INFO : PROGRESS: saving document #163000\n",
      "2018-03-15 23:03:28,460 : INFO : PROGRESS: saving document #164000\n",
      "2018-03-15 23:03:43,828 : INFO : PROGRESS: saving document #165000\n",
      "2018-03-15 23:04:00,447 : INFO : PROGRESS: saving document #166000\n",
      "2018-03-15 23:04:18,256 : INFO : PROGRESS: saving document #167000\n",
      "2018-03-15 23:04:33,362 : INFO : PROGRESS: saving document #168000\n",
      "2018-03-15 23:04:48,535 : INFO : PROGRESS: saving document #169000\n",
      "2018-03-15 23:05:03,798 : INFO : PROGRESS: saving document #170000\n",
      "2018-03-15 23:05:21,158 : INFO : PROGRESS: saving document #171000\n",
      "2018-03-15 23:05:35,631 : INFO : PROGRESS: saving document #172000\n",
      "2018-03-15 23:05:47,690 : INFO : PROGRESS: saving document #173000\n",
      "2018-03-15 23:06:06,474 : INFO : PROGRESS: saving document #174000\n",
      "2018-03-15 23:06:30,055 : INFO : PROGRESS: saving document #175000\n",
      "2018-03-15 23:06:46,904 : INFO : PROGRESS: saving document #176000\n",
      "2018-03-15 23:06:58,770 : INFO : PROGRESS: saving document #177000\n",
      "2018-03-15 23:07:11,163 : INFO : PROGRESS: saving document #178000\n",
      "2018-03-15 23:07:24,246 : INFO : PROGRESS: saving document #179000\n",
      "2018-03-15 23:07:48,882 : INFO : PROGRESS: saving document #180000\n",
      "2018-03-15 23:08:03,448 : INFO : PROGRESS: saving document #181000\n",
      "2018-03-15 23:08:26,266 : INFO : PROGRESS: saving document #182000\n",
      "2018-03-15 23:08:40,732 : INFO : PROGRESS: saving document #183000\n",
      "2018-03-15 23:09:30,329 : INFO : PROGRESS: saving document #184000\n",
      "2018-03-15 23:09:56,888 : INFO : PROGRESS: saving document #185000\n",
      "2018-03-15 23:10:23,948 : INFO : PROGRESS: saving document #186000\n",
      "2018-03-15 23:10:55,529 : INFO : PROGRESS: saving document #187000\n",
      "2018-03-15 23:11:24,368 : INFO : PROGRESS: saving document #188000\n",
      "2018-03-15 23:11:51,905 : INFO : PROGRESS: saving document #189000\n",
      "2018-03-15 23:12:28,002 : INFO : PROGRESS: saving document #190000\n",
      "2018-03-15 23:13:02,440 : INFO : PROGRESS: saving document #191000\n",
      "2018-03-15 23:13:30,683 : INFO : PROGRESS: saving document #192000\n",
      "2018-03-15 23:13:59,128 : INFO : PROGRESS: saving document #193000\n",
      "2018-03-15 23:14:33,953 : INFO : PROGRESS: saving document #194000\n",
      "2018-03-15 23:15:01,381 : INFO : PROGRESS: saving document #195000\n",
      "2018-03-15 23:15:32,612 : INFO : PROGRESS: saving document #196000\n",
      "2018-03-15 23:15:57,533 : INFO : PROGRESS: saving document #197000\n",
      "2018-03-15 23:16:20,891 : INFO : PROGRESS: saving document #198000\n",
      "2018-03-15 23:16:47,016 : INFO : PROGRESS: saving document #199000\n",
      "2018-03-15 23:17:20,986 : INFO : PROGRESS: saving document #200000\n",
      "2018-03-15 23:17:44,164 : INFO : PROGRESS: saving document #201000\n",
      "2018-03-15 23:18:17,151 : INFO : PROGRESS: saving document #202000\n",
      "2018-03-15 23:18:45,645 : INFO : PROGRESS: saving document #203000\n",
      "2018-03-15 23:19:08,213 : INFO : PROGRESS: saving document #204000\n",
      "2018-03-15 23:19:36,691 : INFO : PROGRESS: saving document #205000\n",
      "2018-03-15 23:20:03,575 : INFO : PROGRESS: saving document #206000\n",
      "2018-03-15 23:20:34,589 : INFO : PROGRESS: saving document #207000\n",
      "2018-03-15 23:20:54,757 : INFO : PROGRESS: saving document #208000\n",
      "2018-03-15 23:21:21,984 : INFO : PROGRESS: saving document #209000\n",
      "2018-03-15 23:21:48,295 : INFO : PROGRESS: saving document #210000\n",
      "2018-03-15 23:22:18,305 : INFO : PROGRESS: saving document #211000\n",
      "2018-03-15 23:22:51,286 : INFO : PROGRESS: saving document #212000\n",
      "2018-03-15 23:23:23,934 : INFO : PROGRESS: saving document #213000\n",
      "2018-03-15 23:23:53,927 : INFO : PROGRESS: saving document #214000\n",
      "2018-03-15 23:24:22,548 : INFO : PROGRESS: saving document #215000\n",
      "2018-03-15 23:24:44,509 : INFO : PROGRESS: saving document #216000\n",
      "2018-03-15 23:25:13,875 : INFO : PROGRESS: saving document #217000\n",
      "2018-03-15 23:25:54,046 : INFO : PROGRESS: saving document #218000\n",
      "2018-03-15 23:26:05,740 : INFO : PROGRESS: saving document #219000\n",
      "2018-03-15 23:26:20,890 : INFO : PROGRESS: saving document #220000\n",
      "2018-03-15 23:26:36,471 : INFO : PROGRESS: saving document #221000\n",
      "2018-03-15 23:26:50,266 : INFO : PROGRESS: saving document #222000\n",
      "2018-03-15 23:27:02,923 : INFO : PROGRESS: saving document #223000\n",
      "2018-03-15 23:27:10,802 : INFO : PROGRESS: saving document #224000\n",
      "2018-03-15 23:27:19,237 : INFO : PROGRESS: saving document #225000\n",
      "2018-03-15 23:27:26,019 : INFO : PROGRESS: saving document #226000\n",
      "2018-03-15 23:27:31,967 : INFO : PROGRESS: saving document #227000\n",
      "2018-03-15 23:27:38,980 : INFO : PROGRESS: saving document #228000\n",
      "2018-03-15 23:27:46,845 : INFO : PROGRESS: saving document #229000\n",
      "2018-03-15 23:27:53,487 : INFO : PROGRESS: saving document #230000\n",
      "2018-03-15 23:28:02,673 : INFO : PROGRESS: saving document #231000\n",
      "2018-03-15 23:28:09,140 : INFO : PROGRESS: saving document #232000\n",
      "2018-03-15 23:28:16,286 : INFO : PROGRESS: saving document #233000\n",
      "2018-03-15 23:28:22,063 : INFO : PROGRESS: saving document #234000\n",
      "2018-03-15 23:28:27,963 : INFO : PROGRESS: saving document #235000\n",
      "2018-03-15 23:28:34,864 : INFO : PROGRESS: saving document #236000\n",
      "2018-03-15 23:28:40,589 : INFO : PROGRESS: saving document #237000\n",
      "2018-03-15 23:28:45,776 : INFO : PROGRESS: saving document #238000\n",
      "2018-03-15 23:28:52,628 : INFO : PROGRESS: saving document #239000\n",
      "2018-03-15 23:28:59,207 : INFO : PROGRESS: saving document #240000\n",
      "2018-03-15 23:29:04,528 : INFO : PROGRESS: saving document #241000\n",
      "2018-03-15 23:29:11,716 : INFO : PROGRESS: saving document #242000\n",
      "2018-03-15 23:29:19,009 : INFO : PROGRESS: saving document #243000\n",
      "2018-03-15 23:29:26,519 : INFO : PROGRESS: saving document #244000\n",
      "2018-03-15 23:29:33,328 : INFO : PROGRESS: saving document #245000\n",
      "2018-03-15 23:29:41,473 : INFO : PROGRESS: saving document #246000\n",
      "2018-03-15 23:29:50,617 : INFO : PROGRESS: saving document #247000\n",
      "2018-03-15 23:30:00,131 : INFO : PROGRESS: saving document #248000\n",
      "2018-03-15 23:30:08,637 : INFO : PROGRESS: saving document #249000\n",
      "2018-03-15 23:30:20,311 : INFO : PROGRESS: saving document #250000\n",
      "2018-03-15 23:30:34,823 : INFO : PROGRESS: saving document #251000\n",
      "2018-03-15 23:30:51,361 : INFO : PROGRESS: saving document #252000\n",
      "2018-03-15 23:31:01,177 : INFO : PROGRESS: saving document #253000\n",
      "2018-03-15 23:31:10,769 : INFO : PROGRESS: saving document #254000\n",
      "2018-03-15 23:31:19,716 : INFO : PROGRESS: saving document #255000\n",
      "2018-03-15 23:31:27,434 : INFO : PROGRESS: saving document #256000\n",
      "2018-03-15 23:31:33,964 : INFO : PROGRESS: saving document #257000\n",
      "2018-03-15 23:31:39,292 : INFO : PROGRESS: saving document #258000\n",
      "2018-03-15 23:31:44,457 : INFO : PROGRESS: saving document #259000\n",
      "2018-03-15 23:31:48,911 : INFO : PROGRESS: saving document #260000\n",
      "2018-03-15 23:31:53,173 : INFO : PROGRESS: saving document #261000\n",
      "2018-03-15 23:31:57,832 : INFO : PROGRESS: saving document #262000\n",
      "2018-03-15 23:32:03,568 : INFO : PROGRESS: saving document #263000\n",
      "2018-03-15 23:32:09,102 : INFO : PROGRESS: saving document #264000\n",
      "2018-03-15 23:32:15,160 : INFO : PROGRESS: saving document #265000\n",
      "2018-03-15 23:32:20,565 : INFO : PROGRESS: saving document #266000\n",
      "2018-03-15 23:32:26,654 : INFO : PROGRESS: saving document #267000\n",
      "2018-03-15 23:32:34,344 : INFO : PROGRESS: saving document #268000\n",
      "2018-03-15 23:32:39,979 : INFO : PROGRESS: saving document #269000\n",
      "2018-03-15 23:32:47,295 : INFO : PROGRESS: saving document #270000\n",
      "2018-03-15 23:32:57,627 : INFO : PROGRESS: saving document #271000\n",
      "2018-03-15 23:33:05,782 : INFO : PROGRESS: saving document #272000\n",
      "2018-03-15 23:33:14,839 : INFO : PROGRESS: saving document #273000\n",
      "2018-03-15 23:33:22,035 : INFO : PROGRESS: saving document #274000\n",
      "2018-03-15 23:33:29,846 : INFO : PROGRESS: saving document #275000\n",
      "2018-03-15 23:33:46,559 : INFO : PROGRESS: saving document #276000\n",
      "2018-03-15 23:33:57,720 : INFO : PROGRESS: saving document #277000\n",
      "2018-03-15 23:34:08,403 : INFO : PROGRESS: saving document #278000\n",
      "2018-03-15 23:34:17,607 : INFO : PROGRESS: saving document #279000\n",
      "2018-03-15 23:34:25,269 : INFO : PROGRESS: saving document #280000\n",
      "2018-03-15 23:34:34,335 : INFO : PROGRESS: saving document #281000\n",
      "2018-03-15 23:34:42,234 : INFO : PROGRESS: saving document #282000\n",
      "2018-03-15 23:34:51,698 : INFO : PROGRESS: saving document #283000\n",
      "2018-03-15 23:34:57,637 : INFO : PROGRESS: saving document #284000\n",
      "2018-03-15 23:35:03,272 : INFO : PROGRESS: saving document #285000\n",
      "2018-03-15 23:35:09,167 : INFO : PROGRESS: saving document #286000\n",
      "2018-03-15 23:35:13,398 : INFO : PROGRESS: saving document #287000\n",
      "2018-03-15 23:35:17,506 : INFO : PROGRESS: saving document #288000\n",
      "2018-03-15 23:35:34,507 : INFO : PROGRESS: saving document #289000\n",
      "2018-03-15 23:35:47,154 : INFO : PROGRESS: saving document #290000\n",
      "2018-03-15 23:36:00,000 : INFO : PROGRESS: saving document #291000\n",
      "2018-03-15 23:36:11,700 : INFO : PROGRESS: saving document #292000\n",
      "2018-03-15 23:36:23,819 : INFO : PROGRESS: saving document #293000\n",
      "2018-03-15 23:36:34,825 : INFO : PROGRESS: saving document #294000\n",
      "2018-03-15 23:36:46,131 : INFO : PROGRESS: saving document #295000\n",
      "2018-03-15 23:36:55,019 : INFO : PROGRESS: saving document #296000\n",
      "2018-03-15 23:37:04,490 : INFO : PROGRESS: saving document #297000\n",
      "2018-03-15 23:37:13,058 : INFO : PROGRESS: saving document #298000\n",
      "2018-03-15 23:37:24,171 : INFO : PROGRESS: saving document #299000\n",
      "2018-03-15 23:37:32,260 : INFO : PROGRESS: saving document #300000\n",
      "2018-03-15 23:37:44,770 : INFO : PROGRESS: saving document #301000\n",
      "2018-03-15 23:37:52,891 : INFO : PROGRESS: saving document #302000\n",
      "2018-03-15 23:38:02,611 : INFO : PROGRESS: saving document #303000\n",
      "2018-03-15 23:38:14,150 : INFO : PROGRESS: saving document #304000\n",
      "2018-03-15 23:38:23,865 : INFO : PROGRESS: saving document #305000\n",
      "2018-03-15 23:38:34,703 : INFO : PROGRESS: saving document #306000\n",
      "2018-03-15 23:38:47,769 : INFO : PROGRESS: saving document #307000\n",
      "2018-03-15 23:39:01,184 : INFO : PROGRESS: saving document #308000\n",
      "2018-03-15 23:39:13,802 : INFO : PROGRESS: saving document #309000\n",
      "2018-03-15 23:39:32,104 : INFO : PROGRESS: saving document #310000\n",
      "2018-03-15 23:39:47,234 : INFO : PROGRESS: saving document #311000\n",
      "2018-03-15 23:40:03,375 : INFO : PROGRESS: saving document #312000\n",
      "2018-03-15 23:40:17,615 : INFO : PROGRESS: saving document #313000\n",
      "2018-03-15 23:40:29,142 : INFO : PROGRESS: saving document #314000\n",
      "2018-03-15 23:40:39,342 : INFO : PROGRESS: saving document #315000\n",
      "2018-03-15 23:40:51,855 : INFO : PROGRESS: saving document #316000\n",
      "2018-03-15 23:41:00,796 : INFO : PROGRESS: saving document #317000\n",
      "2018-03-15 23:41:07,933 : INFO : PROGRESS: saving document #318000\n",
      "2018-03-15 23:41:21,443 : INFO : PROGRESS: saving document #319000\n",
      "2018-03-15 23:41:35,019 : INFO : PROGRESS: saving document #320000\n",
      "2018-03-15 23:42:00,457 : INFO : PROGRESS: saving document #321000\n",
      "2018-03-15 23:42:58,036 : INFO : PROGRESS: saving document #322000\n",
      "2018-03-15 23:44:00,339 : INFO : PROGRESS: saving document #323000\n",
      "2018-03-15 23:44:49,391 : INFO : PROGRESS: saving document #324000\n",
      "2018-03-15 23:45:38,996 : INFO : PROGRESS: saving document #325000\n",
      "2018-03-15 23:46:18,421 : INFO : PROGRESS: saving document #326000\n",
      "2018-03-15 23:47:01,273 : INFO : PROGRESS: saving document #327000\n",
      "2018-03-15 23:47:49,779 : INFO : PROGRESS: saving document #328000\n",
      "2018-03-15 23:48:38,598 : INFO : PROGRESS: saving document #329000\n",
      "2018-03-15 23:49:31,798 : INFO : PROGRESS: saving document #330000\n",
      "2018-03-15 23:50:20,444 : INFO : PROGRESS: saving document #331000\n",
      "2018-03-15 23:51:09,723 : INFO : PROGRESS: saving document #332000\n",
      "2018-03-15 23:51:50,908 : INFO : PROGRESS: saving document #333000\n",
      "2018-03-15 23:52:30,871 : INFO : PROGRESS: saving document #334000\n",
      "2018-03-15 23:52:54,582 : INFO : PROGRESS: saving document #335000\n",
      "2018-03-15 23:53:23,177 : INFO : PROGRESS: saving document #336000\n",
      "2018-03-15 23:53:47,056 : INFO : PROGRESS: saving document #337000\n",
      "2018-03-15 23:54:16,265 : INFO : PROGRESS: saving document #338000\n",
      "2018-03-15 23:54:50,342 : INFO : PROGRESS: saving document #339000\n",
      "2018-03-15 23:55:22,691 : INFO : PROGRESS: saving document #340000\n",
      "2018-03-15 23:55:56,348 : INFO : PROGRESS: saving document #341000\n",
      "2018-03-15 23:56:33,102 : INFO : PROGRESS: saving document #342000\n",
      "2018-03-15 23:57:12,230 : INFO : PROGRESS: saving document #343000\n",
      "2018-03-15 23:57:45,525 : INFO : PROGRESS: saving document #344000\n",
      "2018-03-15 23:58:13,798 : INFO : PROGRESS: saving document #345000\n",
      "2018-03-15 23:58:37,290 : INFO : PROGRESS: saving document #346000\n",
      "2018-03-15 23:59:03,677 : INFO : PROGRESS: saving document #347000\n",
      "2018-03-15 23:59:33,467 : INFO : PROGRESS: saving document #348000\n",
      "2018-03-16 00:00:01,464 : INFO : PROGRESS: saving document #349000\n",
      "2018-03-16 00:00:35,134 : INFO : PROGRESS: saving document #350000\n",
      "2018-03-16 00:01:12,224 : INFO : PROGRESS: saving document #351000\n",
      "2018-03-16 00:01:49,940 : INFO : saved 351985x47131 matrix, density=0.689% (114247800/16589405035)\n",
      "2018-03-16 00:01:49,944 : INFO : saving MmCorpus index to newcorpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()\n",
    "# save BOW\n",
    "corpora.MmCorpus.serialize('newcorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 09:44:40,440 : INFO : loaded corpus index from newcorpus.mm.index\n",
      "2018-03-16 09:44:40,444 : INFO : initializing corpus reader from newcorpus.mm\n",
      "2018-03-16 09:44:40,579 : INFO : accepted corpus with 351985 documents, 47131 features, 114247800 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('newcorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tfidf model\n",
    "# Will take time as to generate a model it processes over the whole corpus\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "# save tfidf model\n",
    "tfidf.save('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### load tfidf model\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf wrapper coprus\n",
    "# doesn't take time - yield\n",
    "tfidf_corpus = tfidf[corpus]\n",
    "\n",
    "# actual tfidf corpus serialized\n",
    "corpora.MmCorpus.serialize('newtfidf_corpus.mm', tfidf_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf corpus\n",
    "tfidf_corpus = corpora.MmCorpus('newtfidf_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "# query no. gives the list of 0-1 documents\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create index - Tfidf\n",
    "tfidf_index = gensim.similarities.Similarity('tfidf.index',tfidf_corpus,len(dictionary))\n",
    "\n",
    "# save index - tfidf\n",
    "gensim.similarities.Similarity.save(tfidf_index,'TFIDF_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### load tfidf index\n",
    "tfidf_index = gensim.similarities.Similarity.load('TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate similarity for all \n",
    "tfidf_index.num_best= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################         SIMILIRATIES SCORES AND RANKING    ###########################\n",
    "tfidf_results = copy.deepcopy(query_truth)\n",
    "### lists of list (indexing from 1, dummy element)\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    # inside the square bracket determines query representation\n",
    "    \n",
    "    sims = tfidf_index[tfidf[dictionary.doc2bow(query)]]\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in tfidf_results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "        #x.append(common_words(filenames.index(x[0]+'.txt'), tfidf_corpus, query))\n",
    "    \n",
    "    # sort wrt relevance(from truth) and then ranks(from our model)\n",
    "    #tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[1]),x[3]))     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'tfidf.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"TFIDF\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in tfidf_results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluation_Qwise - Precision, Recall and Fscore\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('tfidf.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'tfidf'\n",
    "places = 4\n",
    "wb = load_workbook('tfidf.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:\n",
    "    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

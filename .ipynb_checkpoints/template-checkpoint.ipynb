{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get stopwords as set\n",
    "# get Vocabulary and remove words with length less than 3 \n",
    "# vocab is sorted in descending order \n",
    "\n",
    "en_stop = set(get_stop_words('en'))\n",
    "vocab  = open(\"wordsEn.txt\").read().splitlines()\n",
    "vocab = [i for i in vocab  if len(i)>2]\n",
    "vocab.sort(key=len, reverse=True)\n",
    "\n",
    "vocab_set = set(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask\\ConsumerCourtAdhocdata\\DCDRC\\**\\*.txt','LegalAdhocTask\\ConsumerCourtAdhocdata\\NCDRC\\**\\*.txt','LegalAdhocTask\\ConsumerCourtAdhocdata\\SCDRC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\DelhiHC\\**\\*.txt','LegalAdhocTask\\SC-HCAdhocData\\JharkhandHC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\JodhpurHC\\**\\*.txt' ,'LegalAdhocTask\\SC-HCAdhocData\\KolkataHC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\SupremeCourt\\**\\*.txt']\n",
    "for glob_filenames in x:\n",
    "    for filename in glob2.glob(glob_filenames):\n",
    "        full_filenames.append(filename)\n",
    "        if not os.path.isfile(filename):\n",
    "            print 'wtffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = []\n",
    "x= ['LegalAdhocTask\\ConsumerCourtAdhocdata\\DCDRC\\**\\*.txt','LegalAdhocTask\\ConsumerCourtAdhocdata\\NCDRC\\**\\*.txt','LegalAdhocTask\\ConsumerCourtAdhocdata\\SCDRC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\DelhiHC\\**\\*.txt','LegalAdhocTask\\SC-HCAdhocData\\JharkhandHC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\JodhpurHC\\**\\*.txt' ,'LegalAdhocTask\\SC-HCAdhocData\\KolkataHC\\**\\*.txt', 'LegalAdhocTask\\SC-HCAdhocData\\SupremeCourt\\**\\*.txt']\n",
    "for glob_filenames in x:\n",
    "    for filename in glob2.glob(glob_filenames):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "        if not os.path.isfile(filename):\n",
    "            print 'wtffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for filename in glob2.glob(glob_filenames):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        \n",
    "        # tokenize that string\n",
    "        tokens =  word_tokenize(string)\n",
    "        \n",
    "        # remove stop words tokens and length two tokens\n",
    "        tokens = [lemmatizer.lemmatize(i) for i in tokens if i not in en_stop and len(i)>2]\n",
    "\n",
    "        #seperate words stuck together\n",
    "        for i,token in enumerate(tokens):\n",
    "            if token not in vocab_set:\n",
    "#                 print token\n",
    "                # both prefix and suffix incase one of the word is of len(2)\n",
    "                # prefix match\n",
    "                flag = 0\n",
    "                for j in range(len(token)-2,2,-1):\n",
    "                    if token[:j] in vocab_set:\n",
    "                            tokens.append(token[:j])\n",
    "                            tokens.append(token[j:])\n",
    "#                             print 'prefixx'\n",
    "#                             print token[:j], token[j:]\n",
    "                            del tokens[i]\n",
    "                            flag = 1\n",
    "                            break               \n",
    "                \n",
    "                if flag == 1:\n",
    "#                     print 'prefix milgaya'\n",
    "                    continue \n",
    "                # suffix match\n",
    "                flag = 0\n",
    "                for j in range(1,len(token)-2):\n",
    "                    if token[j:] in vocab_set:\n",
    "                            tokens.append(token[:j])\n",
    "                            tokens.append(token[j:])\n",
    "#                             print 'suffix'\n",
    "#                             print token[:j], token[j:]\n",
    "                            flag =1\n",
    "                            del tokens[i]                            \n",
    "                            break               \n",
    "#                 if flag == 0:\n",
    "#                     print 'couldnt break' \n",
    "        # again remove stop words and length two tokens AND WORDS not in vocab\n",
    "        tokens = [i for i in tokens if i not in en_stop and len(i)>2 and i in vocab_set]                \n",
    "        if len(tokens) == 0:\n",
    "            print \"empty document\"\n",
    "        yield tokens\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\DCDRC\\**\\*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\NCDRC\\**\\*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\SCDRC\\**\\*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\DelhiHC\\**\\*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\JharkhandHC\\**\\*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\JodhpurHC\\**\\*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\KolkataHC\\**\\*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\SupremeCourt\\**\\*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\DCDRC\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\NCDRC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask\\ConsumerCourtAdhocdata\\SCDRC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\DelhiHC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\JharkhandHC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\JodhpurHC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\KolkataHC\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask\\SC-HCAdhocData\\SupremeCourt\\**\\*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create all the court specific dictionaries\n",
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## check out dictionaries\n",
    "# dictionary_DCDRC.save_as_text('newdictionary_DCDRC.txt') \n",
    "# dictionary_NCDRC.save_as_text('newdictionary_NCDRC.txt') \n",
    "# dictionary_SCDRC.save_as_text('newdictionary_SCDRC.txt') \n",
    "# dictionary_DelhiHC.save_as_text('newdictionary_DelhiHC.txt') \n",
    "# dictionary_JharkhandHC.save_as_text('newdictionary_JharkhandHC.txt') \n",
    "# dictionary_JodhpurHC.save_as_text('newdictionary_JodhpurHC.txt') \n",
    "# dictionary_KolkataHC.save_as_text('newdictionary_KolkataHC.txt') \n",
    "# dictionary_SupremeCourt.save_as_text('newdictionary_SupremeCourt.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save actual dictionaries\n",
    "dictionary_DCDRC.save('newdictionary_DCDRC') \n",
    "dictionary_NCDRC.save('newdictionary_NCDRC') \n",
    "dictionary_SCDRC.save('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC.save('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC.save('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC.save('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC.save('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt.save('newdictionary_SupremeCourt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load those dictionaries\n",
    "dictionary_DCDRC = corpora.Dictionary.load('newdictionary_DCDRC') \n",
    "dictionary_NCDRC = corpora.Dictionary.load('newdictionary_NCDRC') \n",
    "dictionary_SCDRC = corpora.Dictionary.load('newdictionary_SCDRC') \n",
    "dictionary_DelhiHC = corpora.Dictionary.load('newdictionary_DelhiHC') \n",
    "dictionary_JharkhandHC = corpora.Dictionary.load('newdictionary_JharkhandHC') \n",
    "dictionary_JodhpurHC = corpora.Dictionary.load('newdictionary_JodhpurHC') \n",
    "dictionary_KolkataHC = corpora.Dictionary.load('newdictionary_KolkataHC') \n",
    "dictionary_SupremeCourt= corpora.Dictionary.load('newdictionary_SupremeCourt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Don't need this now\n",
    "# def sorted_dic(dic):\n",
    "#     s = sorted(dic.dfs.items(), key=lambda (k,v): v)\n",
    "#     for i,j in enumerate(s):\n",
    "#         s[i] = (dic.get(j[0]), j[1])\n",
    "#     return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "dictionary.save('newdictionary') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('newdictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save BOW\n",
    "corpora.MmCorpus.serialize('newcorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('newcorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tfidf model\n",
    "tfidf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save tfidf model\n",
    "tfidf.save('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf model\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('newtfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf wrapper coprus\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actual tfidf corpus serialized\n",
    "corpora.MmCorpus.serialize('newtfidf_corpus.mm', tfidf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################### load tfidf corpus\n",
    "tfidf_corpus = corpora.MmCorpus('newtfidf_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train lda model on tfidf corpus\n",
    "lda_tfidf_model = gensim.models.ldamodel.LdaModel(tfidf_corpus, num_topics=100, id2word = dictionary, passes=5, eval_every=None, alpha ='auto',\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save tfidf_lda model\n",
    "lda_tfidf_model.save('newlda_tfidf.mdl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################### load lda_tfidf model\n",
    "\n",
    "lda_tfidf_model = gensim.models.ldamodel.LdaModel.load('newlda_tfidf.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create index - LDA on Tfidf\n",
    "lda_tfidf_index = gensim.similarities.Similarity('lda_tfidf.index',lda_tfidf_model[tfidf_corpus],len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save index - LDA on Tfidf\n",
    "gensim.similarities.Similarity.save(lda_tfidf_index,'LDA_TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################### load index - LDA + Tfidf\n",
    "lda_tfidf_index = gensim.similarities.Similarity.load('LDA_TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate similarity for all documents\n",
    "lda_tfidf_index.num_best = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from  scipy.stats import rankdata\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_tfidf_results = copy.deepcopy(query_truth)\n",
    "# sims = [1 - hellinger(document, lda_tfidf_model[tfidf[dictionary.doc2bow(query)]]) for document in lda_tfidf_index]\n",
    "\n",
    "# for i,query in enumerate(files_to_tokens('LegalAdhocTask\\q*.txt')):\n",
    "#     sims = [1 - hellinger(document, sparse2full( lda_tfidf_model[tfidf[dictionary.doc2bow(query)]])) for document in lda_tfidf_index]\n",
    "#     print i\n",
    "#     ranks = rankdata(sims, method='ordinal')\n",
    "#     ranks= len(ranks)+1 - ranks \n",
    "    \n",
    "#     for x in lda_tfidf_results[str(i+1)]:\n",
    "#         x.append(sims[filenames.index(x[1]+'.txt')])\n",
    "#         x.append(ranks[filenames.index(x[1]+'.txt')])\n",
    "#     lda_tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[2]),x[4]) )     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda_tfidf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################### TFIDF INDEX ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create index - Tfidf\n",
    "\n",
    "tfidf_index = gensim.similarities.Similarity('tfidf.index',tfidf_corpus,len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save index - tfidf\n",
    "gensim.similarities.Similarity.save(tfidf_index,'TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################### load tfidf index\n",
    "tfidf_index = gensim.similarities.Similarity.load('TFIDF_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate similarity for all \n",
    "tfidf_index.num_best= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_results = copy.deepcopy(query_truth)\n",
    "\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask\\q*.txt')):\n",
    "    queries.append(query)\n",
    "    sims = tfidf_index[dictionary.doc2bow(query)]\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in tfidf_results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "        #x.append(common_words(filenames.index(x[0]+'.txt'), tfidf_corpus, query))\n",
    "    \n",
    "    # sort wrt relevance(from truth) and then ranks(from our model)\n",
    "    #tfidf_results[str(i+1)].sort(key=lambda x: (-int(x[1]),x[3]))     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BoW query\n",
    "BQ = tfidf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf query\n",
    "TQ = tfidf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to excel\n",
    "wb = Workbook()\n",
    "dest_filename = 'result.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"BoW Query TFIDF Index\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in wow.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "ws3 = wb.create_sheet(title=\"TFIDF Query TFIDF Index\")\n",
    "ws3.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in wow2.iteritems():\n",
    "    for i in value:\n",
    "        ws3.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# given a corpus and document index and a query return intersecting words(scores from corpus) \n",
    "def common_words(index, query):\n",
    "    # quey words\n",
    "    q = [i[0] for i in dictionary.doc2bow(query)]   \n",
    "    q = set(q)\n",
    "    \n",
    "    return sorted([(dictionary.get(i[0]),i[1],j[1]) for i,j in zip(corpus[index],tfidf_corpus[index]) if i[0] in q], key= lambda x: x[1], reverse= True )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses ranklist\n",
    "# return top N documents and the respctive common words for every query given corpus\n",
    "def top_N(n, corp) :\n",
    "    top_results = [0]\n",
    "    for i in range(1,11):\n",
    "        x = sorted(range(len(rank_list[i])), key=lambda j: rank_list[i][j])[:n]\n",
    "        y = []\n",
    "        for j in x:\n",
    "            y.append([filenames[j],common_words(j, queries[i])])\n",
    "            \n",
    "        top_results.append(y)\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# queries as bag of words\n",
    "# index starts from 1 for queries\n",
    "q_words = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask\\q*.txt')):\n",
    "    q_words.append(sorted([(dictionary.get(j[0]), j[1], k[1]) for j,k in zip(dictionary.doc2bow(query),tfidf[dictionary.doc2bow(query)])], reverse= True, key= lambda x:x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# index starts from 1 for queries\n",
    "top_results = top_N(10,corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ground truth ranks BoW query\n",
    "\n",
    "sorted(BQ['1'], key = lambda x: (-int(x[1]),x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ConsumerCourt_DCDRC_42649', '1', 0.45853043, 5],\n",
       " ['ConsumerCourt_DCDRC_96667', '1', 0.41926709, 9],\n",
       " ['ConsumerCourt_DCDRC_80057', '1', 0.41790479, 11],\n",
       " ['ConsumerCourt_DCDRC_226072', '1', 0.41369277, 14],\n",
       " ['ConsumerCourt_DCDRC_130570', '1', 0.35782218, 37],\n",
       " ['ConsumerCourt_DCDRC_55517', '1', 0.35226679, 41],\n",
       " ['ConsumerCourt_DCDRC_98733', '1', 0.34599787, 47],\n",
       " ['ConsumerCourt_DCDRC_41588', '1', 0.33278599, 54],\n",
       " ['ConsumerCourt_DCDRC_140039', '1', 0.32230711, 70],\n",
       " ['ConsumerCourt_DCDRC_55899', '1', 0.30997869, 85],\n",
       " ['ConsumerCourt_DCDRC_217344', '1', 0.30339226, 94],\n",
       " ['ConsumerCourt_DCDRC_133592', '1', 0.27560019, 157],\n",
       " ['ConsumerCourt_DCDRC_55678', '1', 0.25786078, 219],\n",
       " ['ConsumerCourt_DCDRC_100385', '1', 0.24860436, 274],\n",
       " ['ConsumerCourt_DCDRC_46368', '1', 0.23966464, 331],\n",
       " ['ConsumerCourt_DCDRC_222844', '1', 0.2306449, 398],\n",
       " ['ConsumerCourt_DCDRC_217471', '1', 0.22937046, 406],\n",
       " ['ConsumerCourt_DCDRC_42618', '1', 0.22404736, 449],\n",
       " ['ConsumerCourt_DCDRC_46519', '1', 0.21995942, 484],\n",
       " ['ConsumerCourt_DCDRC_114291', '1', 0.19972853, 669],\n",
       " ['ConsumerCourt_DCDRC_39186', '1', 0.19768378, 691],\n",
       " ['ConsumerCourt_DCDRC_74868', '1', 0.19762827, 692],\n",
       " ['ConsumerCourt_DCDRC_44200', '1', 0.19407943, 720],\n",
       " ['ConsumerCourt_DCDRC_42118', '1', 0.18760437, 787],\n",
       " ['ConsumerCourt_DCDRC_55612', '1', 0.17693545, 933],\n",
       " ['ConsumerCourt_DCDRC_131741', '1', 0.16732627, 1063],\n",
       " ['ConsumerCourt_DCDRC_131818', '1', 0.14837956, 1328],\n",
       " ['ConsumerCourt_DCDRC_46543', '1', 0.14554347, 1364],\n",
       " ['ConsumerCourt_DCDRC_114382', '1', 0.14082284, 1461],\n",
       " ['ConsumerCourt_DCDRC_53138', '1', 0.13873544, 1499],\n",
       " ['ConsumerCourt_DCDRC_107608', '1', 0.1246242, 1779],\n",
       " ['ConsumerCourt_DCDRC_131717', '1', 0.12323821, 1815],\n",
       " ['ConsumerCourt_DCDRC_135474', '1', 0.11499894, 2037],\n",
       " ['ConsumerCourt_DCDRC_41317', '1', 0.11379609, 2076],\n",
       " ['ConsumerCourt_DCDRC_187214', '1', 0.10187973, 2490],\n",
       " ['ConsumerCourt_DCDRC_131146', '1', 0.081247769, 3593],\n",
       " ['ConsumerCourt_DCDRC_207784', '1', 0.056903392, 6511],\n",
       " ['ConsumerCourt_DCDRC_118185', '1', 0.046876241, 9035],\n",
       " ['ConsumerCourt_DCDRC_134386', '0', 0.54824233, 1],\n",
       " ['ConsumerCourt_DCDRC_145708', '0', 0.45274115, 6],\n",
       " ['ConsumerCourt_DCDRC_38498', '0', 0.38594341, 21],\n",
       " ['ConsumerCourt_DCDRC_57484', '0', 0.36580664, 31],\n",
       " ['ConsumerCourt_DCDRC_139205', '0', 0.34678739, 45],\n",
       " ['ConsumerCourt_DCDRC_83596', '0', 0.31225777, 79],\n",
       " ['ConsumerCourt_DCDRC_224833', '0', 0.29982787, 99],\n",
       " ['ConsumerCourt_DCDRC_139024', '0', 0.27845153, 151],\n",
       " ['ConsumerCourt_DCDRC_222797', '0', 0.25399891, 244],\n",
       " ['ConsumerCourt_DCDRC_224228', '0', 0.2487818, 273],\n",
       " ['ConsumerCourt_SCDRC_52678', '0', 0.22192861, 469],\n",
       " ['ConsumerCourt_DCDRC_83779', '0', 0.21915299, 490],\n",
       " ['ConsumerCourt_DCDRC_106530', '0', 0.19510467, 711],\n",
       " ['ConsumerCourt_DCDRC_130318', '0', 0.18421404, 840],\n",
       " ['DelhiHC_2011_4267', '0', 0.15694781, 1193],\n",
       " ['ConsumerCourt_DCDRC_131972', '0', 0.13214688, 1619],\n",
       " ['ConsumerCourt_DCDRC_132932', '0', 0.10617077, 2326],\n",
       " ['ConsumerCourt_DCDRC_131950', '0', 0.073916182, 4151],\n",
       " ['KolkataHCOriginalSite_2012_36', '0', 0.037989065, 13044]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth ranks TFIDF query\n",
    "sorted(TQ['1'], key = lambda x: (-int(x[1]),x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ConsumerCourt_DCDRC_134386.txt',\n",
       "  [(u'mobile', 16.0, 0.388335475875),\n",
       "   (u'sung', 15.0, 0.580767919754),\n",
       "   (u'repair', 15.0, 0.290545355934),\n",
       "   (u'phone', 15.0, 0.344089370735),\n",
       "   (u'sam', 12.0, 0.246381114946),\n",
       "   (u'day', 6.0, 0.0312600426101),\n",
       "   (u'wanted', 5.0, 0.103886649847),\n",
       "   (u'also', 3.0, 0.00780533288283),\n",
       "   (u'get', 2.0, 0.0203270424424),\n",
       "   (u'found', 2.0, 0.0172418724896)]],\n",
       " ['ConsumerCourt_DCDRC_35637.txt',\n",
       "  [(u'mobile', 26.0, 0.565296659323),\n",
       "   (u'phone', 21.0, 0.431534259053),\n",
       "   (u'sung', 13.0, 0.450890099274),\n",
       "   (u'sam', 13.0, 0.239103266552),\n",
       "   (u'also', 6.0, 0.0139841930977),\n",
       "   (u'repair', 3.0, 0.0520546967312),\n",
       "   (u'working', 2.0, 0.027353841253),\n",
       "   (u'defect', 2.0, 0.0313326888255),\n",
       "   (u'get', 1.0, 0.00910458563674),\n",
       "   (u'day', 1.0, 0.00466717732889)]],\n",
       " ['ConsumerCourt_DCDRC_42649.txt',\n",
       "  [(u'mobile', 39.0, 0.568431611736),\n",
       "   (u'phone', 35.0, 0.482141564958),\n",
       "   (u'defect', 14.0, 0.147030098984),\n",
       "   (u'sung', 14.0, 0.325511192758),\n",
       "   (u'sam', 14.0, 0.172615876048),\n",
       "   (u'manufacturing', 9.0, 0.116573082224),\n",
       "   (u'day', 9.0, 0.0281583599078),\n",
       "   (u'also', 6.0, 0.00937449660604),\n",
       "   (u'month', 6.0, 0.0251704743488),\n",
       "   (u'working', 3.0, 0.0275055368083),\n",
       "   (u'ready', 2.0, 0.0217355247692),\n",
       "   (u'repair', 2.0, 0.0232637223292),\n",
       "   (u'get', 1.0, 0.00610338448238),\n",
       "   (u'accept', 1.0, 0.009382874658),\n",
       "   (u'wanted', 1.0, 0.012477175042)]],\n",
       " ['ConsumerCourt_DCDRC_135542.txt',\n",
       "  [(u'mobile', 20.0, 0.537274282491),\n",
       "   (u'phone', 15.0, 0.380846729201),\n",
       "   (u'sung', 11.0, 0.47139287953),\n",
       "   (u'sam', 11.0, 0.249975720262),\n",
       "   (u'defect', 5.0, 0.0967833522384),\n",
       "   (u'day', 2.0, 0.011533132567),\n",
       "   (u'month', 1.0, 0.00773201329168)]],\n",
       " ['ConsumerCourt_DCDRC_99425.txt',\n",
       "  [(u'phone', 25.0, 0.609733764679),\n",
       "   (u'mobile', 13.0, 0.335467672075),\n",
       "   (u'sung', 8.0, 0.329322637881),\n",
       "   (u'sam', 8.0, 0.174637053672),\n",
       "   (u'defect', 6.0, 0.11156376752),\n",
       "   (u'month', 2.0, 0.0148546988951),\n",
       "   (u'galaxy', 2.0, 0.101381501337),\n",
       "   (u'now', 1.0, 0.0101071035434),\n",
       "   (u'repair', 1.0, 0.0205941047589),\n",
       "   (u'day', 1.0, 0.00553934677611)]],\n",
       " ['ConsumerCourt_DCDRC_80057.txt',\n",
       "  [(u'phone', 15.0, 0.540283834467),\n",
       "   (u'mobile', 14.0, 0.533538589343),\n",
       "   (u'defect', 8.0, 0.219680944173),\n",
       "   (u'sung', 4.0, 0.243176759922),\n",
       "   (u'sam', 4.0, 0.128954611646),\n",
       "   (u'also', 3.0, 0.0122558135702),\n",
       "   (u'found', 3.0, 0.0406093842517),\n",
       "   (u'get', 2.0, 0.0319172092144),\n",
       "   (u'back', 2.0, 0.0373006036435),\n",
       "   (u'day', 2.0, 0.0163613459404),\n",
       "   (u'month', 1.0, 0.0109689317751),\n",
       "   (u'change', 1.0, 0.0285660180403),\n",
       "   (u'ready', 1.0, 0.0284160900172)]],\n",
       " ['ConsumerCourt_DCDRC_99147.txt',\n",
       "  [(u'phone', 25.0, 0.64604812368),\n",
       "   (u'mobile', 10.0, 0.273421050117),\n",
       "   (u'defect', 6.0, 0.118208252277),\n",
       "   (u'sung', 6.0, 0.261702260003),\n",
       "   (u'sam', 6.0, 0.138778530137),\n",
       "   (u'month', 4.0, 0.0314788220858),\n",
       "   (u'day', 3.0, 0.0176077730893),\n",
       "   (u'working', 2.0, 0.0343991093519),\n",
       "   (u'accept', 1.0, 0.0176016850886),\n",
       "   (u'ready', 1.0, 0.0203872414462)]],\n",
       " ['ConsumerCourt_DCDRC_222695.txt',\n",
       "  [(u'phone', 30.0, 0.565493852883),\n",
       "   (u'mobile', 13.0, 0.259272868188),\n",
       "   (u'defect', 8.0, 0.114965704691),\n",
       "   (u'sung', 8.0, 0.254523556188),\n",
       "   (u'sam', 8.0, 0.134971723258),\n",
       "   (u'month', 6.0, 0.0344422492168),\n",
       "   (u'day', 6.0, 0.0256871665334),\n",
       "   (u'repair', 5.0, 0.0795828190476),\n",
       "   (u'also', 4.0, 0.00855178555408),\n",
       "   (u'get', 4.0, 0.0334064883316),\n",
       "   (u'wanted', 4.0, 0.0682930271978),\n",
       "   (u'working', 3.0, 0.0376374533299),\n",
       "   (u'back', 3.0, 0.0292808067588),\n",
       "   (u'took', 1.0, 0.00947784876812),\n",
       "   (u'found', 1.0, 0.0070840410474),\n",
       "   (u'pro', 1.0, 0.020648753563),\n",
       "   (u'manufacturing', 1.0, 0.0177237605825)]],\n",
       " ['ConsumerCourt_DCDRC_145708.txt',\n",
       "  [(u'sung', 15.0, 0.497298942938),\n",
       "   (u'sam', 15.0, 0.263713490052),\n",
       "   (u'mobile', 15.0, 0.311740523445),\n",
       "   (u'phone', 10.0, 0.196424164768),\n",
       "   (u'defect', 6.0, 0.0898499522372),\n",
       "   (u'day', 5.0, 0.0223060790399),\n",
       "   (u'working', 4.0, 0.05229344437),\n",
       "   (u'get', 3.0, 0.0261084067556),\n",
       "   (u'also', 2.0, 0.00445569123211),\n",
       "   (u'month', 2.0, 0.0119635076503),\n",
       "   (u'ready', 2.0, 0.0309926360453),\n",
       "   (u'bought', 1.0, 0.0271207344846),\n",
       "   (u'instrument', 1.0, 0.0216204178885),\n",
       "   (u'repair', 1.0, 0.0165858447602)]],\n",
       " ['ConsumerCourt_DCDRC_76545.txt',\n",
       "  [(u'mobile', 10.0, 0.385437300108),\n",
       "   (u'phone', 8.0, 0.291431600515),\n",
       "   (u'sung', 7.0, 0.430403759691),\n",
       "   (u'sam', 6.0, 0.195633883881),\n",
       "   (u'back', 4.0, 0.0754504433124),\n",
       "   (u'also', 3.0, 0.0123953297896),\n",
       "   (u'repair', 3.0, 0.0922806383716),\n",
       "   (u'day', 2.0, 0.016547598213),\n",
       "   (u'change', 1.0, 0.0288912043543),\n",
       "   (u'defect', 1.0, 0.0277727151273),\n",
       "   (u'ready', 1.0, 0.028739569599)]]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf[dictionary.doc2bow([])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'repair', 19.0, 0.110276973114),\n",
       " (u'defect', 17.0, 0.0890859389048),\n",
       " (u'replacement', 13.0, 0.0918378931103),\n",
       " (u'manufacturing', 11.0, 0.0710935959178),\n",
       " (u'accept', 7.0, 0.0327729836281),\n",
       " (u'found', 7.0, 0.0180826064506),\n",
       " (u'now', 6.0, 0.0170909531548),\n",
       " (u'day', 5.0, 0.00780579024215),\n",
       " (u'working', 4.0, 0.0182995701334),\n",
       " (u'also', 4.0, 0.00311844955624),\n",
       " (u'month', 2.0, 0.00418651037287),\n",
       " (u'bought', 2.0, 0.018981261944),\n",
       " (u'back', 2.0, 0.00711825760564),\n",
       " (u'took', 2.0, 0.00691228588418),\n",
       " (u'sam', 2.0, 0.0123045213142),\n",
       " (u'get', 1.0, 0.0030454589879),\n",
       " (u'phone', 1.0, 0.00687366805222)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words(filenames.index( 'ConsumerCourt_DCDRC_118185' + '.txt'), queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# p = pyLDAvis.gensim.prepare(lda_tfidf_model, corpus, dictionary)\n",
    "# pyLDAvis.save_html(p, 'BOWlda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# p2 = pyLDAvis.gensim.prepare(lda_tfidf_model, tfidf_corpus, dictionary)\n",
    "# pyLDAvis.save_html(p2, 'TFIDFlda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "np.random.seed(sum(map(ord, \"distributions\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SEE THE DISTRIBUTION OF SIMILARITY SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = sim_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print type(a)\n",
    "\n",
    "def y(x):\n",
    "    return x==0\n",
    "sum(map(y,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "sns.distplot(a[-100:\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

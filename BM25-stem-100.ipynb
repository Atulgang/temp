{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "en_stop = set(get_stop_words('en'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in word_tokenize(string):\n",
    "            if word not in en_stop:\n",
    "                tokens.append(ps.stem(word))\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 18:24:57,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 18:32:02,175 : INFO : adding document #10000 to Dictionary(172845 unique tokens: [u'gudadannavar', u'alsocont', u'beencredit', u'delhifano', u'alsocons']...)\n",
      "2018-03-18 18:38:34,495 : INFO : adding document #20000 to Dictionary(312646 unique tokens: [u'gudadannavar', u'falselitig', u'againstcomplain', u'ofday', u'nonehru']...)\n",
      "2018-03-18 18:44:31,602 : INFO : adding document #30000 to Dictionary(472300 unique tokens: [u'gudadannavar', u'mrmvenkatesancounsel', u'rkgejggll', u'boncer', u'metersand']...)\n",
      "2018-03-18 18:49:40,802 : INFO : adding document #40000 to Dictionary(654903 unique tokens: [u'gudadannavar', u'woodb', u'mrmvenkatesancounsel', u'atsugham', u'rkgejggll']...)\n",
      "2018-03-18 18:53:45,344 : INFO : adding document #50000 to Dictionary(771804 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'khallimahapatro']...)\n",
      "2018-03-18 18:57:38,805 : INFO : adding document #60000 to Dictionary(887421 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'pricepremium']...)\n",
      "2018-03-18 19:01:31,723 : INFO : adding document #70000 to Dictionary(971423 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas', u'pricepremium']...)\n",
      "2018-03-18 19:05:39,014 : INFO : adding document #80000 to Dictionary(1068170 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n",
      "2018-03-18 19:10:30,813 : INFO : adding document #90000 to Dictionary(1159711 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n",
      "2018-03-18 19:16:09,071 : INFO : adding document #100000 to Dictionary(1278422 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'presidenticerakpandamembercdcas']...)\n",
      "2018-03-18 19:21:33,284 : INFO : adding document #110000 to Dictionary(1403899 unique tokens: [u'persontransf', u'asstregistrarcoop', u'boncer', u'sbhagsingh', u'ltdvill']...)\n",
      "2018-03-18 19:25:08,546 : INFO : adding document #120000 to Dictionary(1463453 unique tokens: [u'persontransf', u'asstregistrarcoop', u'boncer', u'sbhagsingh', u'ltdvill']...)\n",
      "2018-03-18 19:26:41,698 : INFO : built Dictionary(1484071 unique tokens: [u'persontransf', u'asstregistrarcoop', u'boncer', u'sbhagsingh', u'ltdvill']...) from 122895 documents (total 106900649 corpus positions)\n",
      "2018-03-18 19:26:41,835 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 19:28:37,231 : INFO : built Dictionary(188880 unique tokens: [u'beencredit', u'vmunicip', u'beingcertif', u'fromdrrajendra', u'gavar']...) from 3484 documents (total 3375813 corpus positions)\n",
      "2018-03-18 19:28:38,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 19:31:47,290 : INFO : adding document #10000 to Dictionary(98808 unique tokens: [u'cfpvfzdzm', u'sonji', u'nehatero', u'gavat', u'pricepremium']...)\n",
      "2018-03-18 19:34:49,627 : INFO : adding document #20000 to Dictionary(252613 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'woodi', u'gavar', u'gavas']...)\n",
      "2018-03-18 19:37:44,501 : INFO : adding document #30000 to Dictionary(412050 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'woodi', u'noseventh', u'pricepremium']...)\n",
      "2018-03-18 19:41:41,821 : INFO : adding document #40000 to Dictionary(577009 unique tokens: [u'iiifollowingmorn', u'mrchandrabali', u'cfpvfzdzm', u'woodi', u'theoryha']...)\n",
      "2018-03-18 19:45:38,204 : INFO : adding document #50000 to Dictionary(707520 unique tokens: [u'mrchandrabali', u'atjagarajup', u'pricepremium', u'ltdvill', u'legalexist']...)\n",
      "2018-03-18 19:47:39,735 : INFO : built Dictionary(773387 unique tokens: [u'gavecredit', u'mrchandrabali', u'atjagarajup', u'pricepremium', u'ltdvill']...) from 56745 documents (total 34673887 corpus positions)\n",
      "2018-03-18 19:47:40,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 19:54:46,660 : INFO : adding document #10000 to Dictionary(119474 unique tokens: [u'circuitri', u'gangoor', u'gai', u'sutain', u'gavat']...)\n",
      "2018-03-18 20:02:03,805 : INFO : adding document #20000 to Dictionary(168822 unique tokens: [u'circuitri', u'gangoor', u'gai', u'sutain', u'jaffrulhaq']...)\n",
      "2018-03-18 20:09:04,743 : INFO : adding document #30000 to Dictionary(249779 unique tokens: [u'insolventwheth', u'beobey', u'gangoor', u'ofday', u'woodi']...)\n",
      "2018-03-18 20:11:52,936 : INFO : built Dictionary(278112 unique tokens: [u'insolventwheth', u'beobey', u'gangoor', u'ofday', u'woodi']...) from 34008 documents (total 45311192 corpus positions)\n",
      "2018-03-18 20:11:54,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 20:13:53,498 : INFO : adding document #10000 to Dictionary(55934 unique tokens: [u'averma', u'ikkzir', u'theappointmentwastobemadeoncontractualbasisforaperiodoftwoyearsth', u'vani', u'gavan']...)\n",
      "2018-03-18 20:14:35,931 : INFO : adding document #20000 to Dictionary(77609 unique tokens: [u'inconnectionwithstnoof', u'asmuddin', u'averma', u'ikkzir', u'theappointmentwastobemadeoncontractualbasisforaperiodoftwoyearsth']...)\n",
      "2018-03-18 20:15:34,825 : INFO : adding document #30000 to Dictionary(114000 unique tokens: [u'ramdhir', u'mrmmprasadadvocatefortherespond', u'indardeo', u'showa', u'thereisadisputebetweenthepartiesthatthepetitionerisnota']...)\n",
      "2018-03-18 20:16:49,839 : INFO : adding document #40000 to Dictionary(147211 unique tokens: [u'thepetitionerscaseisstillpendingitshallbedisposedofwithoutfurtherdelayconsid', u'ramdhir', u'counselfortherespondentssubmittedthatadetailcounteraffidavithasbeenfiledandnowtherespondentshavepassedanorderon', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'aloksinghjmanishtextdoc']...)\n",
      "2018-03-18 20:17:36,106 : INFO : adding document #50000 to Dictionary(160589 unique tokens: [u'thepetitionerscaseisstillpendingitshallbedisposedofwithoutfurtherdelayconsid', u'ramdhir', u'counselfortherespondentssubmittedthatadetailcounteraffidavithasbeenfiledandnowtherespondentshavepassedanorderon', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'aloksinghjmanishtextdoc']...)\n",
      "2018-03-18 20:19:06,127 : INFO : adding document #60000 to Dictionary(201674 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'tajudeen', u'indardeo', u'thereisadisputebetweenthepartiesthatthepetitionerisnota']...)\n",
      "2018-03-18 20:20:16,255 : INFO : adding document #70000 to Dictionary(238297 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'councilwithinfourweekstheremainingamountwillbereturnedthedraftshal', u'tajudeen', u'indardeo']...)\n",
      "2018-03-18 20:20:23,138 : INFO : built Dictionary(240467 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'councilwithinfourweekstheremainingamountwillbereturnedthedraftshal', u'tajudeen', u'indardeo']...) from 71312 documents (total 12668189 corpus positions)\n",
      "2018-03-18 20:20:24,180 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 20:23:02,413 : INFO : adding document #10000 to Dictionary(162961 unique tokens: [u'merelysupervisori', u'appealedbefor', u'icici', u'theoryha', u'hypertechnicalground']...)\n",
      "2018-03-18 20:25:34,655 : INFO : adding document #20000 to Dictionary(248167 unique tokens: [u'selfexplanatorywhich', u'ofday', u'appealedbefor', u'arithmeticallyincorrect', u'theoryha']...)\n",
      "2018-03-18 20:28:48,922 : INFO : adding document #30000 to Dictionary(335033 unique tokens: [u'purposenarendra', u'isintroduc', u'selfexplanatorywhich', u'ofday', u'appealedbefor']...)\n",
      "2018-03-18 20:29:38,445 : INFO : built Dictionary(351612 unique tokens: [u'purposenarendra', u'selfexplanatorywhich', u'icici', u'theoryha', u'brokenetowel']...) from 32240 documents (total 15411571 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 20:29:38,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 20:30:58,970 : INFO : built Dictionary(29543 unique tokens: [u'noezi', u'tapodip', u'publicityori', u'howrahwest', u'jairam']...) from 1210 documents (total 2353135 corpus positions)\n",
      "2018-03-18 20:30:59,760 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-18 20:45:36,503 : INFO : adding document #10000 to Dictionary(183368 unique tokens: [u'insolventwheth', u'vmunicip', u'jaluram', u'balipura', u'pawnor']...)\n",
      "2018-03-18 20:54:08,960 : INFO : adding document #20000 to Dictionary(254545 unique tokens: [u'insolventwheth', u'samul', u'vmunicip', u'jaluram', u'handelsentransport']...)\n",
      "2018-03-18 21:02:51,409 : INFO : adding document #30000 to Dictionary(309687 unique tokens: [u'insolventwheth', u'samul', u'vmunicip', u'jaluram', u'handelsentransport']...)\n",
      "2018-03-18 21:02:59,990 : INFO : built Dictionary(310515 unique tokens: [u'insolventwheth', u'samul', u'vmunicip', u'jaluram', u'handelsentransport']...) from 30091 documents (total 55752702 corpus positions)\n",
      "2018-03-18 21:03:02,891 : INFO : discarding 1258929 tokens: [(u'consum', 116763), (u'follow', 65615), (u'thiruvananthapuramtini', 1), (u'th', 63553), (u'beenakumariamemb', 2), (u'direct', 92987), (u'cost', 106720), (u'defici', 93441), (u'narendranthi', 1), (u'forum', 116556)]...\n",
      "2018-03-18 21:03:02,892 : INFO : keeping 225142 tokens which were in no less than 5 and no more than 61447 (=50.0%) documents\n",
      "2018-03-18 21:03:03,780 : INFO : resulting dictionary: Dictionary(225142 unique tokens: [u'againstcomplain', u'gai', u'nonehru', u'metersand', u'largeord']...)\n",
      "2018-03-18 21:03:04,173 : INFO : discarding 165559 tokens: [(u'consum', 3424), (u'tl', 3), (u'th', 2030), (u'incltlctlat', 2), (u'compens', 2004), (u'twoorder', 3), (u'cost', 2928), (u'pass', 2275), (u'deducteda', 2), (u'sum', 1824)]...\n",
      "2018-03-18 21:03:04,174 : INFO : keeping 23321 tokens which were in no less than 5 and no more than 1742 (=50.0%) documents\n",
      "2018-03-18 21:03:04,271 : INFO : resulting dictionary: Dictionary(23321 unique tokens: [u'gavar', u'anyev', u'evenbefor', u'theresidu', u'commissionchandigarh']...)\n",
      "2018-03-18 21:03:05,655 : INFO : discarding 665817 tokens: [(u'consum', 49039), (u'member', 49431), (u'complaint', 43968), (u'kainthla', 2), (u'respond', 45328), (u'direct', 34882), (u'cost', 38547), (u'commiss', 45575), (u'per', 39912), (u'state', 47416)]...\n",
      "2018-03-18 21:03:05,656 : INFO : keeping 107570 tokens which were in no less than 5 and no more than 28372 (=50.0%) documents\n",
      "2018-03-18 21:03:06,070 : INFO : resulting dictionary: Dictionary(107570 unique tokens: [u'blgn', u'nqlrh', u'ksrinivasa', u'percopi', u'icici']...)\n",
      "2018-03-18 21:03:06,593 : INFO : discarding 229812 tokens: [(u'respond', 31486), (u'coram', 29494), (u'textdoc', 32983), (u'high', 33958), (u'see', 24931), (u'paper', 22574), (u'digest', 20711), (u'page', 31988), (u'versu', 30131), (u'court', 33986)]...\n",
      "2018-03-18 21:03:06,594 : INFO : keeping 48300 tokens which were in no less than 5 and no more than 17004 (=50.0%) documents\n",
      "2018-03-18 21:03:06,749 : INFO : resulting dictionary: Dictionary(48300 unique tokens: [u'zadoo', u'undertakingiv', u'kusumlata', u'gavat', u'anyev']...)\n",
      "2018-03-18 21:03:07,161 : INFO : discarding 213558 tokens: [(u'petition', 57419), (u'state', 51058), (u'jharkhand', 56620), (u'justic', 55009), (u'counsel', 50162), (u'mr', 55665), (u'high', 56201), (u'court', 58748), (u'honbl', 55847), (u'docdocnojharkhandhcdocnotextin', 43475)]...\n",
      "2018-03-18 21:03:07,162 : INFO : keeping 26909 tokens which were in no less than 5 and no more than 35656 (=50.0%) documents\n",
      "2018-03-18 21:03:07,273 : INFO : resulting dictionary: Dictionary(26909 unique tokens: [u'aloksinghjmanishtextdoc', u'gag', u'mramitkumartiwariadvocateforthest', u'heardlearnedcounsel', u'jairam']...)\n",
      "2018-03-18 21:03:07,891 : INFO : discarding 264506 tokens: [(u'petition', 21959), (u'judicatur', 16512), (u'rajasthan', 21074), (u'palidecid', 1), (u'respond', 17290), (u'state', 20257), (u'clarificationsought', 1), (u'mr', 25616), (u'orderexpeditiouslyth', 1), (u'civil', 18993)]...\n",
      "2018-03-18 21:03:07,892 : INFO : keeping 87106 tokens which were in no less than 5 and no more than 16120 (=50.0%) documents\n",
      "2018-03-18 21:03:08,102 : INFO : resulting dictionary: Dictionary(87106 unique tokens: [u'casesangeet', u'becausea', u'adverselyth', u'sbawejatextdoc', u'acontradict']...)\n",
      "2018-03-18 21:03:08,165 : INFO : discarding 22160 tokens: [(u'consider', 768), (u'scc', 607), (u'follow', 1005), (u'th', 893), (u'direct', 1004), (u'therefor', 950), (u'pass', 1039), (u'even', 909), (u'appear', 1092), (u'section', 922)]...\n",
      "2018-03-18 21:03:08,165 : INFO : keeping 7383 tokens which were in no less than 5 and no more than 605 (=50.0%) documents\n",
      "2018-03-18 21:03:08,181 : INFO : resulting dictionary: Dictionary(7383 unique tokens: [u'indranil', u'mohind', u'yellow', u'interchang', u'four']...)\n",
      "2018-03-18 21:03:08,685 : INFO : discarding 264728 tokens: [(u'consider', 18360), (u'jjin', 3), (u'follow', 22418), (u'decid', 15354), (u'decis', 19557), (u'aris', 17053), (u'natur', 15469), (u'cost', 20206), (u'howev', 20846), (u'even', 20980)]...\n",
      "2018-03-18 21:03:08,686 : INFO : keeping 45787 tokens which were in no less than 5 and no more than 15045 (=50.0%) documents\n",
      "2018-03-18 21:03:08,834 : INFO : resulting dictionary: Dictionary(45787 unique tokens: [u'woodi', u'gavat', u'pricepremium', u'pawnor', u'fortythird']...)\n",
      "2018-03-18 21:03:10,303 : INFO : discarding 295808 tokens: [(u'againstcomplain', 81), (u'nonehru', 5), (u'metersand', 9), (u'largeord', 17), (u'gavat', 53), (u'asgiia', 7), (u'pricepremium', 67), (u'pawnor', 17), (u'circuitri', 15), (u'exapostalacknowledg', 7)]...\n",
      "2018-03-18 21:03:10,304 : INFO : keeping 40069 tokens which were in no less than 100 and no more than 175992 (=50.0%) documents\n",
      "2018-03-18 21:03:10,469 : INFO : resulting dictionary: Dictionary(40069 unique tokens: [u'aloksinghjmanishtextdoc', u'anyev', u'evenbefor', u'insuranceschem', u'commissionchandigarh']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 35min 7s, sys: 26.9 s, total: 2h 35min 33s\n",
      "Wall time: 2h 38min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "dictionary.compactify()\n",
    "dictionary.filter_extremes(no_below=100)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 21:17:39,414 : INFO : saving Dictionary object under stem100_dictionary, separately None\n",
      "2018-03-18 21:17:39,449 : INFO : saved stem100_dictionary\n"
     ]
    }
   ],
   "source": [
    "# save the dictionary\n",
    "dictionary.save('stem100_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 21:17:39,768 : INFO : loading Dictionary object from stem100_dictionary\n",
      "2018-03-18 21:17:39,783 : INFO : loaded stem100_dictionary\n"
     ]
    }
   ],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('stem100_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40069"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "######################################## save BOW\n",
    "corpora.MmCorpus.serialize('stem100.mm', corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 18:19:58,760 : INFO : loaded corpus index from stem100.mm.index\n",
      "2018-03-18 18:19:58,761 : INFO : initializing corpus reader from stem100.mm\n",
      "2018-03-18 18:19:58,785 : INFO : accepted corpus with 351985 documents, 40069 features, 92572684 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('stem100.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of document lengths in the corpus\n",
    "doc_len = []\n",
    "for doc in corpus:\n",
    "    temp = 0\n",
    "    for word, freq in doc:\n",
    "        temp += freq \n",
    "    doc_len.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "#     The variable k1 is a positive tuning parameter that calibrates the\n",
    "#     document term frequency scaling. A k1 value of 0 corresponds to a binary\n",
    "#     model (no term frequency), and a large value corresponds to using raw term\n",
    "#     frequency. b is another tuning parameter (0 ≤ b ≤ 1) which determines\n",
    "#     the scaling by document length: b = 1 corresponds to fully scaling the term\n",
    "#     weight by the document length, while b = 0 corresponds to no length normalization\n",
    "\n",
    "\n",
    "# BM25 parameters.\n",
    "PARAM_K1 =1.2\n",
    "PARAM_B = 0.75\n",
    "\n",
    "# incase the idf is zero\n",
    "EPSILON = 0.01\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus_size = dictionary.num_docs\n",
    "        s = 0\n",
    "        for i in corpus:\n",
    "            for x in i:\n",
    "                s = s + x[1]\n",
    "        s = float(s)    \n",
    "        self.avgdl = s / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.df = dictionary.dfs\n",
    "        self.idf = {}\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, query, index, average_idf):\n",
    "        score = 0\n",
    "        index_doc = dict(self.corpus[index])\n",
    "        \n",
    "        for word,freq in query:\n",
    "            if word not in index_doc:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "                      / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / self.avgdl))))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(query, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm25 = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to pass it to the function that calculates the score\n",
    "average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = copy.deepcopy(query_truth)\n",
    "\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    sims = bm25.get_scores(dictionary.doc2bow(query), average_idf)\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "    print i\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'BM25.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"K1 = 1.2 B = 0.75 EPS = 0.01 |\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise (F score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Top_x(x, R):\n",
    "    R = list(R)\n",
    "    x_names = []\n",
    "    for i in range(x):\n",
    "        x_names.append(filenames[R.index(i + 1)].strip('.txt'))\n",
    "    return x_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_stat(word, freq, index):\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    word = dictionary.doc2bow([word])[0][0]\n",
    "    idf = bm25.idf[word] if bm25.idf[word] >= 0 else EPSILON * average_idf\n",
    "    score = freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "              / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / bm25.avgdl))))\n",
    "    return (index_doc[word], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "dest_filename = 'Temp.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "def common_words(q, F):\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    index = filenames.index(F + '.txt')\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    row = []\n",
    "    for word, freq in  zip(q_words, q_freq):\n",
    "        w = dictionary.doc2bow([word])[0][0]\n",
    "        if w in index_doc: \n",
    "            f, s = score_stat(word, freq, index)\n",
    "            row.append([word + ', ' + str(round(bm25.idf[w], 2)), freq, f,  round( (s*100)/sim_list[q][index], 1)])  \n",
    "    row.sort(key = lambda x: (x[3],x[1],x[2]), reverse = True)\n",
    "    wb = load_workbook('Temp.xlsx')\n",
    "    ws = wb.create_sheet(title=str(q) + F)\n",
    "    ws.append(['word,idf', 'Query freq', 'freq', '% score'])\n",
    "    for i in row:\n",
    "        ws.append(i)\n",
    "    wb.save(filename = 'Temp.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"toberead.txt\", 'r') \n",
    "count = 0\n",
    "q = 1\n",
    "for i in f:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    count = count + 1\n",
    "    common_words(q, i.strip())\n",
    "    if count % 5 == 0:\n",
    "        q = q + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in files_to_tokens(full_filenames[filenames.index(doc + '.txt')]):\n",
    "    for j in i:\n",
    "        print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = 'ConsumerCourt_SCDRC_30176'\n",
    "corpus[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words(1, 'ConsumerCourt_DCDRC_41588')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index= None, columns=['words','pair'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.append(['lol', ('we','wew')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncommon_words(q, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "wb = load_workbook('Explore_BM25.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_uncom.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_uncom.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "#         for word, freq in  zip(q_words, q_freq):\n",
    "#             if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "#                 f, s = score_stat(word, freq, index)\n",
    "#                 row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "#             else:\n",
    "#                 row.append('X')\n",
    "        #ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_all.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_all.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(sec_row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

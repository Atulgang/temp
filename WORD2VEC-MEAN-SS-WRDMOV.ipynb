{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-13 05:10:45,730 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "en_stop = set(get_stop_words('en'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in word_tokenize(string):\n",
    "            if word not in en_stop:\n",
    "                tokens.append(ps.stem(word))\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield text\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield text\n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield text\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield text\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield text\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield text\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield text\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield text\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-13 05:11:04,531 : INFO : loading Dictionary object from stem100_dictionary\n",
      "2018-04-13 05:11:04,557 : INFO : loaded stem100_dictionary\n"
     ]
    }
   ],
   "source": [
    "#################################### laod the dictinary\n",
    "dictionary = corpora.Dictionary.load('stem100_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-13 05:11:04,698 : INFO : loaded corpus index from stem100.mm.index\n",
      "2018-04-13 05:11:04,699 : INFO : initializing corpus reader from stem100.mm\n",
      "2018-04-13 05:11:04,700 : INFO : accepted corpus with 351985 documents, 40069 features, 92572684 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('stem100.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# c = texts()\n",
    "# count = 0\n",
    "# for x in c.everything():\n",
    "#     x = [ i for i in x if i in dictionary.token2id]\n",
    "#     with open(os.path.normpath('Token_corpus/' + str(count)), 'wb') as fp:\n",
    "#         pickle.dump(x, fp)\n",
    "#     count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_corpus(i):\n",
    "    with open (os.path.normpath('Token_corpus/' + str(i)), 'rb') as fp:\n",
    "            itemlist = pickle.load(fp)\n",
    "            return itemlist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class iter_token_corpus:\n",
    "    def __iter__(self):\n",
    "        for i in xrange(0, 351985):\n",
    "            with open (os.path.normpath('Token_corpus/' + str(i)), 'rb') as fp:\n",
    "                itemlist = pickle.load(fp)\n",
    "                yield itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = gensim.models.Word2Vec(iter_token_corpus(), size=300, iter = 10, min_count=0, sample=0, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-13 05:11:05,357 : INFO : loading Word2Vec object from word2vecsubsam\n",
      "2018-04-13 05:11:05,736 : INFO : loading wv recursively from word2vecsubsam.wv.* with mmap=None\n",
      "2018-04-13 05:11:05,737 : INFO : loading syn0 from word2vecsubsam.wv.syn0.npy with mmap=None\n",
      "2018-04-13 05:11:06,156 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-04-13 05:11:06,157 : INFO : loading syn1neg from word2vecsubsam.syn1neg.npy with mmap=None\n",
      "2018-04-13 05:11:06,830 : INFO : setting ignored attribute cum_table to None\n",
      "2018-04-13 05:11:06,831 : INFO : loaded word2vecsubsam\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('word2vecsubsam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x.doesnt_match(words)\n",
    "\n",
    "# #     Which word from the given list doesn’t go with the others?\n",
    "# #     Parameters:\twords – List of words\n",
    "# #     Returns:\tThe word further away from the mean of all words.\n",
    "# #     Return type:\tstr\n",
    "\n",
    "# x.index2word\n",
    "# ### list of words in the sorted order of their frequency \n",
    "\n",
    "# y = x.vocab['parti']\n",
    "# y.count\n",
    "# # find freq. of any word\n",
    "\n",
    "\n",
    "# x.most_similar_cosmul(positive=None, negative=None, topn=10)\n",
    "# # Find the top-N most similar words, using the multiplicative combination objective proposed by \n",
    "# # Omer Levy and Yoav Goldberg.Positive words still contribute positively towards the similarity, negative words \n",
    "# # negatively, but with less susceptibility to one large distance dominating the calculation.\n",
    "\n",
    "# x.most_similar_to_given(entity1, entities_list)\n",
    "# #    Return the entity from entities_list most similar to entity1.\n",
    "\n",
    "# x.n_similarity(ws1, ws2)\n",
    "# #Compute cosine similarity between two sets of words\n",
    "\n",
    "\n",
    "# x.similar_by_vector(x['fit'], topn=10, restrict_vocab=None)\n",
    "# # closest vectors\n",
    "# # if topn is False, similar_by_vector returns the vector of similarity scores.\n",
    "\n",
    "# x.similar_by_word(word, topn=10, restrict_vocab=None)\n",
    "# #  Find the top-N most similar words.\n",
    "\n",
    "# x.similarity(w1, w2)\n",
    "# #Compute cosine similarity between two words.\n",
    "\n",
    "# x.wmdistance(doc1, doc2)\n",
    "# # word movers distance\n",
    "\n",
    "# y = x.vocab['parti']\n",
    "# y.count\n",
    "# #freq in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from numpy import dot, float32 as REAL, empty, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "ndarray, sum as np_sum, prod, argmax, divide as np_divide\n",
    "\n",
    "\n",
    "w2v  = model.wv\n",
    "\n",
    "def wmdistance(document1, document2):\n",
    "    dic = Dictionary(documents=[document1, document2])\n",
    "    vocab_len = len(dic)\n",
    "\n",
    "    if vocab_len == 1:\n",
    "        # Both documents are composed by a single unique token\n",
    "        return 0.0\n",
    "\n",
    "    # Sets for faster look-up.\n",
    "    docset1 = set(document1)\n",
    "    docset2 = set(document2)\n",
    "\n",
    "    # Compute distance matrix.\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)\n",
    "    for i, t1 in dic.items():\n",
    "        for j, t2 in dic.items():\n",
    "            if t1 not in docset1 or t2 not in docset2:\n",
    "                continue\n",
    "            # Compute Euclidean distance between word vectors.\n",
    "            distance_matrix[i, j] = sqrt(np_sum((w2v[t1] - w2v[t2])**2))\n",
    "\n",
    "    if np_sum(distance_matrix) == 0.0:\n",
    "        # `emd` gets stuck if the distance matrix contains only zeros.\n",
    "        logger.info('The distance matrix is all zeros. Aborting (returning inf).')\n",
    "        return -1000\n",
    "\n",
    "    def nbow(document):\n",
    "        d = zeros(vocab_len, dtype=double)\n",
    "        nbow = dic.doc2bow(document)  # Word frequencies.\n",
    "        doc_len = len(document)\n",
    "        for idx, freq in nbow:\n",
    "            d[idx] = freq / float(doc_len)  # Normalized word frequencies.\n",
    "        return d\n",
    "\n",
    "    # Compute nBOW representation of documents.\n",
    "    d1 = nbow(document1)\n",
    "    d2 = nbow(document2)\n",
    "\n",
    "    # Compute WMD.\n",
    "    return emd(d1, d2, distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from numpy import dot, float32 as REAL, empty, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "ndarray, sum as np_sum, prod, argmax, divide as np_divide\n",
    "\n",
    "\n",
    "w2v  = model.wv\n",
    "def wmdistance(document1, document2, dlen1, dlen2):\n",
    "    # Sets for faster look-up.\n",
    "    docset1 = set([i[0] for i in document1])\n",
    "    docset2 = set([i[0] for i in document2])\n",
    "    voclist = sorted(set.union(docset1, docset2))\n",
    "    vocab_len = len(voclist)\n",
    "    voc_dic = {k: v for v, k in enumerate(voclist)}\n",
    "    \n",
    "    \n",
    "    # Compute distance matrix.\n",
    "    distance_matrix = zeros((vocab_len, vocab_len), dtype=double)\n",
    "    for i in docset1:\n",
    "        for j in docset2:\n",
    "            if (i,j) not in distance_cache:    \n",
    "                # Compute Euclidean distance between word vectors.\n",
    "                distance_matrix[voc_dic[i], voc_dic[j]] = sqrt(np_sum((w2v[dictionary.id2token[i]] - w2v[dictionary.id2token[j]])**2))\n",
    "                distance_cache[(i,j)] = distance_matrix[voc_dic[i], voc_dic[j]]\n",
    "            else:\n",
    "                distance_matrix[voc_dic[i], voc_dic[j]] = distance_cache[(i,j)]\n",
    "    \n",
    "    #print distance_matrix\n",
    "    if np_sum(distance_matrix) == 0.0:\n",
    "        # `emd` gets stuck if the distance matrix contains only zeros.\n",
    "        return -1000\n",
    "\n",
    "    d1 = zeros(vocab_len, dtype=double)\n",
    "    d2 = zeros(vocab_len, dtype=double)\n",
    "    for w, freq in document1:\n",
    "        d1[voc_dic[w]] = freq / float(dlen1)  # Normalized word frequencies.\n",
    "    for w, freq in document2:\n",
    "        d2[voc_dic[w]] = freq / float(dlen2)  # Normalized word frequencies.\n",
    "\n",
    "    # Compute WMD.\n",
    "    return emd(d1, d2, distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.mmcorpus.MmCorpus at 0x7f6f82ce8150>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc_len\n",
    "with open('doc_len_stem100.pkl', 'rb') as ip:\n",
    "    doc_len = pickle.load(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docsets = []\n",
    "for x in corpus:\n",
    "    docsets.append(set([i[0] for i in x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 22s, sys: 80 ms, total: 15min 22s\n",
      "Wall time: 15min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = copy.deepcopy(query_truth)\n",
    "sim_list = [0] + [[] for i in range(10)]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "qlen = [0]\n",
    "qbow = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    query = [i for i in query if i in dictionary.token2id]\n",
    "    qlen.append(len(query))\n",
    "    queries.append(query)\n",
    "    qbow.append(dictionary.doc2bow(query)\n",
    "t = 0\n",
    "for d,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        for i in range(1, 11):\n",
    "            sim_list[i].append(wmdistance(qbow[i], doc, qlen[i], doc_len[d]))\n",
    "    else:\n",
    "        for i in range(1, 11):\n",
    "            sim_list[i].append(-1000)\n",
    "    t = t + 1\n",
    "    #print t\n",
    "    if t >500:\n",
    "        break\n",
    "for i in range(1, 11):\n",
    "    break\n",
    "    ranks = rankdata(sims[i], method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " [16.890827099511526],\n",
       " [15.420610249286486],\n",
       " [16.063524800096037],\n",
       " [14.813866890527427],\n",
       " [15.612927507185416],\n",
       " [17.153650218064293],\n",
       " [15.523167647444359],\n",
       " [15.43141604724848],\n",
       " [15.142004262530309],\n",
       " [16.1761308301454]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

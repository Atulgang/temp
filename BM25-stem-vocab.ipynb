{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 16:56:24,504 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob2\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "from  scipy.stats import rankdata\n",
    "import copy\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.compat import range\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignores everything except english alphabet and  \n",
    "def only_alphabet(text):\n",
    "    return ''.join(i for i in text if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or ord(i)==32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        full_filenames.append(filename)\n",
    "            \n",
    "filenames = []\n",
    "x= ['LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt','LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt','LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt' ,'LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt', 'LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt']\n",
    "for glob_filenames in x:\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        filenames.append(os.path.basename(filename))\n",
    "\n",
    "# get Vocabulary and remove words with length less than 3 \n",
    "vocab  = open(\"wordsEn.txt\").read().splitlines()\n",
    "vocab = [i for i in vocab  if len(i)>2]\n",
    "\n",
    "# vocab is sorted in descending order \n",
    "vocab.sort(key=len, reverse=True)\n",
    "\n",
    "vocab_set = set(vocab)\n",
    "en_stop = set(get_stop_words('en'))\n",
    "ps = PorterStemmer()\n",
    "# \"yield\" for each file return token list  i.e list of lists\n",
    "def files_to_tokens(glob_filenames):\n",
    "    glob_filenames = os.path.normpath(glob_filenames)\n",
    "    for filename in sorted(glob2.glob(glob_filenames)):\n",
    "        f = open(filename)\n",
    "        # read the whole file as lowercase string\n",
    "        string = only_alphabet(f.read()).lower()\n",
    "        tokens = []\n",
    "        # tokenize that string\n",
    "        for word in word_tokenize(string):\n",
    "            if word not in en_stop:\n",
    "                if word not in vocab_set:\n",
    "                    tokens.append(ps.stem(word))\n",
    "\n",
    "        yield tokens\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \n",
    "# yields token list for files specific to courts; needed for creating dictionaries\n",
    "class texts:\n",
    "    def DCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/**/*.txt')\n",
    "    def NCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt')\n",
    "    def SCDRC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt')\n",
    "    def DelhiHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt')\n",
    "    def JharkhandHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt')\n",
    "    def JodhpurHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt')\n",
    "    def KolkataHC(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt')\n",
    "    def SupremeCourt(self):\n",
    "        return files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt')\n",
    "\n",
    "# yields bow for each file - tuples id,fq ; needed to train models   \n",
    "class my_corpus:    \n",
    "    def DCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/DCDRC/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def NCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/NCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)            \n",
    "    def SCDRC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/ConsumerCourtAdhocdata/SCDRC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def DelhiHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/DelhiHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JharkhandHC(self):\n",
    "        for text in files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JharkhandHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def JodhpurHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/JodhpurHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def KolkataHC(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/KolkataHC/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def SupremeCourt(self):\n",
    "        for text in  files_to_tokens('LegalAdhocTask/SC-HCAdhocData/SupremeCourt/**/*.txt'):\n",
    "            yield dictionary.doc2bow(text)\n",
    "    def everything(self):\n",
    "        return chain(self.DCDRC(), self.NCDRC(), self.SCDRC(), self.DelhiHC(),\n",
    "                     self.JharkhandHC(), self.JodhpurHC(), self.KolkataHC(), self.SupremeCourt())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 16:56:43,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 16:59:10,924 : INFO : adding document #10000 to Dictionary(163645 unique tokens: [u'gudadannavar', u'alsocont', u'beencredit', u'delhifano', u'alsocons']...)\n",
      "2018-03-16 17:01:24,970 : INFO : adding document #20000 to Dictionary(301250 unique tokens: [u'gudadannavar', u'falselitig', u'againstcomplain', u'ofday', u'nonehru']...)\n",
      "2018-03-16 17:03:32,432 : INFO : adding document #30000 to Dictionary(459387 unique tokens: [u'gudadannavar', u'mrmvenkatesancounsel', u'rkgejggll', u'boncer', u'metersand']...)\n",
      "2018-03-16 17:05:40,187 : INFO : adding document #40000 to Dictionary(641134 unique tokens: [u'gudadannavar', u'woodb', u'mrmvenkatesancounsel', u'atsugham', u'rkgejggll']...)\n",
      "2018-03-16 17:07:12,686 : INFO : adding document #50000 to Dictionary(757728 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'khallimahapatro', u'mdbw']...)\n",
      "2018-03-16 17:08:45,485 : INFO : adding document #60000 to Dictionary(872993 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'tenantvanish', u'pricepremium']...)\n",
      "2018-03-16 17:10:07,433 : INFO : adding document #70000 to Dictionary(956792 unique tokens: [u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'tenantvanish', u'pricepremium']...)\n",
      "2018-03-16 17:11:47,532 : INFO : adding document #80000 to Dictionary(1053339 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'tenantvanish']...)\n",
      "2018-03-16 17:13:40,596 : INFO : adding document #90000 to Dictionary(1144735 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'tenantvanish']...)\n",
      "2018-03-16 17:15:53,902 : INFO : adding document #100000 to Dictionary(1263301 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'boncer', u'tenantvanish']...)\n",
      "2018-03-16 17:18:05,322 : INFO : adding document #110000 to Dictionary(1388569 unique tokens: [u'mdbg', u'oldwhirlpool', u'asstregistrarcoop', u'venkatapurshottam', u'boncer']...)\n",
      "2018-03-16 17:19:18,884 : INFO : adding document #120000 to Dictionary(1447978 unique tokens: [u'persontransf', u'asstregistrarcoop', u'boncer', u'sbhagsingh', u'ltdvill']...)\n",
      "2018-03-16 17:19:48,387 : INFO : built Dictionary(1468546 unique tokens: [u'persontransf', u'asstregistrarcoop', u'boncer', u'sbhagsingh', u'ltdvill']...) from 122895 documents (total 27203838 corpus positions)\n",
      "2018-03-16 17:19:48,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 17:20:43,032 : INFO : built Dictionary(177606 unique tokens: [u'beencredit', u'vmunicip', u'beingcertif', u'fromdrrajendra', u'gavar']...) from 3484 documents (total 965867 corpus positions)\n",
      "2018-03-16 17:20:44,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 17:21:57,874 : INFO : adding document #10000 to Dictionary(89326 unique tokens: [u'cfpvfzdzm', u'sonji', u'nehatero', u'gavat', u'pricepremium']...)\n",
      "2018-03-16 17:23:22,418 : INFO : adding document #20000 to Dictionary(241124 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'gavar', u'gavas', u'gavat']...)\n",
      "2018-03-16 17:25:16,883 : INFO : adding document #30000 to Dictionary(399853 unique tokens: [u'mrchandrabali', u'cfpvfzdzm', u'metersand', u'noseventh', u'pricepremium']...)\n",
      "2018-03-16 17:26:57,885 : INFO : adding document #40000 to Dictionary(563654 unique tokens: [u'iiifollowingmorn', u'mrchandrabali', u'cfpvfzdzm', u'metersand', u'theoryha']...)\n",
      "2018-03-16 17:28:41,767 : INFO : adding document #50000 to Dictionary(693521 unique tokens: [u'iiifollowingmorn', u'mrchandrabali', u'isearth', u'cfpvfzdzm', u'metersand']...)\n",
      "2018-03-16 17:29:37,461 : INFO : built Dictionary(758941 unique tokens: [u'gavecredit', u'mrchandrabali', u'atjagarajup', u'pricepremium', u'ltdvill']...) from 56745 documents (total 11283783 corpus positions)\n",
      "2018-03-16 17:29:38,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 17:32:13,339 : INFO : adding document #10000 to Dictionary(104911 unique tokens: [u'throughmrvpchaudhari', u'gangoor', u'gai', u'sutain', u'gavat']...)\n",
      "2018-03-16 17:34:51,697 : INFO : adding document #20000 to Dictionary(152622 unique tokens: [u'throughmrvpchaudhari', u'gangoor', u'gai', u'sutain', u'trramachndra']...)\n",
      "2018-03-16 17:37:32,209 : INFO : adding document #30000 to Dictionary(232743 unique tokens: [u'insolventwheth', u'anyex', u'gangoor', u'ofday', u'gavat']...)\n",
      "2018-03-16 17:38:33,877 : INFO : built Dictionary(260843 unique tokens: [u'insolventwheth', u'anyex', u'gangoor', u'ofday', u'gavat']...) from 34008 documents (total 6947718 corpus positions)\n",
      "2018-03-16 17:38:36,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 17:39:32,034 : INFO : adding document #10000 to Dictionary(47952 unique tokens: [u'jhubaoraon', u'ikkzir', u'theappointmentwastobemadeoncontractualbasisforaperiodoftwoyearsth', u'gavan', u'petitionersborrow']...)\n",
      "2018-03-16 17:39:59,958 : INFO : adding document #20000 to Dictionary(69347 unique tokens: [u'inconnectionwithstnoof', u'asmuddin', u'jhubaoraon', u'ikkzir', u'theappointmentwastobemadeoncontractualbasisforaperiodoftwoyearsth']...)\n",
      "2018-03-16 17:40:33,945 : INFO : adding document #30000 to Dictionary(105358 unique tokens: [u'ramdhir', u'withbarwadihpscasenoofcorrespondingtogrnoaofstnooffortheoffencepunishableundersectionsoftheindianpenalcodesectionbaandofthearmsact', u'indardeo', u'showa', u'thereisadisputebetweenthepartiesthatthepetitionerisnota']...)\n",
      "2018-03-16 17:41:08,795 : INFO : adding document #40000 to Dictionary(138176 unique tokens: [u'ramdhir', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'aloksinghjmanishtextdoc', u'withbarwadihpscasenoofcorrespondingtogrnoaofstnooffortheoffencepunishableundersectionsoftheindianpenalcodesectionbaandofthearmsact', u'statesi']...)\n",
      "2018-03-16 17:41:32,101 : INFO : adding document #50000 to Dictionary(151414 unique tokens: [u'ramdhir', u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'aloksinghjmanishtextdoc', u'withbarwadihpscasenoofcorrespondingtogrnoaofstnooffortheoffencepunishableundersectionsoftheindianpenalcodesectionbaandofthearmsact', u'vishwajeetbhartibunti']...)\n",
      "2018-03-16 17:42:19,497 : INFO : adding document #60000 to Dictionary(192182 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'tajudeen', u'indardeo', u'thereisadisputebetweenthepartiesthatthepetitionerisnota']...)\n",
      "2018-03-16 17:42:51,293 : INFO : adding document #70000 to Dictionary(228521 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'councilwithinfourweekstheremainingamountwillbereturnedthedraftshal', u'tajudeen', u'indardeo']...)\n",
      "2018-03-16 17:42:55,392 : INFO : built Dictionary(230676 unique tokens: [u'counselforthepetitionersubmittedthattheorderdatedthjanuarypassedbythiscourtinwpsnooffornoncomplianceof', u'mrmmprasadadvocatefortherespond', u'councilwithinfourweekstheremainingamountwillbereturnedthedraftshal', u'tajudeen', u'indardeo']...) from 71312 documents (total 3237206 corpus positions)\n",
      "2018-03-16 17:42:56,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-16 17:44:31,446 : INFO : adding document #10000 to Dictionary(154790 unique tokens: [u'merelysupervisori', u'appealedbefor', u'icici', u'theoryha', u'hypertechnicalground']...)\n",
      "2018-03-16 17:45:45,758 : INFO : adding document #20000 to Dictionary(238972 unique tokens: [u'selfexplanatorywhich', u'ofday', u'appealedbefor', u'arithmeticallyincorrect', u'theoryha']...)\n",
      "2018-03-16 17:47:25,217 : INFO : adding document #30000 to Dictionary(324946 unique tokens: [u'purposenarendra', u'isintroduc', u'selfexplanatorywhich', u'ofday', u'appealedbefor']...)\n",
      "2018-03-16 17:47:48,711 : INFO : built Dictionary(341370 unique tokens: [u'purposenarendra', u'isintroduc', u'selfexplanatorywhich', u'ofday', u'appealedbefor']...) from 32240 documents (total 5240052 corpus positions)\n",
      "2018-03-16 17:47:48,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-16 17:48:12,413 : INFO : built Dictionary(19715 unique tokens: [u'mohini', u'noezi', u'mahensaria', u'fourfiv', u'indranil']...) from 1210 documents (total 275987 corpus positions)\n",
      "2018-03-16 17:48:13,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    }
   ],
   "source": [
    "T = texts()\n",
    "dictionary_DCDRC = corpora.Dictionary(text for text in T.DCDRC())\n",
    "dictionary_NCDRC = corpora.Dictionary(text for text in T.NCDRC())\n",
    "dictionary_SCDRC = corpora.Dictionary(text for text in T.SCDRC())\n",
    "dictionary_DelhiHC = corpora.Dictionary(text for text in T.DelhiHC())\n",
    "dictionary_JharkhandHC = corpora.Dictionary(text for text in T.JharkhandHC())\n",
    "dictionary_JodhpurHC = corpora.Dictionary(text for text in T.JodhpurHC())\n",
    "dictionary_KolkataHC = corpora.Dictionary(text for text in T.KolkataHC())\n",
    "dictionary_SupremeCourt = corpora.Dictionary(text for text in T.SupremeCourt())\n",
    "# discard words occuring in less than 5 documents and in more than 50% \n",
    "dictionary_DCDRC.filter_extremes(keep_n=None)\n",
    "dictionary_NCDRC.filter_extremes(keep_n=None) \n",
    "dictionary_SCDRC .filter_extremes(keep_n=None)\n",
    "dictionary_DelhiHC.filter_extremes(keep_n=None) \n",
    "dictionary_JharkhandHC.filter_extremes(keep_n=None) \n",
    "dictionary_JodhpurHC.filter_extremes(keep_n=None) \n",
    "dictionary_KolkataHC.filter_extremes(keep_n=None) \n",
    "dictionary_SupremeCourt.filter_extremes(keep_n=None) \n",
    "# Merge all the dictionaries\n",
    "dictionary = dictionary_DCDRC\n",
    "dictionary.merge_with(dictionary_DelhiHC)\n",
    "dictionary.merge_with(dictionary_NCDRC)\n",
    "dictionary.merge_with(dictionary_SCDRC)\n",
    "dictionary.merge_with(dictionary_SupremeCourt)\n",
    "dictionary.merge_with(dictionary_JharkhandHC)\n",
    "dictionary.merge_with(dictionary_JodhpurHC)\n",
    "dictionary.merge_with(dictionary_KolkataHC)\n",
    "dictionary.compactify()\n",
    "dictionary.filter_extremes(keep_n=None)\n",
    "# Need this after merging\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is BOW wrapper\n",
    "c = my_corpus()\n",
    "corpus = c.everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('e.mm', corpus)\n",
    "######################################## load BOW\n",
    "corpus = corpora.MmCorpus('e.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the query truth in dictionary\n",
    "query_truth = {}\n",
    "for i in range(1,11):\n",
    "    query_truth[str(i)]=[]\n",
    "    \n",
    "    \n",
    "f = open('LegalAdhocTask/Consumer.qrels')\n",
    "lines = [line.rstrip('\\n').split(\"\\t\") for line in f]\n",
    "for line in lines:\n",
    "    del line[1]\n",
    "    query_truth[line[0]].append(line[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save list of document lengths in the corpus\n",
    "doc_len = []\n",
    "for doc in corpus:\n",
    "    temp = 0\n",
    "    for word, freq in doc:\n",
    "        temp += freq \n",
    "    doc_len.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "#     The variable k1 is a positive tuning parameter that calibrates the\n",
    "#     document term frequency scaling. A k1 value of 0 corresponds to a binary\n",
    "#     model (no term frequency), and a large value corresponds to using raw term\n",
    "#     frequency. b is another tuning parameter (0 ≤ b ≤ 1) which determines\n",
    "#     the scaling by document length: b = 1 corresponds to fully scaling the term\n",
    "#     weight by the document length, while b = 0 corresponds to no length normalization\n",
    "\n",
    "\n",
    "# BM25 parameters.\n",
    "PARAM_K1 =1.2\n",
    "PARAM_B = 0.75\n",
    "\n",
    "# incase the idf is zero\n",
    "EPSILON = 0.01\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus_size = dictionary.num_docs\n",
    "        s = 0\n",
    "        for i in corpus:\n",
    "            for x in i:\n",
    "                s = s + x[1]\n",
    "        s = float(s)    \n",
    "        self.avgdl = s / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.df = dictionary.dfs\n",
    "        self.idf = {}\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, query, index, average_idf):\n",
    "        score = 0\n",
    "        index_doc = dict(self.corpus[index])\n",
    "        \n",
    "        for word,freq in query:\n",
    "            if word not in index_doc:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "                      / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / self.avgdl))))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(query, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bm25 = BM25(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to pass it to the function that calculates the score\n",
    "average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = copy.deepcopy(query_truth)\n",
    "\n",
    "sim_list = [0]\n",
    "rank_list = [0]\n",
    "queries = [0]\n",
    "for i,query in enumerate(files_to_tokens('LegalAdhocTask/q*.txt')):\n",
    "    queries.append(query)\n",
    "    sims = bm25.get_scores(dictionary.doc2bow(query), average_idf)\n",
    "    sim_list.append(sims)\n",
    "    # rank of every document wrt similarity\n",
    "    ranks = rankdata(sims, method='ordinal')\n",
    "    ranks= len(ranks)+1 - ranks \n",
    "    rank_list.append(ranks)\n",
    "    \n",
    "    # update the query truth tuples with similarity score and the ranks\n",
    "    for x in results[str(i+1)]:\n",
    "        x.append(sims[filenames.index(x[0]+'.txt')])\n",
    "        x.append(ranks[filenames.index(x[0]+'.txt')])\n",
    "    print i\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'BM25.xlsx'\n",
    "ws1 = wb.active\n",
    "ws1.title = \"K1 = 1.2 B = 0.75 EPS = 0.01 |\"\n",
    "ws1.append(['Query','Filename', 'Relevance', 'Score', 'Rank'])\n",
    "for key,value in results.iteritems():\n",
    "    for i in value:\n",
    "        ws1.append([int(key)]+ i)\n",
    "\n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise (F score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"evaluation_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "k = [1.0, 5.0 ,10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "for sheet in f.sheet_names:\n",
    "    if(sheet == \"evaluation\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total_1 = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total_0 = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total = (float)(x.shape[0])\n",
    "        \n",
    "        ####  repitition due to complication in considering both 1 & 0 relevance\n",
    "        # precision 1\n",
    "        row = [q, 1, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/i)\n",
    "        ws1.append(row);\n",
    "        p1 = row;\n",
    "        # precision 0\n",
    "        row = [q, 0, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/i)    \n",
    "        ws1.append(row);\n",
    "        p0 = row;\n",
    "        #precision 10\n",
    "        row = [q, 10, 'P', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/i)  \n",
    "        ws1.append(row);\n",
    "        p10 = row;  \n",
    "        \n",
    "        \n",
    "        # recall 1\n",
    "        row = [q, 1, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 1 , x['Rank'] <= i)].shape[0]/total_1)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r1 = row;    \n",
    "        # recall 0\n",
    "        row = [q, 0, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[np.logical_and(x['Relevance'] == 0 , x['Rank'] <= i)].shape[0]/total_0)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r0 = row;\n",
    "        #recall 10\n",
    "        row = [q, 10, 'R', model_name + ' :' + sheet]\n",
    "        for i in k:\n",
    "            row.append(x[x['Rank'] <= i].shape[0]/total)\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        r10 = row;      \n",
    "\n",
    "\n",
    "        # F 1\n",
    "        row = [q, 1, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p1[i] == 0.0 :\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p1[i],r1[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 0\n",
    "        row = [q, 0, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p0[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p0[i],r0[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "\n",
    "        # F 10\n",
    "        row = [q, 10, 'F', model_name + ' :' + sheet]\n",
    "        for i in range(-len(k), 0, 1):\n",
    "            if p10[i] == 0.0:\n",
    "                row.append(0.0)\n",
    "            else:\n",
    "                row.append(hmean([p10[i],r10[i]]))\n",
    "            row[-1] = round(row[-1],places)\n",
    "        ws1.append(row);\n",
    "        \n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Query wise Average precision\n",
    "# precision sum\n",
    "def p_sum(z):\n",
    "    z = z.copy()  \n",
    "    z.sort_values(inplace=True)\n",
    "    result = 0\n",
    "    for i,val in enumerate(z):\n",
    "        result += (i+1)/float(val) \n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.ExcelFile('BM25.xlsx')\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "\n",
    "model_name = 'BM25'\n",
    "places = 4\n",
    "wb = load_workbook('BM25.xlsx')\n",
    "ws1 = wb.create_sheet(title=\"AP_Qwise\")\n",
    "\n",
    "ws1.append(['Query','Docs considered', 'Precision/Recall', 'Model', 1, 5 ,10, 25, 50, 100, 500, 1000])\n",
    "\n",
    "\n",
    "for sheet in f.sheet_names:    \n",
    "    if(sheet == \"evaluation\"or sheet == \"evaluation_Qwise\"):\n",
    "        continue\n",
    "    for q in np.arange(1,11):\n",
    "        x = f.parse(sheet)\n",
    "        x = x[x['Query'] == q]\n",
    "        total = {}\n",
    "        total[1] = (float)(x[x['Relevance'] == 1].shape[0])\n",
    "        total[0] = (float)(x[x['Relevance'] == 0].shape[0])\n",
    "        total[10] = (float)(x.shape[0])\n",
    "        \n",
    "        for rel in [1,0]:\n",
    "            # precision 1\n",
    "            row = [q, rel, 'AP', model_name + ' :' + sheet]\n",
    "            for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "                row.append(p_sum(x[np.logical_and(x['Relevance'] == rel, x['Rank'] <= i)]['Rank'])/total[rel])\n",
    "            ws1.append(row);\n",
    "\n",
    "        row = [q, 10, 'AP', model_name + ' :' + sheet]\n",
    "        for i in [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0]:\n",
    "            row.append(p_sum(x[x['Rank'] <= i]['Rank'])/total[10])\n",
    "        ws1.append(row);\n",
    "wb.save(filename = 'BM25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Top_x(x, R):\n",
    "    R = list(R)\n",
    "    x_names = []\n",
    "    for i in range(x):\n",
    "        x_names.append(filenames[R.index(i + 1)].strip('.txt'))\n",
    "    return x_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_stat(word, freq, index):\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    word = dictionary.doc2bow([word])[0][0]\n",
    "    idf = bm25.idf[word] if bm25.idf[word] >= 0 else EPSILON * average_idf\n",
    "    score = freq*((idf * index_doc[word] * (PARAM_K1 + 1)\n",
    "              / (index_doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len[index] / bm25.avgdl))))\n",
    "    return (index_doc[word], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "dest_filename = 'Temp.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "def common_words(q, F):\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    index = filenames.index(F + '.txt')\n",
    "    index_doc = dict(bm25.corpus[index])\n",
    "    row = []\n",
    "    for word, freq in  zip(q_words, q_freq):\n",
    "        w = dictionary.doc2bow([word])[0][0]\n",
    "        if w in index_doc: \n",
    "            f, s = score_stat(word, freq, index)\n",
    "            row.append([word + ', ' + str(round(bm25.idf[w], 2)), freq, f,  round( (s*100)/sim_list[q][index], 1)])  \n",
    "    row.sort(key = lambda x: (x[3],x[1],x[2]), reverse = True)\n",
    "    wb = load_workbook('Temp.xlsx')\n",
    "    ws = wb.create_sheet(title=str(q) + F)\n",
    "    ws.append(['word,idf', 'Query freq', 'freq', '% score'])\n",
    "    for i in row:\n",
    "        ws.append(i)\n",
    "    wb.save(filename = 'Temp.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"toberead.txt\", 'r') \n",
    "count = 0\n",
    "q = 1\n",
    "for i in f:\n",
    "    if i == '\\n':\n",
    "        continue\n",
    "    count = count + 1\n",
    "    common_words(q, i.strip())\n",
    "    if count % 5 == 0:\n",
    "        q = q + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in files_to_tokens(full_filenames[filenames.index(doc + '.txt')]):\n",
    "    for j in i:\n",
    "        print j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = 'ConsumerCourt_SCDRC_30176'\n",
    "corpus[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words(1, 'ConsumerCourt_DCDRC_41588')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index= None, columns=['words','pair'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.append(['lol', ('we','wew')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncommon_words(q, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "wb = load_workbook('Explore_BM25.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_uncom.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_uncom.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "#         for word, freq in  zip(q_words, q_freq):\n",
    "#             if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "#                 f, s = score_stat(word, freq, index)\n",
    "#                 row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "#             else:\n",
    "#                 row.append('X')\n",
    "        #ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store results in 1st sheet\n",
    "wb = Workbook()\n",
    "dest_filename = 'Explore_BM25_all.xlsx'\n",
    "ws = wb.active\n",
    "ws.title = \"Blank\"\n",
    "#ws.append([(2.2,2.2)])\n",
    "wb.save(filename = dest_filename)\n",
    "\n",
    "\n",
    "\n",
    "wb = load_workbook('Explore_BM25_all.xlsx')\n",
    "\n",
    "for q in range(1, 11):\n",
    "    ws = wb.create_sheet(title=\"q\" + str(q))\n",
    "    #x_names = Top_x(10, rank_list[q])\n",
    "    top_row = ['Doc Name', 'Rel', 'Doc Length', ' Rank', 'Score']\n",
    "    sec_row = [' ']*5\n",
    "    temp = []\n",
    "    for word, freq in dictionary.doc2bow(queries[q]) :\n",
    "        temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "    temp.sort(key=lambda x: (x[2],x[1]), reverse=True)\n",
    "    ws.append(top_row + [i[0]  for i in temp])\n",
    "    ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "    q_words = [i[0]  for i in temp]\n",
    "    q_freq = [i[1]  for i in temp]\n",
    "    \n",
    "    #continue\n",
    "    doc_list = [i[0] for i in query_truth[str(q)]] + Top_x(10, rank_list[q])\n",
    "    doc_list = list((set(doc_list)))\n",
    "    doc_list.sort(key = lambda doc : rank_list[q][filenames.index(doc + '.txt')])\n",
    "    doc_row = []\n",
    "    for doc in doc_list:\n",
    "        temp  = [doc]\n",
    "        rel_dic = dict(query_truth[str(q)])\n",
    "        if doc in rel_dic:\n",
    "            temp.append(rel_dic[doc])\n",
    "        else:\n",
    "            temp.append('Not')\n",
    "        temp.append(doc_len[filenames.index(doc + '.txt')])\n",
    "        temp.append(rank_list[q][filenames.index(doc + '.txt')])\n",
    "        temp.append(round(sim_list[q][filenames.index(doc + '.txt')], 2))\n",
    "        doc_row.append(temp)\n",
    "\n",
    "    for row in doc_row:\n",
    "        index = filenames.index(row[0] + '.txt')\n",
    "        index_doc = dict(bm25.corpus[index])\n",
    "        \n",
    "        for word, freq in  zip(q_words, q_freq):\n",
    "            if dictionary.doc2bow([word])[0][0] in index_doc: \n",
    "                f, s = score_stat(word, freq, index)\n",
    "                row.append( str(f) + ', ' +  str(round( (s*100)/sim_list[q][index], 1) ))\n",
    "            else:\n",
    "                row.append('X')\n",
    "        ws.append(row)\n",
    "        \n",
    "        sec_row = [' ']*5\n",
    "        temp = []\n",
    "        for word, freq in index_doc.iteritems():\n",
    "            if dictionary[word] not in q_words:\n",
    "                temp.append((dictionary[word], freq, round(bm25.idf[word], 2)))\n",
    "        ws.append(sec_row + [i[0]  for i in temp])\n",
    "        ws.append(sec_row + [str(i[1]) + ', ' + str(i[2]) for i in temp])\n",
    "        ws.append([])        \n",
    "wb.save(filename = dest_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
